2020-04-27 22:39:52.042131 (MainThread): Running with dbt=0.16.1
2020-04-27 22:39:52.113931 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:39:52.115049 (MainThread): Tracking: tracking
2020-04-27 22:39:52.121281 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11281fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112828510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112828990>]}
2020-04-27 22:39:52.142575 (MainThread): Partial parsing not enabled
2020-04-27 22:39:52.146197 (MainThread): Parsing macros/core.sql
2020-04-27 22:39:52.153287 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:39:52.162754 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:39:52.166139 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:39:52.187242 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:39:52.221968 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:39:52.246754 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:39:52.251189 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:39:52.261106 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:39:52.275508 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:39:52.284214 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:39:52.292079 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:39:52.298336 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:39:52.300152 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:39:52.302039 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:39:52.304528 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:39:52.307422 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:39:52.317902 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:39:52.320710 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:39:52.323552 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:39:52.369041 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:39:52.371318 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:39:52.372942 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:39:52.374776 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:39:52.377906 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:39:52.381114 (MainThread): Parsing macros/relations.sql
2020-04-27 22:39:52.383323 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:39:52.401336 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:39:52.419373 (MainThread): Partial parsing not enabled
2020-04-27 22:39:52.446077 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:39:52.446177 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:39:52.461226 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:39:52.461317 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:39:52.467563 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:39:52.467805 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:39:52.607089 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:39:52.608371 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:39:52.608540 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d522d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d1b190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3bf10>]}
2020-04-27 22:39:52.608698 (MainThread): Flushing usage events
2020-04-27 22:39:52.925870 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:40:24.655588 (MainThread): Running with dbt=0.16.1
2020-04-27 22:40:24.717701 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models/staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:40:24.718402 (MainThread): Tracking: tracking
2020-04-27 22:40:24.723250 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108197fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10819b410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108188690>]}
2020-04-27 22:40:24.741250 (MainThread): Partial parsing not enabled
2020-04-27 22:40:24.743085 (MainThread): Parsing macros/core.sql
2020-04-27 22:40:24.747527 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:40:24.755424 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:40:24.757204 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:40:24.775508 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:40:24.816528 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:40:24.838032 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:40:24.839948 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:40:24.846366 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:40:24.859244 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:40:24.866229 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:40:24.872700 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:40:24.877900 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:40:24.878884 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:40:24.879995 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:40:24.881746 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:40:24.883901 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:40:24.893152 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:40:24.895215 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:40:24.896312 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:40:24.938530 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:40:24.939734 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:40:24.940694 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:40:24.941881 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:40:24.944175 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:40:24.946475 (MainThread): Parsing macros/relations.sql
2020-04-27 22:40:24.947810 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:40:24.964219 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:40:24.981655 (MainThread): Partial parsing not enabled
2020-04-27 22:40:25.009918 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:40:25.010018 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:25.025207 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:40:25.025294 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:25.029282 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:40:25.029363 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:25.158546 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:40:25.160696 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:40:25.160960 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086bf7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108708910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10876a4d0>]}
2020-04-27 22:40:25.161165 (MainThread): Flushing usage events
2020-04-27 22:40:25.470004 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:40:32.161212 (MainThread): Running with dbt=0.16.1
2020-04-27 22:40:32.233420 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:40:32.234180 (MainThread): Tracking: tracking
2020-04-27 22:40:32.238862 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114bbb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b3f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b34d0>]}
2020-04-27 22:40:32.261058 (MainThread): Partial parsing not enabled
2020-04-27 22:40:32.263210 (MainThread): Parsing macros/core.sql
2020-04-27 22:40:32.268007 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:40:32.276384 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:40:32.278216 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:40:32.296983 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:40:32.331332 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:40:32.353072 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:40:32.355018 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:40:32.361419 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:40:32.374615 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:40:32.381628 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:40:32.388218 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:40:32.393291 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:40:32.394265 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:40:32.395388 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:40:32.397095 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:40:32.399230 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:40:32.408419 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:40:32.410471 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:40:32.411561 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:40:32.453955 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:40:32.455153 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:40:32.456084 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:40:32.457188 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:40:32.459492 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:40:32.462561 (MainThread): Parsing macros/relations.sql
2020-04-27 22:40:32.464132 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:40:32.487015 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:40:32.505129 (MainThread): Partial parsing not enabled
2020-04-27 22:40:32.531841 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:40:32.531946 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:32.547555 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:40:32.547641 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:32.551552 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:40:32.551636 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:32.673820 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:40:32.675159 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:40:32.675314 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a6bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119ebf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a38450>]}
2020-04-27 22:40:32.675467 (MainThread): Flushing usage events
2020-04-27 22:40:32.983675 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:41:21.278144 (MainThread): Running with dbt=0.16.1
2020-04-27 22:41:21.352066 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:41:21.352817 (MainThread): Tracking: tracking
2020-04-27 22:41:21.357579 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075d1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075d1ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107856290>]}
2020-04-27 22:41:21.376205 (MainThread): Partial parsing not enabled
2020-04-27 22:41:21.378025 (MainThread): Parsing macros/core.sql
2020-04-27 22:41:21.382740 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:41:21.390768 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:41:21.392543 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:41:21.410536 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:41:21.444044 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:41:21.465234 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:41:21.467122 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:41:21.473408 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:41:21.486112 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:41:21.492875 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:41:21.499124 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:41:21.504060 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:41:21.505011 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:41:21.506086 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:41:21.507761 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:41:21.509837 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:41:21.518770 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:41:21.520760 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:41:21.521824 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:41:21.562444 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:41:21.563576 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:41:21.564478 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:41:21.565595 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:41:21.567794 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:41:21.570069 (MainThread): Parsing macros/relations.sql
2020-04-27 22:41:21.571398 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:41:21.588160 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:41:21.606453 (MainThread): Partial parsing not enabled
2020-04-27 22:41:21.641959 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:41:21.642088 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:41:21.658570 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:41:21.658674 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:41:21.662637 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:41:21.662727 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:41:21.788849 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:41:21.790522 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:41:21.790814 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107befe10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107befb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bef250>]}
2020-04-27 22:41:21.791007 (MainThread): Flushing usage events
2020-04-27 22:41:22.169616 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:43:55.998498 (MainThread): Running with dbt=0.16.1
2020-04-27 22:43:56.062845 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:43:56.063755 (MainThread): Tracking: tracking
2020-04-27 22:43:56.069527 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d31bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d270d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d42490>]}
2020-04-27 22:43:56.089438 (MainThread): Partial parsing not enabled
2020-04-27 22:43:56.091205 (MainThread): Parsing macros/core.sql
2020-04-27 22:43:56.095578 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:43:56.103771 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:43:56.105467 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:43:56.122828 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:43:56.155702 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:43:56.177941 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:43:56.180544 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:43:56.190655 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:43:56.203940 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:43:56.210963 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:43:56.217408 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:43:56.222520 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:43:56.223504 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:43:56.225016 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:43:56.226993 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:43:56.229454 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:43:56.238790 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:43:56.240846 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:43:56.241947 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:43:56.284490 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:43:56.285679 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:43:56.286625 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:43:56.287762 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:43:56.290079 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:43:56.292470 (MainThread): Parsing macros/relations.sql
2020-04-27 22:43:56.293865 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:43:56.311031 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:43:56.329287 (MainThread): Partial parsing not enabled
2020-04-27 22:43:56.356362 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:43:56.356458 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:43:56.372232 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:43:56.372331 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:43:56.376174 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:43:56.376259 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:43:56.507054 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:43:56.509029 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:43:56.509362 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11226a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122ca150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112274350>]}
2020-04-27 22:43:56.509544 (MainThread): Flushing usage events
2020-04-27 22:43:56.828615 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:45:44.307277 (MainThread): Running with dbt=0.16.1
2020-04-27 22:45:44.381549 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:45:44.382292 (MainThread): Tracking: tracking
2020-04-27 22:45:44.386657 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfac650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d227e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfac210>]}
2020-04-27 22:45:44.406244 (MainThread): Partial parsing not enabled
2020-04-27 22:45:44.408197 (MainThread): Parsing macros/core.sql
2020-04-27 22:45:44.412924 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:45:44.420885 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:45:44.422641 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:45:44.440519 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:45:44.473728 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:45:44.494891 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:45:44.496805 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:45:44.503247 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:45:44.516204 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:45:44.523138 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:45:44.529526 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:45:44.534816 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:45:44.535795 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:45:44.536898 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:45:44.538609 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:45:44.540777 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:45:44.549968 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:45:44.551923 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:45:44.553011 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:45:44.595298 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:45:44.596447 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:45:44.597509 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:45:44.598598 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:45:44.600835 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:45:44.603138 (MainThread): Parsing macros/relations.sql
2020-04-27 22:45:44.604488 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:45:44.620940 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:45:44.638654 (MainThread): Partial parsing not enabled
2020-04-27 22:45:44.666267 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:45:44.666395 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:45:44.687130 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:45:44.687317 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:45:44.692816 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:45:44.692917 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:45:44.818216 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:45:44.820200 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:45:44.820395 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5773d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4eaa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4d6410>]}
2020-04-27 22:45:44.820570 (MainThread): Flushing usage events
2020-04-27 22:45:45.140926 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:48:37.990790 (MainThread): Running with dbt=0.16.1
2020-04-27 22:48:38.055236 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:48:38.056067 (MainThread): Tracking: tracking
2020-04-27 22:48:38.061211 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a1b6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a18850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a18a10>]}
2020-04-27 22:48:38.083369 (MainThread): Partial parsing not enabled
2020-04-27 22:48:38.085625 (MainThread): Parsing macros/core.sql
2020-04-27 22:48:38.091390 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:48:38.102370 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:48:38.105588 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:48:38.127475 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:48:38.161641 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:48:38.183809 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:48:38.185792 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:48:38.192346 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:48:38.205647 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:48:38.212893 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:48:38.219490 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:48:38.224717 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:48:38.225725 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:48:38.226895 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:48:38.228662 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:48:38.230858 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:48:38.240411 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:48:38.242517 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:48:38.243722 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:48:38.287734 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:48:38.288935 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:48:38.289871 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:48:38.290979 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:48:38.293379 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:48:38.295723 (MainThread): Parsing macros/relations.sql
2020-04-27 22:48:38.297080 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:48:38.313989 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:48:38.331642 (MainThread): Partial parsing not enabled
2020-04-27 22:48:38.359126 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:48:38.359235 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:48:38.375647 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:38.375747 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:48:38.379773 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:38.379859 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:48:38.511822 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:48:38.513931 (MainThread): 
2020-04-27 22:48:38.514219 (MainThread): Acquiring new postgres connection "master".
2020-04-27 22:48:38.514304 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:48:38.525367 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 22:48:38.525476 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 22:48:38.615820 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 22:48:38.615960 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 22:48:39.165381 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.55 seconds
2020-04-27 22:48:39.188459 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:48:39.188685 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 22:48:39.190358 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:48:39.190469 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 22:48:39.254970 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.06 seconds
2020-04-27 22:48:39.255380 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:48:39.255636 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 22:48:39.378616 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-27 22:48:39.380655 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 22:48:39.452209 (MainThread): Using postgres connection "master".
2020-04-27 22:48:39.452371 (MainThread): On master: BEGIN
2020-04-27 22:48:39.856936 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-27 22:48:39.857374 (MainThread): Using postgres connection "master".
2020-04-27 22:48:39.857696 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 22:48:40.075637 (MainThread): SQL status: SELECT in 0.22 seconds
2020-04-27 22:48:40.151720 (MainThread): On master: ROLLBACK
2020-04-27 22:48:40.192948 (MainThread): Using postgres connection "master".
2020-04-27 22:48:40.193341 (MainThread): On master: BEGIN
2020-04-27 22:48:40.300530 (MainThread): SQL status: BEGIN in 0.11 seconds
2020-04-27 22:48:40.300972 (MainThread): On master: COMMIT
2020-04-27 22:48:40.301280 (MainThread): Using postgres connection "master".
2020-04-27 22:48:40.301439 (MainThread): On master: COMMIT
2020-04-27 22:48:40.339843 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 22:48:40.340461 (MainThread): 15:48:40 | Concurrency: 1 threads (target='dev')
2020-04-27 22:48:40.340739 (MainThread): 15:48:40 | 
2020-04-27 22:48:40.344987 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 22:48:40.345209 (Thread-1): 15:48:40 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 22:48:40.345605 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.345740 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 22:48:40.345867 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 22:48:40.361635 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 22:48:40.362398 (Thread-1): finished collecting timing info
2020-04-27 22:48:40.400994 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.401155 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 22:48:40.488170 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-27 22:48:40.492417 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.492569 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 22:48:40.542261 (Thread-1): SQL status: DROP VIEW in 0.05 seconds
2020-04-27 22:48:40.545496 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 22:48:40.546335 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.546486 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 22:48:40.598028 (Thread-1): SQL status: BEGIN in 0.05 seconds
2020-04-27 22:48:40.598420 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.598666 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 22:48:40.662149 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 22:48:40.666359 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.666508 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 22:48:40.709836 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:48:40.711782 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 22:48:40.711966 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.712117 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 22:48:41.015200 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-27 22:48:41.018396 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:41.018560 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 22:48:41.321997 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-27 22:48:41.326309 (Thread-1): finished collecting timing info
2020-04-27 22:48:41.327163 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa0b3084-a491-4ca0-b7a7-0f675b0c1300', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112039690>]}
2020-04-27 22:48:41.327470 (Thread-1): 15:48:41 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.98s]
2020-04-27 22:48:41.327653 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 22:48:41.327838 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 22:48:41.328122 (Thread-1): 15:48:41 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 22:48:41.328483 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.328609 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 22:48:41.328750 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 22:48:41.334967 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 22:48:41.335391 (Thread-1): finished collecting timing info
2020-04-27 22:48:41.343935 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.344077 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 22:48:41.629067 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2020-04-27 22:48:41.633216 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.633362 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 22:48:41.833378 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-27 22:48:41.836418 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 22:48:41.837044 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.837197 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 22:48:41.880081 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:48:41.880469 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.880720 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 22:48:41.942049 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 22:48:41.946257 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.946406 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 22:48:41.989226 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:48:41.991171 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 22:48:41.991362 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.991518 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 22:48:42.346177 (Thread-1): SQL status: COMMIT in 0.35 seconds
2020-04-27 22:48:42.349587 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:42.349734 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 22:48:42.653026 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-27 22:48:42.657171 (Thread-1): finished collecting timing info
2020-04-27 22:48:42.658041 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa0b3084-a491-4ca0-b7a7-0f675b0c1300', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e5d450>]}
2020-04-27 22:48:42.658353 (Thread-1): 15:48:42 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.33s]
2020-04-27 22:48:42.658558 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 22:48:42.659092 (Thread-1): Began running node model.order_history.customers
2020-04-27 22:48:42.659425 (Thread-1): 15:48:42 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 22:48:42.659900 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:48:42.660049 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 22:48:42.660208 (Thread-1): Compiling model.order_history.customers
2020-04-27 22:48:42.668434 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 22:48:42.668825 (Thread-1): finished collecting timing info
2020-04-27 22:48:42.675194 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:48:42.675319 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 22:48:42.927263 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-27 22:48:42.931460 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:48:42.931616 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 22:48:43.124347 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-27 22:48:43.126596 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 22:48:43.127106 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:48:43.127251 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 22:48:43.170750 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:48:43.170940 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:48:43.171043 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
orders as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        count(order_unique_id) as number_of_orders
    from orders
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 22:48:43.213118 (Thread-1): Postgres error: syntax error at or near "."
LINE 23:         customer_orders.first_order_date,
                                ^

2020-04-27 22:48:43.213539 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 22:48:43.255871 (Thread-1): finished collecting timing info
2020-04-27 22:48:43.256912 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "."
  LINE 23:         customer_orders.first_order_date,
                                  ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "."
LINE 23:         customer_orders.first_order_date,
                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "."
  LINE 23:         customer_orders.first_order_date,
                                  ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 22:48:43.277933 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa0b3084-a491-4ca0-b7a7-0f675b0c1300', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e5d450>]}
2020-04-27 22:48:43.278216 (Thread-1): 15:48:43 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.62s]
2020-04-27 22:48:43.278372 (Thread-1): Finished running node model.order_history.customers
2020-04-27 22:48:43.311453 (MainThread): Using postgres connection "master".
2020-04-27 22:48:43.311708 (MainThread): On master: BEGIN
2020-04-27 22:48:43.351293 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:48:43.351749 (MainThread): On master: COMMIT
2020-04-27 22:48:43.352015 (MainThread): Using postgres connection "master".
2020-04-27 22:48:43.352214 (MainThread): On master: COMMIT
2020-04-27 22:48:43.392556 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 22:48:43.393469 (MainThread): 15:48:43 | 
2020-04-27 22:48:43.393692 (MainThread): 15:48:43 | Finished running 3 view models in 4.88s.
2020-04-27 22:48:43.393883 (MainThread): Connection 'master' was left open.
2020-04-27 22:48:43.394028 (MainThread): On master: Close
2020-04-27 22:48:43.394403 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 22:48:43.394557 (MainThread): On model.order_history.customers: Close
2020-04-27 22:48:43.405834 (MainThread): 
2020-04-27 22:48:43.406094 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 22:48:43.406273 (MainThread): 
2020-04-27 22:48:43.406414 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 22:48:43.406539 (MainThread):   syntax error at or near "."
2020-04-27 22:48:43.406653 (MainThread):   LINE 23:         customer_orders.first_order_date,
2020-04-27 22:48:43.406765 (MainThread):                                   ^
2020-04-27 22:48:43.406875 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 22:48:43.406997 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-27 22:48:43.407222 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117a6850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112080310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120801d0>]}
2020-04-27 22:48:43.407488 (MainThread): Flushing usage events
2020-04-27 22:49:05.622256 (MainThread): Running with dbt=0.16.1
2020-04-27 22:49:05.688701 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:49:05.689490 (MainThread): Tracking: tracking
2020-04-27 22:49:05.694368 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10984bb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109acce50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109accf10>]}
2020-04-27 22:49:05.716281 (MainThread): Partial parsing not enabled
2020-04-27 22:49:05.718236 (MainThread): Parsing macros/core.sql
2020-04-27 22:49:05.723239 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:49:05.731866 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:49:05.733724 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:49:05.753576 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:49:05.789702 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:49:05.813393 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:49:05.815883 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:49:05.822599 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:49:05.836877 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:49:05.844502 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:49:05.851042 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:49:05.856284 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:49:05.857307 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:49:05.858538 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:49:05.860310 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:49:05.862503 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:49:05.872171 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:49:05.874358 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:49:05.875524 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:49:05.922018 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:49:05.923289 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:49:05.924269 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:49:05.925423 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:49:05.927791 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:49:05.930264 (MainThread): Parsing macros/relations.sql
2020-04-27 22:49:05.931702 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:49:05.949152 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:49:05.967299 (MainThread): Partial parsing not enabled
2020-04-27 22:49:05.994871 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:49:05.994981 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:49:06.011282 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:06.011376 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:49:06.015360 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:06.015449 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:49:06.138272 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:49:06.139878 (MainThread): 
2020-04-27 22:49:06.140139 (MainThread): Acquiring new postgres connection "master".
2020-04-27 22:49:06.140219 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:49:06.149881 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 22:49:06.149980 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 22:49:06.246082 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 22:49:06.246215 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 22:49:06.700647 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.45 seconds
2020-04-27 22:49:06.721681 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:49:06.721818 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 22:49:06.723606 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:49:06.723817 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 22:49:06.764392 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:06.764801 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:49:06.765057 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 22:49:06.896409 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-27 22:49:06.900208 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 22:49:06.960000 (MainThread): Using postgres connection "master".
2020-04-27 22:49:06.960154 (MainThread): On master: BEGIN
2020-04-27 22:49:07.359596 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-27 22:49:07.360013 (MainThread): Using postgres connection "master".
2020-04-27 22:49:07.360275 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 22:49:07.534328 (MainThread): SQL status: SELECT in 0.17 seconds
2020-04-27 22:49:07.615015 (MainThread): On master: ROLLBACK
2020-04-27 22:49:07.654730 (MainThread): Using postgres connection "master".
2020-04-27 22:49:07.655135 (MainThread): On master: BEGIN
2020-04-27 22:49:07.734787 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 22:49:07.735236 (MainThread): On master: COMMIT
2020-04-27 22:49:07.735551 (MainThread): Using postgres connection "master".
2020-04-27 22:49:07.735702 (MainThread): On master: COMMIT
2020-04-27 22:49:07.774968 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 22:49:07.775851 (MainThread): 15:49:07 | Concurrency: 1 threads (target='dev')
2020-04-27 22:49:07.776096 (MainThread): 15:49:07 | 
2020-04-27 22:49:07.778511 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 22:49:07.778766 (Thread-1): 15:49:07 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 22:49:07.779148 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.779284 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 22:49:07.779425 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 22:49:07.796067 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 22:49:07.796554 (Thread-1): finished collecting timing info
2020-04-27 22:49:07.837542 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.837700 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 22:49:07.916117 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 22:49:07.919171 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.919321 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 22:49:07.958108 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 22:49:07.959868 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 22:49:07.960301 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.960411 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 22:49:07.999022 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:07.999446 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.999711 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 22:49:08.086040 (Thread-1): SQL status: CREATE VIEW in 0.09 seconds
2020-04-27 22:49:08.092364 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:08.092517 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 22:49:08.135415 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:08.139777 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:08.139931 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 22:49:08.181222 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:08.183182 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 22:49:08.183377 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:08.183535 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 22:49:08.457938 (Thread-1): SQL status: COMMIT in 0.27 seconds
2020-04-27 22:49:08.461523 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:08.461699 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 22:49:08.844013 (Thread-1): SQL status: DROP VIEW in 0.38 seconds
2020-04-27 22:49:08.848488 (Thread-1): finished collecting timing info
2020-04-27 22:49:08.849403 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c385504d-1ac6-4a73-a806-7505d91acc34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f16fd0>]}
2020-04-27 22:49:08.849724 (Thread-1): 15:49:08 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.07s]
2020-04-27 22:49:08.849913 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 22:49:08.850148 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 22:49:08.850540 (Thread-1): 15:49:08 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 22:49:08.851034 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:08.851220 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 22:49:08.851358 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 22:49:08.858116 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 22:49:08.858590 (Thread-1): finished collecting timing info
2020-04-27 22:49:08.867722 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:08.867933 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 22:49:09.061236 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-27 22:49:09.065416 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.065569 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 22:49:09.246761 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-27 22:49:09.249851 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 22:49:09.250509 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.250669 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 22:49:09.289138 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:09.289339 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.289452 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 22:49:09.347426 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 22:49:09.353725 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.353885 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 22:49:09.395343 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:09.399465 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.399611 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 22:49:09.439263 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:09.441207 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 22:49:09.441401 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.441557 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 22:49:09.658961 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-27 22:49:09.661201 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.661329 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 22:49:09.993363 (Thread-1): SQL status: DROP VIEW in 0.33 seconds
2020-04-27 22:49:09.997206 (Thread-1): finished collecting timing info
2020-04-27 22:49:09.998145 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c385504d-1ac6-4a73-a806-7505d91acc34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c6d3d0>]}
2020-04-27 22:49:09.998498 (Thread-1): 15:49:09 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.15s]
2020-04-27 22:49:09.998722 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 22:49:09.999329 (Thread-1): Began running node model.order_history.customers
2020-04-27 22:49:09.999608 (Thread-1): 15:49:09 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 22:49:10.000086 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:49:10.000303 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 22:49:10.000475 (Thread-1): Compiling model.order_history.customers
2020-04-27 22:49:10.013611 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 22:49:10.014043 (Thread-1): finished collecting timing info
2020-04-27 22:49:10.021181 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.021363 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 22:49:10.235594 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-27 22:49:10.238327 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.238439 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 22:49:10.505824 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2020-04-27 22:49:10.508912 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 22:49:10.509526 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.509683 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 22:49:10.548686 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:10.549114 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.549386 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
orders as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        count(order_unique_id) as number_of_orders
    from orders
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 22:49:10.616895 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-27 22:49:10.622044 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.622202 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-27 22:49:10.661075 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:10.662347 (Thread-1): On model.order_history.customers: COMMIT
2020-04-27 22:49:10.662494 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.662604 (Thread-1): On model.order_history.customers: COMMIT
2020-04-27 22:49:10.915402 (Thread-1): SQL status: COMMIT in 0.25 seconds
2020-04-27 22:49:10.953040 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.953242 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 22:49:11.177077 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-27 22:49:11.181315 (Thread-1): finished collecting timing info
2020-04-27 22:49:11.182159 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c385504d-1ac6-4a73-a806-7505d91acc34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad7c90>]}
2020-04-27 22:49:11.182463 (Thread-1): 15:49:11 | 3 of 3 OK created view model data_science.customers.................. [CREATE VIEW in 1.18s]
2020-04-27 22:49:11.182641 (Thread-1): Finished running node model.order_history.customers
2020-04-27 22:49:11.287946 (MainThread): Using postgres connection "master".
2020-04-27 22:49:11.288267 (MainThread): On master: BEGIN
2020-04-27 22:49:11.329661 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:11.329885 (MainThread): On master: COMMIT
2020-04-27 22:49:11.330005 (MainThread): Using postgres connection "master".
2020-04-27 22:49:11.330112 (MainThread): On master: COMMIT
2020-04-27 22:49:11.367623 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 22:49:11.368079 (MainThread): 15:49:11 | 
2020-04-27 22:49:11.368233 (MainThread): 15:49:11 | Finished running 3 view models in 5.23s.
2020-04-27 22:49:11.368355 (MainThread): Connection 'master' was left open.
2020-04-27 22:49:11.368459 (MainThread): On master: Close
2020-04-27 22:49:11.368726 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 22:49:11.368829 (MainThread): On model.order_history.customers: Close
2020-04-27 22:49:11.377851 (MainThread): 
2020-04-27 22:49:11.378002 (MainThread): Completed successfully
2020-04-27 22:49:11.378130 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2020-04-27 22:49:11.378318 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c47d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0da850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b41950>]}
2020-04-27 22:49:11.378510 (MainThread): Flushing usage events
2020-04-27 23:06:17.500612 (MainThread): Running with dbt=0.16.1
2020-04-27 23:06:17.579773 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:06:17.580603 (MainThread): Tracking: tracking
2020-04-27 23:06:17.585662 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6f4590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c128210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6f4810>]}
2020-04-27 23:06:17.604288 (MainThread): Partial parsing not enabled
2020-04-27 23:06:17.606320 (MainThread): Parsing macros/core.sql
2020-04-27 23:06:17.610907 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:06:17.618910 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:06:17.620674 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:06:17.638655 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:06:17.672136 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:06:17.693450 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:06:17.695371 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:06:17.701872 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:06:17.714627 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:06:17.721560 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:06:17.727917 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:06:17.732947 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:06:17.733920 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:06:17.735417 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:06:17.737377 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:06:17.739787 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:06:17.749324 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:06:17.751306 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:06:17.752539 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:06:17.793128 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:06:17.794271 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:06:17.795181 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:06:17.796296 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:06:17.798494 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:06:17.800773 (MainThread): Parsing macros/relations.sql
2020-04-27 23:06:17.802099 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:06:17.818771 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:06:17.836514 (MainThread): Partial parsing not enabled
2020-04-27 23:06:17.864584 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:06:17.864756 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:06:17.886623 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:17.886794 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:06:17.892986 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:17.893170 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:06:18.018108 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:06:18.019911 (MainThread): 
2020-04-27 23:06:18.020238 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:06:18.020325 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:06:18.030405 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:06:18.030514 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:06:18.117380 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:06:18.117527 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:06:18.573399 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.46 seconds
2020-04-27 23:06:18.591520 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:06:18.591720 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:06:18.593362 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:06:18.593478 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:06:18.630927 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:18.631353 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:06:18.631622 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:06:18.742170 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-27 23:06:18.747399 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:06:18.810372 (MainThread): Using postgres connection "master".
2020-04-27 23:06:18.810538 (MainThread): On master: BEGIN
2020-04-27 23:06:19.166628 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-27 23:06:19.166916 (MainThread): Using postgres connection "master".
2020-04-27 23:06:19.167082 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:06:19.340464 (MainThread): SQL status: SELECT in 0.17 seconds
2020-04-27 23:06:19.409862 (MainThread): On master: ROLLBACK
2020-04-27 23:06:19.449754 (MainThread): Using postgres connection "master".
2020-04-27 23:06:19.450171 (MainThread): On master: BEGIN
2020-04-27 23:06:19.529551 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:06:19.530025 (MainThread): On master: COMMIT
2020-04-27 23:06:19.530261 (MainThread): Using postgres connection "master".
2020-04-27 23:06:19.530416 (MainThread): On master: COMMIT
2020-04-27 23:06:19.570301 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:06:19.570754 (MainThread): 16:06:19 | Concurrency: 1 threads (target='dev')
2020-04-27 23:06:19.570926 (MainThread): 16:06:19 | 
2020-04-27 23:06:19.572633 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 23:06:19.572882 (Thread-1): 16:06:19 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 23:06:19.573417 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.573544 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:06:19.573677 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 23:06:19.592263 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 23:06:19.592930 (Thread-1): finished collecting timing info
2020-04-27 23:06:19.634672 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.634832 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 23:06:19.710707 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 23:06:19.715104 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.715259 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:06:19.753820 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:06:19.757008 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 23:06:19.757622 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.757777 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 23:06:19.797363 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:19.797797 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.798073 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 23:06:19.859445 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:06:19.865163 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.865306 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 23:06:19.904249 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:06:19.907389 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.907537 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 23:06:19.945860 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:06:19.946990 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:06:19.947115 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.947213 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:06:20.169582 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-27 23:06:20.173094 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:20.173255 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:06:20.410322 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-27 23:06:20.414818 (Thread-1): finished collecting timing info
2020-04-27 23:06:20.415681 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5f9cdf2-d4ce-429a-9566-0d2c76d70060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd60e50>]}
2020-04-27 23:06:20.416003 (Thread-1): 16:06:20 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.84s]
2020-04-27 23:06:20.416194 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 23:06:20.416381 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 23:06:20.416570 (Thread-1): 16:06:20 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 23:06:20.416918 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:20.417068 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 23:06:20.417182 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 23:06:20.423439 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:06:20.423876 (Thread-1): finished collecting timing info
2020-04-27 23:06:20.432387 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:20.432521 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 23:06:20.853570 (Thread-1): SQL status: DROP VIEW in 0.42 seconds
2020-04-27 23:06:20.857754 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:20.857910 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:06:21.467620 (Thread-1): SQL status: DROP VIEW in 0.61 seconds
2020-04-27 23:06:21.469679 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:06:21.470178 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.470312 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 23:06:21.509986 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:21.510289 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.510469 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 23:06:21.570243 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:06:21.576463 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.576616 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 23:06:21.615287 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:06:21.619816 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.620001 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 23:06:21.658440 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:06:21.659956 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:06:21.660135 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.660277 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:06:22.082405 (Thread-1): SQL status: COMMIT in 0.42 seconds
2020-04-27 23:06:22.085871 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:22.086031 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:06:22.311419 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-27 23:06:22.314798 (Thread-1): finished collecting timing info
2020-04-27 23:06:22.315569 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5f9cdf2-d4ce-429a-9566-0d2c76d70060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10df73710>]}
2020-04-27 23:06:22.315835 (Thread-1): 16:06:22 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.90s]
2020-04-27 23:06:22.315990 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 23:06:22.316390 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:06:22.316606 (Thread-1): 16:06:22 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 23:06:22.317016 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:06:22.317161 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 23:06:22.317285 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:06:22.326788 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:06:22.327262 (Thread-1): finished collecting timing info
2020-04-27 23:06:22.334941 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:06:22.335117 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:06:22.561275 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-27 23:06:22.565453 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:06:22.565607 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:06:22.797998 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-27 23:06:22.800655 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:06:22.801244 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:06:22.801410 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:06:22.840453 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:22.840919 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:06:22.841153 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(order_unique_id) as number_of_orders,
        COUNT(order_ticket_unique_id) AS tickets_sold,
    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:06:22.880243 (Thread-1): Postgres error: syntax error at or near "from"
LINE 17:     from order_tickets
             ^

2020-04-27 23:06:22.880680 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 23:06:22.921101 (Thread-1): finished collecting timing info
2020-04-27 23:06:22.922188 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "from"
  LINE 17:     from order_tickets
               ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "from"
LINE 17:     from order_tickets
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "from"
  LINE 17:     from order_tickets
               ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:06:22.925323 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5f9cdf2-d4ce-429a-9566-0d2c76d70060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbd9850>]}
2020-04-27 23:06:22.925619 (Thread-1): 16:06:22 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.61s]
2020-04-27 23:06:22.925803 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:06:22.976113 (MainThread): Using postgres connection "master".
2020-04-27 23:06:22.976488 (MainThread): On master: BEGIN
2020-04-27 23:06:23.015374 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:23.015587 (MainThread): On master: COMMIT
2020-04-27 23:06:23.015710 (MainThread): Using postgres connection "master".
2020-04-27 23:06:23.015814 (MainThread): On master: COMMIT
2020-04-27 23:06:23.068217 (MainThread): SQL status: COMMIT in 0.05 seconds
2020-04-27 23:06:23.069144 (MainThread): 16:06:23 | 
2020-04-27 23:06:23.069387 (MainThread): 16:06:23 | Finished running 3 view models in 5.05s.
2020-04-27 23:06:23.069589 (MainThread): Connection 'master' was left open.
2020-04-27 23:06:23.069744 (MainThread): On master: Close
2020-04-27 23:06:23.070126 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:06:23.070290 (MainThread): On model.order_history.customers: Close
2020-04-27 23:06:23.080377 (MainThread): 
2020-04-27 23:06:23.080582 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 23:06:23.080714 (MainThread): 
2020-04-27 23:06:23.080836 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 23:06:23.080945 (MainThread):   syntax error at or near "from"
2020-04-27 23:06:23.081073 (MainThread):   LINE 17:     from order_tickets
2020-04-27 23:06:23.081185 (MainThread):                ^
2020-04-27 23:06:23.081285 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:06:23.081397 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-27 23:06:23.081589 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd96d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dfdf310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dfd1350>]}
2020-04-27 23:06:23.081790 (MainThread): Flushing usage events
2020-04-27 23:09:06.879499 (MainThread): Running with dbt=0.16.1
2020-04-27 23:09:06.956548 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:09:06.957371 (MainThread): Tracking: tracking
2020-04-27 23:09:06.965932 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba6f090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d011d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d002410>]}
2020-04-27 23:09:06.989267 (MainThread): Partial parsing not enabled
2020-04-27 23:09:06.991461 (MainThread): Parsing macros/core.sql
2020-04-27 23:09:06.996967 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:09:07.007154 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:09:07.009524 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:09:07.030303 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:09:07.065232 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:09:07.088228 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:09:07.090402 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:09:07.096904 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:09:07.110443 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:09:07.117519 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:09:07.124169 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:09:07.129309 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:09:07.130299 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:09:07.131412 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:09:07.133139 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:09:07.135280 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:09:07.145637 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:09:07.147881 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:09:07.149012 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:09:07.201311 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:09:07.202856 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:09:07.204042 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:09:07.205470 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:09:07.208091 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:09:07.210660 (MainThread): Parsing macros/relations.sql
2020-04-27 23:09:07.212123 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:09:07.230366 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:09:07.249557 (MainThread): Partial parsing not enabled
2020-04-27 23:09:07.278566 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:09:07.278720 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:07.295503 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:07.295629 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:07.299757 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:07.299847 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:07.438090 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:09:07.439973 (MainThread): 
2020-04-27 23:09:07.440363 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:09:07.440463 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:07.451333 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:09:07.451540 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:09:07.556582 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:09:07.556725 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:09:08.076358 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.52 seconds
2020-04-27 23:09:08.096619 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:08.096863 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:09:08.098732 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:08.098860 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:09:08.140715 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:08.141125 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:08.141276 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:09:08.250267 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-27 23:09:08.254155 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:09:08.318637 (MainThread): Using postgres connection "master".
2020-04-27 23:09:08.318819 (MainThread): On master: BEGIN
2020-04-27 23:09:08.695110 (MainThread): SQL status: BEGIN in 0.38 seconds
2020-04-27 23:09:08.695420 (MainThread): Using postgres connection "master".
2020-04-27 23:09:08.695593 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:09:09.006209 (MainThread): SQL status: SELECT in 0.31 seconds
2020-04-27 23:09:09.080901 (MainThread): On master: ROLLBACK
2020-04-27 23:09:09.121465 (MainThread): Using postgres connection "master".
2020-04-27 23:09:09.121771 (MainThread): On master: BEGIN
2020-04-27 23:09:09.204487 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:09:09.204799 (MainThread): On master: COMMIT
2020-04-27 23:09:09.204997 (MainThread): Using postgres connection "master".
2020-04-27 23:09:09.205175 (MainThread): On master: COMMIT
2020-04-27 23:09:09.245351 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:09:09.245880 (MainThread): 16:09:09 | Concurrency: 1 threads (target='dev')
2020-04-27 23:09:09.246079 (MainThread): 16:09:09 | 
2020-04-27 23:09:09.248187 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 23:09:09.248456 (Thread-1): 16:09:09 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 23:09:09.248967 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.249127 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:09:09.249262 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 23:09:09.264025 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 23:09:09.264446 (Thread-1): finished collecting timing info
2020-04-27 23:09:09.302789 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.302955 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 23:09:09.389796 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-27 23:09:09.394162 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.394313 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:09:09.437904 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:09:09.440941 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 23:09:09.441651 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.441835 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 23:09:09.484313 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:09.484621 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.484790 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 23:09:09.547634 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:09:09.554607 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.554832 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 23:09:09.599105 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:09.603492 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.603650 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 23:09:09.657937 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-27 23:09:09.659908 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:09:09.660099 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.660255 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:09:09.925847 (Thread-1): SQL status: COMMIT in 0.27 seconds
2020-04-27 23:09:09.928127 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.928272 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:09:10.335914 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-04-27 23:09:10.339474 (Thread-1): finished collecting timing info
2020-04-27 23:09:10.340480 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8515ba72-0963-4752-bb00-685c24949c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6b9090>]}
2020-04-27 23:09:10.340870 (Thread-1): 16:09:10 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.09s]
2020-04-27 23:09:10.341085 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 23:09:10.341400 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 23:09:10.341734 (Thread-1): 16:09:10 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 23:09:10.342166 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:10.342491 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 23:09:10.342772 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 23:09:10.350367 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:09:10.350901 (Thread-1): finished collecting timing info
2020-04-27 23:09:10.360058 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:10.360240 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 23:09:10.644119 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2020-04-27 23:09:10.648156 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:10.648314 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:09:11.052775 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-27 23:09:11.056063 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:09:11.056799 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.057013 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 23:09:11.101425 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:11.101704 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.101839 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 23:09:11.159809 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:09:11.165474 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.165633 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 23:09:11.208140 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:11.212107 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.212256 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 23:09:11.256946 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:11.258348 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:09:11.258502 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.258608 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:09:11.474971 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-27 23:09:11.478444 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.478591 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:09:11.973872 (Thread-1): SQL status: DROP VIEW in 0.50 seconds
2020-04-27 23:09:11.976552 (Thread-1): finished collecting timing info
2020-04-27 23:09:11.977204 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8515ba72-0963-4752-bb00-685c24949c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6a5a90>]}
2020-04-27 23:09:11.977436 (Thread-1): 16:09:11 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.64s]
2020-04-27 23:09:11.977568 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 23:09:11.977956 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:09:11.978121 (Thread-1): 16:09:11 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 23:09:11.978431 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:09:11.978532 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 23:09:11.978631 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:09:11.986748 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:09:11.987190 (Thread-1): finished collecting timing info
2020-04-27 23:09:11.994348 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:11.994517 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:09:12.281096 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-04-27 23:09:12.283947 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:12.284075 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:09:12.588860 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-27 23:09:12.591676 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:09:12.592337 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:12.592509 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:09:12.635390 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:12.635691 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:12.635895 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(order_unique_id) as number_of_orders,
        COUNT(order_ticket_unique_id) AS tickets_purchased,
        SUM(amount_gross) AS total_revenue
    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.tickets_sold, 0) as ticket_purchased,
        cusomter_orders.total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:09:12.683813 (Thread-1): Postgres error: column customer_orders.tickets_sold does not exist

2020-04-27 23:09:12.684230 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 23:09:12.726773 (Thread-1): finished collecting timing info
2020-04-27 23:09:12.727482 (Thread-1): Database Error in model customers (models/customers.sql)
  column customer_orders.tickets_sold does not exist
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column customer_orders.tickets_sold does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  column customer_orders.tickets_sold does not exist
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:09:12.730273 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8515ba72-0963-4752-bb00-685c24949c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8d2710>]}
2020-04-27 23:09:12.730524 (Thread-1): 16:09:12 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.75s]
2020-04-27 23:09:12.730677 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:09:12.832358 (MainThread): Using postgres connection "master".
2020-04-27 23:09:12.832649 (MainThread): On master: BEGIN
2020-04-27 23:09:12.873039 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:12.873493 (MainThread): On master: COMMIT
2020-04-27 23:09:12.873688 (MainThread): Using postgres connection "master".
2020-04-27 23:09:12.873836 (MainThread): On master: COMMIT
2020-04-27 23:09:12.913275 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:09:12.914172 (MainThread): 16:09:12 | 
2020-04-27 23:09:12.914449 (MainThread): 16:09:12 | Finished running 3 view models in 5.47s.
2020-04-27 23:09:12.914740 (MainThread): Connection 'master' was left open.
2020-04-27 23:09:12.914909 (MainThread): On master: Close
2020-04-27 23:09:12.915311 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:09:12.915472 (MainThread): On model.order_history.customers: Close
2020-04-27 23:09:12.926303 (MainThread): 
2020-04-27 23:09:12.926532 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 23:09:12.926679 (MainThread): 
2020-04-27 23:09:12.926810 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 23:09:12.926928 (MainThread):   column customer_orders.tickets_sold does not exist
2020-04-27 23:09:12.927037 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:09:12.927155 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-27 23:09:12.927354 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d421710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8e69d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d69e750>]}
2020-04-27 23:09:12.927565 (MainThread): Flushing usage events
2020-04-27 23:09:42.207134 (MainThread): Running with dbt=0.16.1
2020-04-27 23:09:42.282142 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:09:42.283263 (MainThread): Tracking: tracking
2020-04-27 23:09:42.289321 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104614f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104863fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104887a10>]}
2020-04-27 23:09:42.311706 (MainThread): Partial parsing not enabled
2020-04-27 23:09:42.313872 (MainThread): Parsing macros/core.sql
2020-04-27 23:09:42.319702 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:09:42.329499 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:09:42.331664 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:09:42.351392 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:09:42.387795 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:09:42.409536 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:09:42.411504 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:09:42.418019 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:09:42.431165 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:09:42.438567 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:09:42.445150 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:09:42.450481 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:09:42.451885 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:09:42.453298 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:09:42.455076 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:09:42.457207 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:09:42.466507 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:09:42.468983 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:09:42.470129 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:09:42.520410 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:09:42.521709 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:09:42.522691 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:09:42.523848 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:09:42.526235 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:09:42.528654 (MainThread): Parsing macros/relations.sql
2020-04-27 23:09:42.530054 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:09:42.547551 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:09:42.567186 (MainThread): Partial parsing not enabled
2020-04-27 23:09:42.597261 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:09:42.597395 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:42.613925 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:42.614056 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:42.618719 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:42.618918 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:42.752295 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:09:42.754037 (MainThread): 
2020-04-27 23:09:42.754337 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:09:42.754428 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:42.765860 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:09:42.765998 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:09:42.872139 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:09:42.872287 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:09:43.268706 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.40 seconds
2020-04-27 23:09:43.289385 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:43.289628 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:09:43.291473 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:43.291598 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:09:43.329759 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:43.330185 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:43.330448 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:09:43.531532 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.20 seconds
2020-04-27 23:09:43.534936 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:09:43.618549 (MainThread): Using postgres connection "master".
2020-04-27 23:09:43.618735 (MainThread): On master: BEGIN
2020-04-27 23:09:43.979019 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-27 23:09:43.979486 (MainThread): Using postgres connection "master".
2020-04-27 23:09:43.979670 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:09:44.160670 (MainThread): SQL status: SELECT in 0.18 seconds
2020-04-27 23:09:44.238564 (MainThread): On master: ROLLBACK
2020-04-27 23:09:44.297287 (MainThread): Using postgres connection "master".
2020-04-27 23:09:44.297697 (MainThread): On master: BEGIN
2020-04-27 23:09:44.381968 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:09:44.382412 (MainThread): On master: COMMIT
2020-04-27 23:09:44.382701 (MainThread): Using postgres connection "master".
2020-04-27 23:09:44.382880 (MainThread): On master: COMMIT
2020-04-27 23:09:44.428274 (MainThread): SQL status: COMMIT in 0.05 seconds
2020-04-27 23:09:44.429168 (MainThread): 16:09:44 | Concurrency: 1 threads (target='dev')
2020-04-27 23:09:44.429415 (MainThread): 16:09:44 | 
2020-04-27 23:09:44.431578 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 23:09:44.431835 (Thread-1): 16:09:44 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 23:09:44.432297 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:44.432464 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:09:44.432627 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 23:09:44.449117 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 23:09:44.449656 (Thread-1): finished collecting timing info
2020-04-27 23:09:44.494536 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:44.494711 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 23:09:44.571646 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 23:09:44.574497 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:44.574624 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:09:44.614155 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:09:44.617551 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 23:09:44.618254 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:44.618422 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 23:09:45.204362 (Thread-1): SQL status: BEGIN in 0.59 seconds
2020-04-27 23:09:45.204573 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.204691 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 23:09:45.261651 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:09:45.268285 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.268554 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 23:09:45.311713 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:45.316149 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.316308 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 23:09:45.356312 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:45.358041 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:09:45.358248 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.358404 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:09:45.573680 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-27 23:09:45.576525 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.576700 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:09:46.073401 (Thread-1): SQL status: DROP VIEW in 0.50 seconds
2020-04-27 23:09:46.077091 (Thread-1): finished collecting timing info
2020-04-27 23:09:46.077844 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88e9649f-3c60-464d-b114-46296ebe098f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ced710>]}
2020-04-27 23:09:46.078116 (Thread-1): 16:09:46 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.65s]
2020-04-27 23:09:46.078278 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 23:09:46.078458 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 23:09:46.078814 (Thread-1): 16:09:46 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 23:09:46.079365 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:46.079559 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 23:09:46.079694 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 23:09:46.086148 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:09:46.086640 (Thread-1): finished collecting timing info
2020-04-27 23:09:46.095097 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:46.095230 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 23:09:46.380672 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-04-27 23:09:46.384931 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:46.385106 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:09:46.995344 (Thread-1): SQL status: DROP VIEW in 0.61 seconds
2020-04-27 23:09:46.998399 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:09:46.999076 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:46.999230 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 23:09:47.038929 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:47.039253 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.039430 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 23:09:47.102511 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:09:47.107207 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.107366 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 23:09:47.149047 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:47.152266 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.152435 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 23:09:47.197663 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-27 23:09:47.199623 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:09:47.199821 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.199980 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:09:47.507775 (Thread-1): SQL status: COMMIT in 0.31 seconds
2020-04-27 23:09:47.510539 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.510697 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:09:47.916513 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-04-27 23:09:47.920652 (Thread-1): finished collecting timing info
2020-04-27 23:09:47.921541 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88e9649f-3c60-464d-b114-46296ebe098f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ea8a50>]}
2020-04-27 23:09:47.921847 (Thread-1): 16:09:47 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.84s]
2020-04-27 23:09:47.922025 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 23:09:47.922437 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:09:47.922791 (Thread-1): 16:09:47 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 23:09:47.923169 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:09:47.923342 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 23:09:47.923485 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:09:47.932760 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:09:47.933220 (Thread-1): finished collecting timing info
2020-04-27 23:09:47.941667 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:47.941838 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:09:48.214764 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2020-04-27 23:09:48.219080 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:48.219273 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:09:48.775277 (Thread-1): SQL status: DROP VIEW in 0.56 seconds
2020-04-27 23:09:48.778352 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:09:48.778934 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:48.779088 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:09:48.817283 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:48.817599 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:48.817770 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(order_unique_id) as number_of_orders,
        COUNT(order_ticket_unique_id) AS tickets_purchased,
        SUM(amount_gross) AS total_revenue
    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.tickets_purchased, 0) as ticket_purchased,
        cusomter_orders.total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:09:48.864480 (Thread-1): Postgres error: relation "cusomter_orders" does not exist

2020-04-27 23:09:48.864964 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 23:09:48.903461 (Thread-1): finished collecting timing info
2020-04-27 23:09:48.904212 (Thread-1): Database Error in model customers (models/customers.sql)
  relation "cusomter_orders" does not exist
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "cusomter_orders" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  relation "cusomter_orders" does not exist
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:09:48.907006 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88e9649f-3c60-464d-b114-46296ebe098f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a24050>]}
2020-04-27 23:09:48.907303 (Thread-1): 16:09:48 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.98s]
2020-04-27 23:09:48.907484 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:09:49.008978 (MainThread): Using postgres connection "master".
2020-04-27 23:09:49.009202 (MainThread): On master: BEGIN
2020-04-27 23:09:49.047682 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:49.048145 (MainThread): On master: COMMIT
2020-04-27 23:09:49.048438 (MainThread): Using postgres connection "master".
2020-04-27 23:09:49.048594 (MainThread): On master: COMMIT
2020-04-27 23:09:49.086416 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:09:49.087084 (MainThread): 16:09:49 | 
2020-04-27 23:09:49.087332 (MainThread): 16:09:49 | Finished running 3 view models in 6.33s.
2020-04-27 23:09:49.087597 (MainThread): Connection 'master' was left open.
2020-04-27 23:09:49.087822 (MainThread): On master: Close
2020-04-27 23:09:49.088222 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:09:49.088370 (MainThread): On model.order_history.customers: Close
2020-04-27 23:09:49.098153 (MainThread): 
2020-04-27 23:09:49.098330 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 23:09:49.098472 (MainThread): 
2020-04-27 23:09:49.098608 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 23:09:49.098730 (MainThread):   relation "cusomter_orders" does not exist
2020-04-27 23:09:49.098844 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:09:49.098966 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-27 23:09:49.099166 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f00d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104efce90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104efcd90>]}
2020-04-27 23:09:49.099384 (MainThread): Flushing usage events
2020-04-27 23:10:04.454654 (MainThread): Running with dbt=0.16.1
2020-04-27 23:10:04.534255 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:10:04.535408 (MainThread): Tracking: tracking
2020-04-27 23:10:04.541660 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110331a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100d7a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110331510>]}
2020-04-27 23:10:04.570989 (MainThread): Partial parsing not enabled
2020-04-27 23:10:04.573229 (MainThread): Parsing macros/core.sql
2020-04-27 23:10:04.578794 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:10:04.588503 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:10:04.590508 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:10:04.609603 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:10:04.644667 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:10:04.666832 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:10:04.669166 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:10:04.675908 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:10:04.689562 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:10:04.696631 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:10:04.703607 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:10:04.708861 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:10:04.709862 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:10:04.710963 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:10:04.712673 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:10:04.714796 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:10:04.724264 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:10:04.726339 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:10:04.727438 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:10:04.770606 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:10:04.771861 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:10:04.772829 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:10:04.773959 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:10:04.776279 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:10:04.778693 (MainThread): Parsing macros/relations.sql
2020-04-27 23:10:04.780141 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:10:04.798161 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:10:04.819649 (MainThread): Partial parsing not enabled
2020-04-27 23:10:04.855058 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:10:04.855200 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:10:04.872792 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:04.872932 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:10:04.877308 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:04.877407 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:10:05.008215 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:10:05.009687 (MainThread): 
2020-04-27 23:10:05.009970 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:10:05.010056 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:10:05.020538 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:10:05.020676 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:10:05.123984 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:10:05.124155 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:10:05.563543 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.44 seconds
2020-04-27 23:10:05.584186 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:10:05.584431 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:10:05.586289 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:10:05.586416 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:10:05.941047 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.35 seconds
2020-04-27 23:10:05.941339 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:10:05.941512 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:10:06.116554 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.17 seconds
2020-04-27 23:10:06.120345 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:10:06.185541 (MainThread): Using postgres connection "master".
2020-04-27 23:10:06.185799 (MainThread): On master: BEGIN
2020-04-27 23:10:06.555341 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-27 23:10:06.555527 (MainThread): Using postgres connection "master".
2020-04-27 23:10:06.555631 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:10:06.770820 (MainThread): SQL status: SELECT in 0.22 seconds
2020-04-27 23:10:06.844559 (MainThread): On master: ROLLBACK
2020-04-27 23:10:06.885358 (MainThread): Using postgres connection "master".
2020-04-27 23:10:06.885523 (MainThread): On master: BEGIN
2020-04-27 23:10:06.967367 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:10:06.967676 (MainThread): On master: COMMIT
2020-04-27 23:10:06.967864 (MainThread): Using postgres connection "master".
2020-04-27 23:10:06.968022 (MainThread): On master: COMMIT
2020-04-27 23:10:07.010363 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:10:07.011251 (MainThread): 16:10:07 | Concurrency: 1 threads (target='dev')
2020-04-27 23:10:07.011490 (MainThread): 16:10:07 | 
2020-04-27 23:10:07.013745 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 23:10:07.014017 (Thread-1): 16:10:07 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 23:10:07.014407 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.014548 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:10:07.014694 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 23:10:07.031554 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 23:10:07.032092 (Thread-1): finished collecting timing info
2020-04-27 23:10:07.074137 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.074319 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 23:10:07.156614 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 23:10:07.160791 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.160950 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:10:07.202036 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:10:07.205166 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 23:10:07.205874 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.206070 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 23:10:07.246734 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:10:07.247172 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.247442 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 23:10:07.312937 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-27 23:10:07.319239 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.319487 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 23:10:07.363314 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:07.367591 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.367766 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 23:10:07.408954 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:07.410373 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:10:07.410516 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.410625 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:10:07.612393 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-27 23:10:07.616011 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.616186 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:10:07.861344 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-27 23:10:07.864546 (Thread-1): finished collecting timing info
2020-04-27 23:10:07.865319 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '994319a0-36a7-47f6-8bd3-c9a30f1584be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107ab290>]}
2020-04-27 23:10:07.865607 (Thread-1): 16:10:07 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.85s]
2020-04-27 23:10:07.865784 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 23:10:07.865953 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 23:10:07.866215 (Thread-1): 16:10:07 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 23:10:07.866576 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:07.866699 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 23:10:07.866817 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 23:10:07.873452 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:10:07.873951 (Thread-1): finished collecting timing info
2020-04-27 23:10:07.882598 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:07.882782 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 23:10:08.093566 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-27 23:10:08.097351 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.097513 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:10:08.499106 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-27 23:10:08.502284 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:10:08.503051 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.503325 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 23:10:08.544324 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:10:08.544530 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.544645 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 23:10:08.616789 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-27 23:10:08.622592 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.622735 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 23:10:08.664383 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:08.668979 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.669224 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 23:10:08.713596 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:08.715553 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:10:08.715751 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.715910 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:10:09.013014 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-27 23:10:09.016559 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:09.016718 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:10:09.273850 (Thread-1): SQL status: DROP VIEW in 0.26 seconds
2020-04-27 23:10:09.276469 (Thread-1): finished collecting timing info
2020-04-27 23:10:09.277117 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '994319a0-36a7-47f6-8bd3-c9a30f1584be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107ad4d0>]}
2020-04-27 23:10:09.277348 (Thread-1): 16:10:09 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.41s]
2020-04-27 23:10:09.277483 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 23:10:09.277864 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:10:09.278033 (Thread-1): 16:10:09 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 23:10:09.278335 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:10:09.278445 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 23:10:09.278543 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:10:09.286805 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:10:09.287348 (Thread-1): finished collecting timing info
2020-04-27 23:10:09.294298 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.294418 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:10:09.493289 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-27 23:10:09.497087 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.497239 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:10:09.718770 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-27 23:10:09.721609 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:10:09.722240 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.722412 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:10:09.765922 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:10:09.766346 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.766511 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(order_unique_id) as number_of_orders,
        COUNT(order_ticket_unique_id) AS tickets_purchased,
        SUM(amount_gross) AS total_revenue
    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.tickets_purchased, 0) as ticket_purchased,
        customer_orders.total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:10:09.866668 (Thread-1): SQL status: CREATE VIEW in 0.10 seconds
2020-04-27 23:10:09.871544 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.871710 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-27 23:10:09.914597 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:09.916551 (Thread-1): On model.order_history.customers: COMMIT
2020-04-27 23:10:09.916790 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.916969 (Thread-1): On model.order_history.customers: COMMIT
2020-04-27 23:10:10.119759 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-27 23:10:10.156449 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:10.156664 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:10:10.444576 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-04-27 23:10:10.448117 (Thread-1): finished collecting timing info
2020-04-27 23:10:10.448944 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '994319a0-36a7-47f6-8bd3-c9a30f1584be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110321fd0>]}
2020-04-27 23:10:10.449204 (Thread-1): 16:10:10 | 3 of 3 OK created view model data_science.customers.................. [CREATE VIEW in 1.17s]
2020-04-27 23:10:10.449353 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:10:10.490968 (MainThread): Using postgres connection "master".
2020-04-27 23:10:10.491287 (MainThread): On master: BEGIN
2020-04-27 23:10:10.532872 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:10:10.533167 (MainThread): On master: COMMIT
2020-04-27 23:10:10.533344 (MainThread): Using postgres connection "master".
2020-04-27 23:10:10.533511 (MainThread): On master: COMMIT
2020-04-27 23:10:10.574035 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:10:10.574682 (MainThread): 16:10:10 | 
2020-04-27 23:10:10.574907 (MainThread): 16:10:10 | Finished running 3 view models in 5.56s.
2020-04-27 23:10:10.575091 (MainThread): Connection 'master' was left open.
2020-04-27 23:10:10.575237 (MainThread): On master: Close
2020-04-27 23:10:10.575613 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:10:10.575896 (MainThread): On model.order_history.customers: Close
2020-04-27 23:10:10.586826 (MainThread): 
2020-04-27 23:10:10.587160 (MainThread): Completed successfully
2020-04-27 23:10:10.587391 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2020-04-27 23:10:10.587667 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110980bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110979490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107a23d0>]}
2020-04-27 23:10:10.587943 (MainThread): Flushing usage events
2020-04-27 23:50:16.645439 (MainThread): Running with dbt=0.16.1
2020-04-27 23:50:16.761355 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:50:16.762589 (MainThread): Tracking: tracking
2020-04-27 23:50:16.770382 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9db790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc3d450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9e4990>]}
2020-04-27 23:50:16.796341 (MainThread): Partial parsing not enabled
2020-04-27 23:50:16.799832 (MainThread): Parsing macros/core.sql
2020-04-27 23:50:16.806840 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:50:16.816113 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:50:16.819195 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:50:16.839946 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:50:16.874872 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:50:16.897658 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:50:16.900358 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:50:16.907750 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:50:16.921624 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:50:16.929607 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:50:16.937097 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:50:16.943151 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:50:16.945182 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:50:16.947211 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:50:16.950118 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:50:16.953175 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:50:16.967394 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:50:16.970648 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:50:16.972854 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:50:17.017670 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:50:17.019975 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:50:17.021635 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:50:17.023475 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:50:17.026597 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:50:17.029827 (MainThread): Parsing macros/relations.sql
2020-04-27 23:50:17.032054 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:50:17.050487 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:50:17.068912 (MainThread): Partial parsing not enabled
2020-04-27 23:50:17.095879 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:50:17.095982 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:50:17.112692 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:50:17.112796 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:50:17.116830 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:50:17.116921 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:50:17.245787 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:50:17.247756 (MainThread): 
2020-04-27 23:50:17.248058 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:50:17.248148 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:50:17.254038 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:50:17.254404 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:50:17.350389 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:50:17.350529 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:50:17.882978 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-27 23:50:17.908102 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:50:17.908336 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:50:17.910136 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:50:17.910260 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:50:17.952016 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:50:17.952210 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:50:17.952315 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:50:18.062644 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-27 23:50:18.067610 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:50:18.127837 (MainThread): Using postgres connection "master".
2020-04-27 23:50:18.128004 (MainThread): On master: BEGIN
2020-04-27 23:50:18.508744 (MainThread): SQL status: BEGIN in 0.38 seconds
2020-04-27 23:50:18.509174 (MainThread): Using postgres connection "master".
2020-04-27 23:50:18.509451 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:50:18.677203 (MainThread): SQL status: SELECT in 0.17 seconds
2020-04-27 23:50:18.745960 (MainThread): On master: ROLLBACK
2020-04-27 23:50:18.785998 (MainThread): Using postgres connection "master".
2020-04-27 23:50:18.786415 (MainThread): On master: BEGIN
2020-04-27 23:50:18.867943 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:50:18.868406 (MainThread): On master: COMMIT
2020-04-27 23:50:18.868713 (MainThread): Using postgres connection "master".
2020-04-27 23:50:18.868873 (MainThread): On master: COMMIT
2020-04-27 23:50:18.908567 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:50:18.909464 (MainThread): 16:50:18 | Concurrency: 1 threads (target='dev')
2020-04-27 23:50:18.909718 (MainThread): 16:50:18 | 
2020-04-27 23:50:18.914476 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:50:18.914731 (Thread-1): 16:50:18 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-27 23:50:18.915097 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:50:18.915220 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:50:18.915347 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:50:18.934859 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:50:18.935581 (Thread-1): finished collecting timing info
2020-04-27 23:50:18.982429 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:50:18.982595 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:50:19.066528 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 23:50:19.070881 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:50:19.071036 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:50:19.111142 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:50:19.114240 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:50:19.114880 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:50:19.115035 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:50:19.154506 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:50:19.154943 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:50:19.155218 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id  ELSE NULL END) AS number_of_orders,

        COALESCE(COALESCE(CAST( ( SUM(DISTINCT (CAST(FLOOR(COALESCE(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN amount_gross ELSE NULL END,0)*(1000000*1.0)) AS DECIMAL(38,0))) 
        + CAST(STRTOL(LEFT(MD5(CAST(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN order_ticket_unique_id  ELSE NULL END AS VARCHAR)),15),16) AS DECIMAL(38,0))* 1.0e8 
        + CAST(STRTOL(RIGHT(MD5(CAST(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN  order_ticket_unique_id  ELSE NULL END AS VARCHAR)),15),16) AS DECIMAL(38,0)) ) 
        - SUM(DISTINCT CAST(STRTOL(LEFT(MD5(CAST(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN order_ticket_unique_id  ELSE NULL END AS VARCHAR)),15),16) AS DECIMAL(38,0))* 1.0e8 
        + CAST(STRTOL(RIGHT(MD5(CAST(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),15),16) AS DECIMAL(38,0))) ) AS DOUBLE PRECISION) / 
        CAST((1000000*1.0) AS DOUBLE PRECISION), 0), 0) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:50:19.202997 (Thread-1): Postgres error: syntax error at or near "VARCHAR"
LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
                                                              ^

2020-04-27 23:50:19.203309 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 23:50:19.245724 (Thread-1): finished collecting timing info
2020-04-27 23:50:19.246668 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "VARCHAR"
  LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
                                                                ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "VARCHAR"
LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
                                                              ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "VARCHAR"
  LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
                                                                ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:50:19.268858 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9928aeb-e3b1-4d17-9c7d-65c785bbf9d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e281a50>]}
2020-04-27 23:50:19.269160 (Thread-1): 16:50:19 | 1 of 1 ERROR creating view model data_science.customers.............. [ERROR in 0.35s]
2020-04-27 23:50:19.269325 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:50:19.322116 (MainThread): Using postgres connection "master".
2020-04-27 23:50:19.322285 (MainThread): On master: BEGIN
2020-04-27 23:50:19.361981 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:50:19.362446 (MainThread): On master: COMMIT
2020-04-27 23:50:19.362743 (MainThread): Using postgres connection "master".
2020-04-27 23:50:19.362899 (MainThread): On master: COMMIT
2020-04-27 23:50:19.402884 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:50:19.403787 (MainThread): 16:50:19 | 
2020-04-27 23:50:19.404027 (MainThread): 16:50:19 | Finished running 1 view model in 2.16s.
2020-04-27 23:50:19.404231 (MainThread): Connection 'master' was left open.
2020-04-27 23:50:19.404389 (MainThread): On master: Close
2020-04-27 23:50:19.404791 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:50:19.404955 (MainThread): On model.order_history.customers: Close
2020-04-27 23:50:19.409503 (MainThread): 
2020-04-27 23:50:19.409729 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 23:50:19.409876 (MainThread): 
2020-04-27 23:50:19.410014 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 23:50:19.410139 (MainThread):   syntax error at or near "VARCHAR"
2020-04-27 23:50:19.410255 (MainThread):   LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
2020-04-27 23:50:19.410371 (MainThread):                                                                 ^
2020-04-27 23:50:19.410485 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:50:19.410605 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2020-04-27 23:50:19.410801 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e055990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2b3ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e07abd0>]}
2020-04-27 23:50:19.411072 (MainThread): Flushing usage events
2020-04-28 00:23:21.500161 (MainThread): Running with dbt=0.16.1
2020-04-28 00:23:21.592426 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:23:21.593586 (MainThread): Tracking: tracking
2020-04-28 00:23:21.599826 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114efad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111774e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111774ed0>]}
2020-04-28 00:23:21.622401 (MainThread): Partial parsing not enabled
2020-04-28 00:23:21.624667 (MainThread): Parsing macros/core.sql
2020-04-28 00:23:21.630223 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:23:21.639092 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:23:21.641272 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:23:21.659622 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:23:21.693847 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:23:21.715718 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:23:21.718407 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:23:21.725038 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:23:21.738745 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:23:21.746952 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:23:21.758101 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:23:21.764151 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:23:21.765481 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:23:21.767467 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:23:21.769707 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:23:21.772342 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:23:21.782586 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:23:21.784906 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:23:21.786373 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:23:21.828975 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:23:21.831099 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:23:21.832585 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:23:21.834301 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:23:21.836796 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:23:21.839784 (MainThread): Parsing macros/relations.sql
2020-04-28 00:23:21.841586 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:23:21.861297 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:23:21.886238 (MainThread): Partial parsing not enabled
2020-04-28 00:23:21.915161 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:23:21.915302 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:23:21.932628 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:21.932780 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:23:21.937833 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:21.937937 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:23:22.067670 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:23:22.070095 (MainThread): 
2020-04-28 00:23:22.070520 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:23:22.070608 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:23:22.081506 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:23:22.081708 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:23:22.166743 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:23:22.166876 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:23:22.761456 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.59 seconds
2020-04-28 00:23:22.783675 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:23:22.783899 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:23:22.785747 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:23:22.785876 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:23:22.824651 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:22.825082 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:23:22.825355 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:23:22.989705 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.16 seconds
2020-04-28 00:23:22.994735 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:23:23.053766 (MainThread): Using postgres connection "master".
2020-04-28 00:23:23.053933 (MainThread): On master: BEGIN
2020-04-28 00:23:23.440599 (MainThread): SQL status: BEGIN in 0.39 seconds
2020-04-28 00:23:23.441027 (MainThread): Using postgres connection "master".
2020-04-28 00:23:23.441306 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:23:23.837975 (MainThread): SQL status: SELECT in 0.40 seconds
2020-04-28 00:23:23.919064 (MainThread): On master: ROLLBACK
2020-04-28 00:23:23.960495 (MainThread): Using postgres connection "master".
2020-04-28 00:23:23.960904 (MainThread): On master: BEGIN
2020-04-28 00:23:24.041246 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:23:24.041708 (MainThread): On master: COMMIT
2020-04-28 00:23:24.042012 (MainThread): Using postgres connection "master".
2020-04-28 00:23:24.042167 (MainThread): On master: COMMIT
2020-04-28 00:23:24.081652 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:23:24.082459 (MainThread): 17:23:24 | Concurrency: 1 threads (target='dev')
2020-04-28 00:23:24.082706 (MainThread): 17:23:24 | 
2020-04-28 00:23:24.085528 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 00:23:24.085793 (Thread-1): 17:23:24 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-28 00:23:24.086354 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.086494 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:23:24.086637 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 00:23:24.102633 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 00:23:24.103106 (Thread-1): finished collecting timing info
2020-04-28 00:23:24.143828 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.143992 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 00:23:24.222940 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:23:24.226085 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.226221 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:23:24.265779 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:23:24.267740 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 00:23:24.268724 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.268849 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 00:23:24.308065 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:24.308515 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.308827 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 00:23:24.380136 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:23:24.386397 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.386556 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 00:23:24.438716 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 00:23:24.442850 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.443007 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 00:23:24.482493 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:23:24.484527 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:23:24.484730 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.484890 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:23:25.068114 (Thread-1): SQL status: COMMIT in 0.58 seconds
2020-04-28 00:23:25.071638 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:25.071803 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:23:25.303794 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 00:23:25.306633 (Thread-1): finished collecting timing info
2020-04-28 00:23:25.307311 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03f8f154-c088-4874-91c2-b123601dd746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11208e1d0>]}
2020-04-28 00:23:25.307558 (Thread-1): 17:23:25 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.22s]
2020-04-28 00:23:25.307704 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 00:23:25.307947 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-28 00:23:25.308197 (Thread-1): 17:23:25 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-28 00:23:25.308477 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:25.308575 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 00:23:25.308671 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-28 00:23:25.313863 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:23:25.314302 (Thread-1): finished collecting timing info
2020-04-28 00:23:25.322215 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:25.322358 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-28 00:23:25.577077 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-28 00:23:25.581178 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:25.581343 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:23:25.987130 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-04-28 00:23:25.988857 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:23:25.989286 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:25.989400 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-28 00:23:26.028291 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:26.028724 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.028993 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
  );

2020-04-28 00:23:26.091875 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:23:26.097915 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.098071 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-28 00:23:26.141864 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:23:26.146223 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.146381 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-28 00:23:26.186476 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:23:26.188449 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:23:26.188655 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.188818 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:23:26.384059 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 00:23:26.387641 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.387808 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:23:26.908240 (Thread-1): SQL status: DROP VIEW in 0.52 seconds
2020-04-28 00:23:26.912499 (Thread-1): finished collecting timing info
2020-04-28 00:23:26.913343 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03f8f154-c088-4874-91c2-b123601dd746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11192dfd0>]}
2020-04-28 00:23:26.913651 (Thread-1): 17:23:26 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.60s]
2020-04-28 00:23:26.913830 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-28 00:23:26.914363 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:23:26.914659 (Thread-1): 17:23:26 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-28 00:23:26.915093 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:23:26.915259 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-28 00:23:26.915387 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:23:26.924988 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:23:26.925471 (Thread-1): finished collecting timing info
2020-04-28 00:23:26.933096 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:26.933252 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:23:27.122121 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 00:23:27.126326 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.126490 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:23:27.522649 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-28 00:23:27.525714 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:23:27.526336 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.526494 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:23:27.565958 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:27.566400 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.566677 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS number_of_orders,

        SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:23:27.633011 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:23:27.636880 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.637043 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:23:27.676721 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:23:27.677762 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:23:27.677876 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.677969 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:23:27.866039 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-28 00:23:27.903014 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.903232 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:23:28.115203 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 00:23:28.119506 (Thread-1): finished collecting timing info
2020-04-28 00:23:28.120351 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03f8f154-c088-4874-91c2-b123601dd746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111740410>]}
2020-04-28 00:23:28.120658 (Thread-1): 17:23:28 | 3 of 3 OK created view model data_science.customers.................. [CREATE VIEW in 1.21s]
2020-04-28 00:23:28.120838 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:23:28.169358 (MainThread): Using postgres connection "master".
2020-04-28 00:23:28.169690 (MainThread): On master: BEGIN
2020-04-28 00:23:28.211491 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:28.211957 (MainThread): On master: COMMIT
2020-04-28 00:23:28.212262 (MainThread): Using postgres connection "master".
2020-04-28 00:23:28.212419 (MainThread): On master: COMMIT
2020-04-28 00:23:28.251522 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:23:28.252253 (MainThread): 17:23:28 | 
2020-04-28 00:23:28.252491 (MainThread): 17:23:28 | Finished running 3 view models in 6.18s.
2020-04-28 00:23:28.252778 (MainThread): Connection 'master' was left open.
2020-04-28 00:23:28.252985 (MainThread): On master: Close
2020-04-28 00:23:28.254800 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:23:28.255088 (MainThread): On model.order_history.customers: Close
2020-04-28 00:23:28.265606 (MainThread): 
2020-04-28 00:23:28.265804 (MainThread): Completed successfully
2020-04-28 00:23:28.265951 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2020-04-28 00:23:28.266159 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111740610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111da3a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b76890>]}
2020-04-28 00:23:28.266377 (MainThread): Flushing usage events
2020-04-28 00:25:33.579994 (MainThread): Running with dbt=0.16.1
2020-04-28 00:25:33.667882 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:25:33.669030 (MainThread): Tracking: tracking
2020-04-28 00:25:33.676008 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102d8350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11054bdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d386ad0>]}
2020-04-28 00:25:33.700270 (MainThread): Partial parsing not enabled
2020-04-28 00:25:33.702820 (MainThread): Parsing macros/core.sql
2020-04-28 00:25:33.708552 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:25:33.717346 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:25:33.719831 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:25:33.739915 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:25:33.780122 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:25:33.802183 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:25:33.804662 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:25:33.814320 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:25:33.829588 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:25:33.837377 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:25:33.844016 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:25:33.849461 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:25:33.850772 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:25:33.852256 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:25:33.854488 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:25:33.857168 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:25:33.870641 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:25:33.873496 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:25:33.875369 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:25:33.919565 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:25:33.921077 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:25:33.922290 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:25:33.923598 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:25:33.926164 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:25:33.929072 (MainThread): Parsing macros/relations.sql
2020-04-28 00:25:33.930885 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:25:33.950010 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:25:33.968197 (MainThread): Partial parsing not enabled
2020-04-28 00:25:33.996115 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:25:33.996246 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:25:34.013739 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:25:34.013894 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:25:34.019501 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:25:34.019659 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:25:34.155430 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:25:34.157318 (MainThread): 
2020-04-28 00:25:34.157611 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:25:34.157701 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:25:34.162472 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:25:34.162626 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:25:34.247451 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:25:34.247592 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:25:34.736747 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.49 seconds
2020-04-28 00:25:34.756731 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:25:34.757053 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:25:34.759758 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:25:34.759936 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:25:34.803025 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:25:34.803450 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:25:34.803713 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:25:34.915546 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 00:25:34.920485 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:25:34.980118 (MainThread): Using postgres connection "master".
2020-04-28 00:25:34.980292 (MainThread): On master: BEGIN
2020-04-28 00:25:35.341395 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 00:25:35.341826 (MainThread): Using postgres connection "master".
2020-04-28 00:25:35.342086 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:25:35.626798 (MainThread): SQL status: SELECT in 0.28 seconds
2020-04-28 00:25:35.700557 (MainThread): On master: ROLLBACK
2020-04-28 00:25:35.741971 (MainThread): Using postgres connection "master".
2020-04-28 00:25:35.742412 (MainThread): On master: BEGIN
2020-04-28 00:25:35.821385 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:25:35.821829 (MainThread): On master: COMMIT
2020-04-28 00:25:35.822115 (MainThread): Using postgres connection "master".
2020-04-28 00:25:35.822288 (MainThread): On master: COMMIT
2020-04-28 00:25:35.862403 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:25:35.863288 (MainThread): 17:25:35 | Concurrency: 1 threads (target='dev')
2020-04-28 00:25:35.863528 (MainThread): 17:25:35 | 
2020-04-28 00:25:35.867015 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:25:35.867264 (Thread-1): 17:25:35 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 00:25:35.867594 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:25:35.867709 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:25:35.867828 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:25:35.886463 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:25:35.886973 (Thread-1): finished collecting timing info
2020-04-28 00:25:35.927141 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:35.927303 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:25:36.006787 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:25:36.011157 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.011304 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:25:36.050940 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:25:36.053893 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:25:36.054460 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.054609 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:25:36.093259 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:25:36.093459 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.093572 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS number_of_orders,
        SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:25:36.164080 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:25:36.170431 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.170586 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 00:25:36.211053 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:25:36.215467 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.215617 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:25:36.255717 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:25:36.257662 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:25:36.257854 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.258006 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:25:36.455124 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 00:25:36.457368 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.457507 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:25:36.855418 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-28 00:25:36.859932 (Thread-1): finished collecting timing info
2020-04-28 00:25:36.860804 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd09fafe6-4cd8-4f3a-880e-91f67d8f5cec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b98d10>]}
2020-04-28 00:25:36.861119 (Thread-1): 17:25:36 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 0.99s]
2020-04-28 00:25:36.861304 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:25:36.881786 (MainThread): Using postgres connection "master".
2020-04-28 00:25:36.882020 (MainThread): On master: BEGIN
2020-04-28 00:25:36.928658 (MainThread): SQL status: BEGIN in 0.05 seconds
2020-04-28 00:25:36.929088 (MainThread): On master: COMMIT
2020-04-28 00:25:36.929290 (MainThread): Using postgres connection "master".
2020-04-28 00:25:36.929437 (MainThread): On master: COMMIT
2020-04-28 00:25:36.969268 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:25:36.970151 (MainThread): 17:25:36 | 
2020-04-28 00:25:36.970386 (MainThread): 17:25:36 | Finished running 1 view model in 2.81s.
2020-04-28 00:25:36.970585 (MainThread): Connection 'master' was left open.
2020-04-28 00:25:36.970775 (MainThread): On master: Close
2020-04-28 00:25:36.971246 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:25:36.971415 (MainThread): On model.order_history.customers: Close
2020-04-28 00:25:36.975994 (MainThread): 
2020-04-28 00:25:36.976198 (MainThread): Completed successfully
2020-04-28 00:25:36.976357 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 00:25:36.976584 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bcd0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11080b390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de15690>]}
2020-04-28 00:25:36.976814 (MainThread): Flushing usage events
2020-04-28 00:32:55.034258 (MainThread): Running with dbt=0.16.1
2020-04-28 00:32:55.125906 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:32:55.127102 (MainThread): Tracking: tracking
2020-04-28 00:32:55.134601 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c2d210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e83f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c2d190>]}
2020-04-28 00:32:55.159796 (MainThread): Partial parsing not enabled
2020-04-28 00:32:55.163843 (MainThread): Parsing macros/core.sql
2020-04-28 00:32:55.170651 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:32:55.179775 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:32:55.181900 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:32:55.200598 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:32:55.241325 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:32:55.264671 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:32:55.268945 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:32:55.277920 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:32:55.293325 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:32:55.300981 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:32:55.308390 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:32:55.313634 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:32:55.314798 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:32:55.316106 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:32:55.318113 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:32:55.320392 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:32:55.330313 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:32:55.332589 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:32:55.334264 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:32:55.384158 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:32:55.385738 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:32:55.386939 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:32:55.388400 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:32:55.390893 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:32:55.393544 (MainThread): Parsing macros/relations.sql
2020-04-28 00:32:55.395124 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:32:55.413050 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:32:55.436680 (MainThread): Partial parsing not enabled
2020-04-28 00:32:55.465081 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:32:55.465207 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:32:55.482877 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:32:55.483029 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:32:55.487935 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:32:55.488035 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:32:55.618850 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:32:55.621888 (MainThread): 
2020-04-28 00:32:55.622212 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:32:55.622357 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:32:55.627982 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:32:55.628157 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:32:55.723974 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:32:55.724118 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:32:56.251550 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-28 00:32:56.274163 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:32:56.274390 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:32:56.276273 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:32:56.276404 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:32:56.316648 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:32:56.317082 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:32:56.317357 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:32:56.444614 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-28 00:32:56.449656 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:32:56.508908 (MainThread): Using postgres connection "master".
2020-04-28 00:32:56.509074 (MainThread): On master: BEGIN
2020-04-28 00:32:56.852469 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-04-28 00:32:56.852925 (MainThread): Using postgres connection "master".
2020-04-28 00:32:56.853142 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:32:57.133418 (MainThread): SQL status: SELECT in 0.28 seconds
2020-04-28 00:32:57.207289 (MainThread): On master: ROLLBACK
2020-04-28 00:32:57.246580 (MainThread): Using postgres connection "master".
2020-04-28 00:32:57.246991 (MainThread): On master: BEGIN
2020-04-28 00:32:57.322134 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 00:32:57.322444 (MainThread): On master: COMMIT
2020-04-28 00:32:57.322625 (MainThread): Using postgres connection "master".
2020-04-28 00:32:57.322783 (MainThread): On master: COMMIT
2020-04-28 00:32:57.359864 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:32:57.360509 (MainThread): 17:32:57 | Concurrency: 1 threads (target='dev')
2020-04-28 00:32:57.360755 (MainThread): 17:32:57 | 
2020-04-28 00:32:57.364243 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:32:57.364506 (Thread-1): 17:32:57 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 00:32:57.364887 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:32:57.365043 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:32:57.365196 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:32:57.385319 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:32:57.385951 (Thread-1): finished collecting timing info
2020-04-28 00:32:57.433238 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:32:57.433403 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:32:57.515762 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:32:57.520178 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:32:57.520351 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:32:57.561299 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:32:57.564432 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:32:57.565046 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:32:57.565200 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:32:57.604587 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:32:57.604873 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:32:57.605040 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS number_of_orders,

        SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:32:57.665309 (Thread-1): Postgres error: column customer_orders.tickets_sold_no_comps does not exist

2020-04-28 00:32:57.665747 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 00:32:57.705977 (Thread-1): finished collecting timing info
2020-04-28 00:32:57.707029 (Thread-1): Database Error in model customers (models/customers.sql)
  column customer_orders.tickets_sold_no_comps does not exist
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column customer_orders.tickets_sold_no_comps does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  column customer_orders.tickets_sold_no_comps does not exist
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 00:32:57.720260 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9fa13076-6cc3-4e6a-87e1-791b5c9dece2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4b1710>]}
2020-04-28 00:32:57.720599 (Thread-1): 17:32:57 | 1 of 1 ERROR creating view model data_science.customers.............. [ERROR in 0.36s]
2020-04-28 00:32:57.720798 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:32:57.773506 (MainThread): Using postgres connection "master".
2020-04-28 00:32:57.773705 (MainThread): On master: BEGIN
2020-04-28 00:32:57.812718 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:32:57.813184 (MainThread): On master: COMMIT
2020-04-28 00:32:57.813484 (MainThread): Using postgres connection "master".
2020-04-28 00:32:57.813643 (MainThread): On master: COMMIT
2020-04-28 00:32:57.851289 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:32:57.852179 (MainThread): 17:32:57 | 
2020-04-28 00:32:57.852415 (MainThread): 17:32:57 | Finished running 1 view model in 2.23s.
2020-04-28 00:32:57.852635 (MainThread): Connection 'master' was left open.
2020-04-28 00:32:57.852797 (MainThread): On master: Close
2020-04-28 00:32:57.853192 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:32:57.853356 (MainThread): On model.order_history.customers: Close
2020-04-28 00:32:57.858224 (MainThread): 
2020-04-28 00:32:57.858441 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 00:32:57.858591 (MainThread): 
2020-04-28 00:32:57.858731 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 00:32:57.858858 (MainThread):   column customer_orders.tickets_sold_no_comps does not exist
2020-04-28 00:32:57.858976 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 00:32:57.859099 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2020-04-28 00:32:57.859308 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2c6390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4b9d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a293450>]}
2020-04-28 00:32:57.859528 (MainThread): Flushing usage events
2020-04-28 00:33:06.959211 (MainThread): Running with dbt=0.16.1
2020-04-28 00:33:07.036671 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:33:07.037581 (MainThread): Tracking: tracking
2020-04-28 00:33:07.043441 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e49f550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e49f1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e49f310>]}
2020-04-28 00:33:07.066345 (MainThread): Partial parsing not enabled
2020-04-28 00:33:07.068540 (MainThread): Parsing macros/core.sql
2020-04-28 00:33:07.074096 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:33:07.083929 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:33:07.086161 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:33:07.105904 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:33:07.140783 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:33:07.163512 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:33:07.165696 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:33:07.172290 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:33:07.185625 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:33:07.192821 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:33:07.199408 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:33:07.204630 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:33:07.205634 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:33:07.206767 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:33:07.208524 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:33:07.210710 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:33:07.220616 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:33:07.222807 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:33:07.223962 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:33:07.273459 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:33:07.274773 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:33:07.275777 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:33:07.276961 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:33:07.279402 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:33:07.281965 (MainThread): Parsing macros/relations.sql
2020-04-28 00:33:07.283436 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:33:07.301040 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:33:07.319598 (MainThread): Partial parsing not enabled
2020-04-28 00:33:07.347099 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:33:07.347218 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:33:07.363665 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:33:07.363763 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:33:07.367804 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:33:07.367894 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:33:07.493834 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:33:07.495312 (MainThread): 
2020-04-28 00:33:07.495584 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:33:07.495671 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:33:07.499855 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:33:07.499959 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:33:07.603326 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:33:07.603463 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:33:07.976586 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.37 seconds
2020-04-28 00:33:07.995695 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:33:07.995900 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:33:07.997592 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:33:07.997711 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:33:08.038138 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:33:08.038571 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:33:08.038774 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:33:08.158188 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-28 00:33:08.162510 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:33:08.225881 (MainThread): Using postgres connection "master".
2020-04-28 00:33:08.226052 (MainThread): On master: BEGIN
2020-04-28 00:33:08.586492 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 00:33:08.586925 (MainThread): Using postgres connection "master".
2020-04-28 00:33:08.587109 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:33:08.743813 (MainThread): SQL status: SELECT in 0.16 seconds
2020-04-28 00:33:08.818151 (MainThread): On master: ROLLBACK
2020-04-28 00:33:08.858377 (MainThread): Using postgres connection "master".
2020-04-28 00:33:08.858641 (MainThread): On master: BEGIN
2020-04-28 00:33:08.940143 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:33:08.940391 (MainThread): On master: COMMIT
2020-04-28 00:33:08.940532 (MainThread): Using postgres connection "master".
2020-04-28 00:33:08.940657 (MainThread): On master: COMMIT
2020-04-28 00:33:08.980917 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:33:08.981509 (MainThread): 17:33:08 | Concurrency: 1 threads (target='dev')
2020-04-28 00:33:08.981683 (MainThread): 17:33:08 | 
2020-04-28 00:33:08.984102 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:33:08.984339 (Thread-1): 17:33:08 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 00:33:08.984665 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:33:08.984773 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:33:08.984891 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:33:09.002901 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:33:09.003797 (Thread-1): finished collecting timing info
2020-04-28 00:33:09.044609 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.044776 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:33:09.126592 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:33:09.130953 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.131109 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:33:09.171596 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:33:09.174393 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:33:09.174967 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.175123 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:33:09.216365 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:33:09.216596 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.216732 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS number_of_orders,

        SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        -- coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:33:09.287891 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:33:09.293206 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.293353 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 00:34:20.193670 (Thread-1): SQL status: ALTER TABLE in 70.90 seconds
2020-04-28 00:34:20.197272 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:34:20.197455 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:34:20.489068 (Thread-1): SQL status: ALTER TABLE in 0.29 seconds
2020-04-28 00:34:20.490266 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:34:20.490401 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:34:20.490506 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:34:20.815324 (Thread-1): SQL status: COMMIT in 0.32 seconds
2020-04-28 00:34:20.817415 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:34:20.817552 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:34:21.040850 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 00:34:21.045040 (Thread-1): finished collecting timing info
2020-04-28 00:34:21.045947 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd843c344-48ef-4584-abf4-972615d7ec9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed31850>]}
2020-04-28 00:34:21.046275 (Thread-1): 17:34:21 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 72.06s]
2020-04-28 00:34:21.046469 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:34:21.119779 (MainThread): Using postgres connection "master".
2020-04-28 00:34:21.119964 (MainThread): On master: BEGIN
2020-04-28 00:34:21.173254 (MainThread): SQL status: BEGIN in 0.05 seconds
2020-04-28 00:34:21.173462 (MainThread): On master: COMMIT
2020-04-28 00:34:21.173568 (MainThread): Using postgres connection "master".
2020-04-28 00:34:21.173665 (MainThread): On master: COMMIT
2020-04-28 00:34:21.214486 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:34:21.215007 (MainThread): 17:34:21 | 
2020-04-28 00:34:21.215196 (MainThread): 17:34:21 | Finished running 1 view model in 73.72s.
2020-04-28 00:34:21.215357 (MainThread): Connection 'master' was left open.
2020-04-28 00:34:21.215481 (MainThread): On master: Close
2020-04-28 00:34:21.215875 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:34:21.216037 (MainThread): On model.order_history.customers: Close
2020-04-28 00:34:21.220413 (MainThread): 
2020-04-28 00:34:21.220610 (MainThread): Completed successfully
2020-04-28 00:34:21.220757 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 00:34:21.220963 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb07d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb07e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed63c10>]}
2020-04-28 00:34:21.221173 (MainThread): Flushing usage events
2020-04-28 00:42:53.258082 (MainThread): Running with dbt=0.16.1
2020-04-28 00:42:53.354125 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:42:53.355316 (MainThread): Tracking: tracking
2020-04-28 00:42:53.363048 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051c0310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051df410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105422190>]}
2020-04-28 00:42:53.383061 (MainThread): Partial parsing not enabled
2020-04-28 00:42:53.385242 (MainThread): Parsing macros/core.sql
2020-04-28 00:42:53.390614 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:42:53.399622 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:42:53.401813 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:42:53.420258 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:42:53.454389 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:42:53.476231 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:42:53.478376 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:42:53.485045 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:42:53.498161 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:42:53.505204 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:42:53.511789 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:42:53.517038 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:42:53.518188 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:42:53.520023 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:42:53.521914 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:42:53.524249 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:42:53.533556 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:42:53.535767 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:42:53.537493 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:42:53.580171 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:42:53.582017 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:42:53.583221 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:42:53.584534 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:42:53.587090 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:42:53.589941 (MainThread): Parsing macros/relations.sql
2020-04-28 00:42:53.591543 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:42:53.608232 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:42:53.625646 (MainThread): Partial parsing not enabled
2020-04-28 00:42:53.656466 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:42:53.656645 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:42:53.678821 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:53.678948 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:42:53.683072 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:53.683165 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:42:53.814281 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:42:53.817268 (MainThread): 
2020-04-28 00:42:53.817597 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:42:53.817685 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:42:53.828383 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:42:53.828532 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:42:53.920225 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:42:53.920375 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:42:54.466529 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.55 seconds
2020-04-28 00:42:54.488582 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:42:54.488815 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:42:54.490622 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:42:54.490749 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:42:54.563060 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.07 seconds
2020-04-28 00:42:54.563487 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:42:54.563721 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:42:54.674945 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 00:42:54.678593 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:42:54.733157 (MainThread): Using postgres connection "master".
2020-04-28 00:42:54.733302 (MainThread): On master: BEGIN
2020-04-28 00:42:55.093951 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 00:42:55.094377 (MainThread): Using postgres connection "master".
2020-04-28 00:42:55.094638 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:42:55.271753 (MainThread): SQL status: SELECT in 0.18 seconds
2020-04-28 00:42:55.343179 (MainThread): On master: ROLLBACK
2020-04-28 00:42:55.381593 (MainThread): Using postgres connection "master".
2020-04-28 00:42:55.381842 (MainThread): On master: BEGIN
2020-04-28 00:42:55.459673 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:42:55.459976 (MainThread): On master: COMMIT
2020-04-28 00:42:55.460153 (MainThread): Using postgres connection "master".
2020-04-28 00:42:55.460308 (MainThread): On master: COMMIT
2020-04-28 00:42:55.499484 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:42:55.500400 (MainThread): 17:42:55 | Concurrency: 1 threads (target='dev')
2020-04-28 00:42:55.500639 (MainThread): 17:42:55 | 
2020-04-28 00:42:55.503624 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 00:42:55.503892 (Thread-1): 17:42:55 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-28 00:42:55.504294 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.504424 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:42:55.504551 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 00:42:55.520503 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 00:42:55.520981 (Thread-1): finished collecting timing info
2020-04-28 00:42:55.562152 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.562319 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 00:42:55.640509 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:42:55.644862 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.645015 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:42:55.683835 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:42:55.686438 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 00:42:55.687064 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.687223 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 00:42:55.725122 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:42:55.725385 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.725553 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 00:42:55.787176 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:42:55.793501 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.793675 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 00:42:55.836405 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:42:55.840785 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.840945 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 00:42:55.904239 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 00:42:55.906171 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:42:55.906360 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.906517 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:42:56.127299 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 00:42:56.130767 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:56.130925 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:42:56.453980 (Thread-1): SQL status: DROP VIEW in 0.32 seconds
2020-04-28 00:42:56.458458 (Thread-1): finished collecting timing info
2020-04-28 00:42:56.459320 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0d27138a-a2dc-4591-874e-0703917ecc8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10582c110>]}
2020-04-28 00:42:56.459634 (Thread-1): 17:42:56 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.95s]
2020-04-28 00:42:56.459827 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 00:42:56.460016 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-28 00:42:56.460299 (Thread-1): 17:42:56 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-28 00:42:56.460670 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.460802 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 00:42:56.460932 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-28 00:42:56.466644 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:42:56.467059 (Thread-1): finished collecting timing info
2020-04-28 00:42:56.474399 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.474511 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-28 00:42:56.683814 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 00:42:56.687984 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.688132 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:42:56.885999 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 00:42:56.889047 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:42:56.889653 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.889807 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-28 00:42:56.928882 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:42:56.929313 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.929582 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 00:42:56.996029 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:42:57.002300 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:57.002450 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-28 00:42:57.045596 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:42:57.049975 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:57.050192 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-28 00:42:57.089017 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:42:57.090755 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:42:57.090928 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:57.091073 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:42:57.310237 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 00:42:57.313780 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:57.313941 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:42:57.507229 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 00:42:57.511498 (Thread-1): finished collecting timing info
2020-04-28 00:42:57.512424 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0d27138a-a2dc-4591-874e-0703917ecc8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a91f90>]}
2020-04-28 00:42:57.512735 (Thread-1): 17:42:57 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.05s]
2020-04-28 00:42:57.512913 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-28 00:42:57.513441 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:42:57.513766 (Thread-1): 17:42:57 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-28 00:42:57.514216 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:42:57.514357 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-28 00:42:57.514469 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:42:57.523366 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:42:57.523813 (Thread-1): finished collecting timing info
2020-04-28 00:42:57.531195 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:57.531377 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:42:57.728795 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 00:42:57.732944 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:57.733095 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:42:57.918307 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 00:42:57.921448 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:42:57.922055 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:57.922209 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:42:57.961280 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:42:57.961705 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:57.961980 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,

        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS total_revenue

        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT order_unique_id) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        -- coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:42:58.027093 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:42:58.032387 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:58.032535 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:42:58.072843 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:42:58.074778 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:42:58.074968 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:58.075121 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:42:58.373113 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-28 00:42:58.410242 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:58.410453 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:42:58.613414 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 00:42:58.617728 (Thread-1): finished collecting timing info
2020-04-28 00:42:58.618574 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0d27138a-a2dc-4591-874e-0703917ecc8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10584a3d0>]}
2020-04-28 00:42:58.618897 (Thread-1): 17:42:58 | 3 of 3 OK created view model data_science.customers.................. [CREATE VIEW in 1.10s]
2020-04-28 00:42:58.619079 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:42:58.725177 (MainThread): Using postgres connection "master".
2020-04-28 00:42:58.725505 (MainThread): On master: BEGIN
2020-04-28 00:42:58.764233 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:42:58.764683 (MainThread): On master: COMMIT
2020-04-28 00:42:58.764957 (MainThread): Using postgres connection "master".
2020-04-28 00:42:58.765137 (MainThread): On master: COMMIT
2020-04-28 00:42:58.804005 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:42:58.804467 (MainThread): 17:42:58 | 
2020-04-28 00:42:58.804625 (MainThread): 17:42:58 | Finished running 3 view models in 4.99s.
2020-04-28 00:42:58.804754 (MainThread): Connection 'master' was left open.
2020-04-28 00:42:58.804855 (MainThread): On master: Close
2020-04-28 00:42:58.805130 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:42:58.805233 (MainThread): On model.order_history.customers: Close
2020-04-28 00:42:58.816886 (MainThread): 
2020-04-28 00:42:58.817203 (MainThread): Completed successfully
2020-04-28 00:42:58.817413 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2020-04-28 00:42:58.817734 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a73250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a73fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105477690>]}
2020-04-28 00:42:58.818072 (MainThread): Flushing usage events
2020-04-28 00:48:52.912809 (MainThread): Running with dbt=0.16.1
2020-04-28 00:48:53.004099 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:48:53.005534 (MainThread): Tracking: tracking
2020-04-28 00:48:53.012282 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112958650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bdfed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bcbe50>]}
2020-04-28 00:48:53.037605 (MainThread): Partial parsing not enabled
2020-04-28 00:48:53.041446 (MainThread): Parsing macros/core.sql
2020-04-28 00:48:53.048037 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:48:53.056771 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:48:53.059019 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:48:53.079524 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:48:53.119397 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:48:53.141249 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:48:53.143701 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:48:53.151420 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:48:53.165235 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:48:53.172548 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:48:53.179312 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:48:53.184786 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:48:53.186151 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:48:53.187710 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:48:53.189873 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:48:53.192503 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:48:53.202588 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:48:53.204862 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:48:53.206745 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:48:53.251516 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:48:53.254366 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:48:53.256286 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:48:53.258401 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:48:53.261748 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:48:53.264970 (MainThread): Parsing macros/relations.sql
2020-04-28 00:48:53.266857 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:48:53.286063 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:48:53.306139 (MainThread): Partial parsing not enabled
2020-04-28 00:48:53.334946 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:48:53.335091 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:48:53.352922 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:53.353062 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:48:53.357989 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:53.358124 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:48:53.496191 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:48:53.498415 (MainThread): 
2020-04-28 00:48:53.498751 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:48:53.498842 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:48:53.510437 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:48:53.510560 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:48:53.599081 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:48:53.599224 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:48:54.132937 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-28 00:48:54.150572 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:48:54.150709 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:48:54.152230 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:48:54.152338 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:48:54.192769 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:48:54.193197 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:48:54.193479 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:48:54.325026 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-28 00:48:54.329688 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:48:54.389426 (MainThread): Using postgres connection "master".
2020-04-28 00:48:54.389595 (MainThread): On master: BEGIN
2020-04-28 00:48:54.772593 (MainThread): SQL status: BEGIN in 0.38 seconds
2020-04-28 00:48:54.772850 (MainThread): Using postgres connection "master".
2020-04-28 00:48:54.772983 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:48:54.950677 (MainThread): SQL status: SELECT in 0.18 seconds
2020-04-28 00:48:55.025328 (MainThread): On master: ROLLBACK
2020-04-28 00:48:55.068219 (MainThread): Using postgres connection "master".
2020-04-28 00:48:55.068634 (MainThread): On master: BEGIN
2020-04-28 00:48:55.163260 (MainThread): SQL status: BEGIN in 0.09 seconds
2020-04-28 00:48:55.163467 (MainThread): On master: COMMIT
2020-04-28 00:48:55.163581 (MainThread): Using postgres connection "master".
2020-04-28 00:48:55.163676 (MainThread): On master: COMMIT
2020-04-28 00:48:55.206261 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:48:55.207152 (MainThread): 17:48:55 | Concurrency: 1 threads (target='dev')
2020-04-28 00:48:55.207408 (MainThread): 17:48:55 | 
2020-04-28 00:48:55.210590 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 00:48:55.210851 (Thread-1): 17:48:55 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-28 00:48:55.211223 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.211345 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:48:55.211473 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 00:48:55.227335 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 00:48:55.227791 (Thread-1): finished collecting timing info
2020-04-28 00:48:55.268384 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.268556 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 00:48:55.350507 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:48:55.354765 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.354922 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:48:55.394551 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:48:55.397230 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 00:48:55.399040 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.399333 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 00:48:55.438586 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:48:55.439011 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.439277 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 00:48:55.498661 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:48:55.502966 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.503108 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 00:48:55.543018 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:48:55.546681 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.546852 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 00:48:55.602628 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 00:48:55.604594 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:48:55.604791 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.604949 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:48:55.874692 (Thread-1): SQL status: COMMIT in 0.27 seconds
2020-04-28 00:48:55.877985 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.878168 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:48:56.259690 (Thread-1): SQL status: DROP VIEW in 0.38 seconds
2020-04-28 00:48:56.264175 (Thread-1): finished collecting timing info
2020-04-28 00:48:56.265048 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '78a106d7-f913-459a-889b-1104fcd36168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1130263d0>]}
2020-04-28 00:48:56.265375 (Thread-1): 17:48:56 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.05s]
2020-04-28 00:48:56.265568 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 00:48:56.265779 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-28 00:48:56.266066 (Thread-1): 17:48:56 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-28 00:48:56.266776 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:56.266944 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 00:48:56.267095 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-28 00:48:56.273237 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:48:56.273695 (Thread-1): finished collecting timing info
2020-04-28 00:48:56.282252 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:56.282393 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-28 00:48:56.684762 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-28 00:48:56.688301 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:56.688453 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:48:57.078966 (Thread-1): SQL status: DROP VIEW in 0.39 seconds
2020-04-28 00:48:57.082007 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:48:57.082636 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:57.082796 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-28 00:48:57.122926 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:48:57.123369 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:57.123692 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 00:48:57.183861 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:48:57.188481 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:57.188627 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-28 00:49:19.463331 (Thread-1): SQL status: ALTER TABLE in 22.27 seconds
2020-04-28 00:49:19.467414 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:49:19.467600 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-28 00:49:19.508388 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:49:19.509646 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:49:19.509796 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:49:19.509906 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:49:19.990916 (Thread-1): SQL status: COMMIT in 0.48 seconds
2020-04-28 00:49:19.993733 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:49:19.993890 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:49:20.214872 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 00:49:20.218448 (Thread-1): finished collecting timing info
2020-04-28 00:49:20.219298 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '78a106d7-f913-459a-889b-1104fcd36168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113209450>]}
2020-04-28 00:49:20.219553 (Thread-1): 17:49:20 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 23.95s]
2020-04-28 00:49:20.219705 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-28 00:49:20.220121 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:49:20.220322 (Thread-1): 17:49:20 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-28 00:49:20.220876 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:49:20.221249 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-28 00:49:20.221403 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:49:20.230842 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:49:20.231304 (Thread-1): finished collecting timing info
2020-04-28 00:49:20.238385 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:49:20.238520 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:49:20.428620 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 00:49:20.432831 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:49:20.432984 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:49:20.664557 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 00:49:20.667800 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:49:20.668380 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:49:20.668550 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:49:20.707995 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:49:20.708324 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:49:20.708553 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,

        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN total_revenue ELSE NULL END) AS total_revenue

        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT total_revenue) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        -- coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:49:20.754271 (Thread-1): Postgres error: column "total_revenue" does not exist in order_tickets

2020-04-28 00:49:20.754479 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 00:49:20.793579 (Thread-1): finished collecting timing info
2020-04-28 00:49:20.794421 (Thread-1): Database Error in model customers (models/customers.sql)
  column "total_revenue" does not exist in order_tickets
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column "total_revenue" does not exist in order_tickets


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  column "total_revenue" does not exist in order_tickets
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 00:49:20.802581 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '78a106d7-f913-459a-889b-1104fcd36168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113273850>]}
2020-04-28 00:49:20.802932 (Thread-1): 17:49:20 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.58s]
2020-04-28 00:49:20.803120 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:49:20.828893 (MainThread): Using postgres connection "master".
2020-04-28 00:49:20.829089 (MainThread): On master: BEGIN
2020-04-28 00:49:20.871821 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:49:20.872135 (MainThread): On master: COMMIT
2020-04-28 00:49:20.872316 (MainThread): Using postgres connection "master".
2020-04-28 00:49:20.872484 (MainThread): On master: COMMIT
2020-04-28 00:49:20.914843 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:49:20.915423 (MainThread): 17:49:20 | 
2020-04-28 00:49:20.915634 (MainThread): 17:49:20 | Finished running 3 view models in 27.42s.
2020-04-28 00:49:20.916031 (MainThread): Connection 'master' was left open.
2020-04-28 00:49:20.916208 (MainThread): On master: Close
2020-04-28 00:49:20.916851 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:49:20.917002 (MainThread): On model.order_history.customers: Close
2020-04-28 00:49:20.929505 (MainThread): 
2020-04-28 00:49:20.929754 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 00:49:20.929914 (MainThread): 
2020-04-28 00:49:20.930057 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 00:49:20.930189 (MainThread):   column "total_revenue" does not exist in order_tickets
2020-04-28 00:49:20.930310 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 00:49:20.930456 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-28 00:49:20.930675 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113013f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112fddb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113022290>]}
2020-04-28 00:49:20.930909 (MainThread): Flushing usage events
2020-04-28 00:50:05.046587 (MainThread): Running with dbt=0.16.1
2020-04-28 00:50:05.131261 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:50:05.132720 (MainThread): Tracking: tracking
2020-04-28 00:50:05.139296 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4cac50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d723e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d723f10>]}
2020-04-28 00:50:05.162630 (MainThread): Partial parsing not enabled
2020-04-28 00:50:05.165274 (MainThread): Parsing macros/core.sql
2020-04-28 00:50:05.171301 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:50:05.180167 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:50:05.182330 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:50:05.201050 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:50:05.236336 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:50:05.263860 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:50:05.266038 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:50:05.272845 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:50:05.285888 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:50:05.293482 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:50:05.300053 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:50:05.305213 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:50:05.306389 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:50:05.307737 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:50:05.309768 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:50:05.312468 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:50:05.322724 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:50:05.325575 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:50:05.327335 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:50:05.377137 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:50:05.379534 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:50:05.381503 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:50:05.383305 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:50:05.387397 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:50:05.391087 (MainThread): Parsing macros/relations.sql
2020-04-28 00:50:05.393105 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:50:05.413892 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:50:05.434119 (MainThread): Partial parsing not enabled
2020-04-28 00:50:05.461424 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:50:05.461526 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:50:05.478791 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:50:05.478926 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:50:05.483843 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:50:05.483946 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:50:05.624837 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:50:05.626331 (MainThread): 
2020-04-28 00:50:05.626624 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:50:05.626714 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:50:05.631725 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:50:05.631953 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:50:05.717969 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:50:05.718112 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:50:06.186179 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.47 seconds
2020-04-28 00:50:06.208109 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:50:06.208334 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:50:06.210151 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:50:06.210276 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:50:06.283132 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.07 seconds
2020-04-28 00:50:06.283345 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:50:06.283463 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:50:06.413881 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-28 00:50:06.417825 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:50:06.479987 (MainThread): Using postgres connection "master".
2020-04-28 00:50:06.480162 (MainThread): On master: BEGIN
2020-04-28 00:50:06.826552 (MainThread): SQL status: BEGIN in 0.35 seconds
2020-04-28 00:50:06.826977 (MainThread): Using postgres connection "master".
2020-04-28 00:50:06.827247 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:50:07.121780 (MainThread): SQL status: SELECT in 0.29 seconds
2020-04-28 00:50:07.192126 (MainThread): On master: ROLLBACK
2020-04-28 00:50:07.231048 (MainThread): Using postgres connection "master".
2020-04-28 00:50:07.231454 (MainThread): On master: BEGIN
2020-04-28 00:50:07.308625 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:50:07.308840 (MainThread): On master: COMMIT
2020-04-28 00:50:07.308958 (MainThread): Using postgres connection "master".
2020-04-28 00:50:07.309060 (MainThread): On master: COMMIT
2020-04-28 00:50:07.347266 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:50:07.347686 (MainThread): 17:50:07 | Concurrency: 1 threads (target='dev')
2020-04-28 00:50:07.347832 (MainThread): 17:50:07 | 
2020-04-28 00:50:07.350038 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:50:07.350231 (Thread-1): 17:50:07 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 00:50:07.350524 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:50:07.350626 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:50:07.350730 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:50:07.367595 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:50:07.368024 (Thread-1): finished collecting timing info
2020-04-28 00:50:07.405612 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.405773 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:50:07.484704 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:50:07.488713 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.488882 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:50:07.528789 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:50:07.531879 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:50:07.532510 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.532667 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:50:07.571388 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:50:07.571819 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.572089 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,

        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN amount_gross ELSE NULL END) AS total_revenue

        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        -- coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:50:07.642892 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:50:07.645953 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.646094 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:50:07.686487 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:50:07.687573 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:50:07.687691 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.687786 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:50:07.932601 (Thread-1): SQL status: COMMIT in 0.24 seconds
2020-04-28 00:50:07.935999 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.936173 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:50:08.555104 (Thread-1): SQL status: DROP VIEW in 0.62 seconds
2020-04-28 00:50:08.559431 (Thread-1): finished collecting timing info
2020-04-28 00:50:08.560294 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9553c5cc-1be7-4cc9-8cf9-ce4fb95c6579', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db36850>]}
2020-04-28 00:50:08.560614 (Thread-1): 17:50:08 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 1.21s]
2020-04-28 00:50:08.560805 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:50:08.586457 (MainThread): Using postgres connection "master".
2020-04-28 00:50:08.586756 (MainThread): On master: BEGIN
2020-04-28 00:50:08.625044 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:50:08.625376 (MainThread): On master: COMMIT
2020-04-28 00:50:08.625547 (MainThread): Using postgres connection "master".
2020-04-28 00:50:08.625701 (MainThread): On master: COMMIT
2020-04-28 00:50:08.686292 (MainThread): SQL status: COMMIT in 0.06 seconds
2020-04-28 00:50:08.686740 (MainThread): 17:50:08 | 
2020-04-28 00:50:08.686898 (MainThread): 17:50:08 | Finished running 1 view model in 3.06s.
2020-04-28 00:50:08.687033 (MainThread): Connection 'master' was left open.
2020-04-28 00:50:08.687139 (MainThread): On master: Close
2020-04-28 00:50:08.687417 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:50:08.687524 (MainThread): On model.order_history.customers: Close
2020-04-28 00:50:08.691515 (MainThread): 
2020-04-28 00:50:08.691695 (MainThread): Completed successfully
2020-04-28 00:50:08.691838 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 00:50:08.692045 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd6c590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8b7e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd2ef50>]}
2020-04-28 00:50:08.692255 (MainThread): Flushing usage events
2020-04-28 01:00:32.034299 (MainThread): Running with dbt=0.16.1
2020-04-28 01:00:32.109420 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 01:00:32.110171 (MainThread): Tracking: tracking
2020-04-28 01:00:32.115645 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a076d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c77fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a07d90>]}
2020-04-28 01:00:32.138033 (MainThread): Partial parsing not enabled
2020-04-28 01:00:32.140255 (MainThread): Parsing macros/core.sql
2020-04-28 01:00:32.145726 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 01:00:32.154657 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 01:00:32.156525 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 01:00:32.175184 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 01:00:32.209856 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 01:00:32.231709 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 01:00:32.233684 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 01:00:32.240502 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 01:00:32.253344 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 01:00:32.260355 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 01:00:32.266797 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 01:00:32.272026 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 01:00:32.273015 (MainThread): Parsing macros/etc/query.sql
2020-04-28 01:00:32.274575 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 01:00:32.276434 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 01:00:32.278832 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 01:00:32.289766 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 01:00:32.292209 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 01:00:32.293473 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 01:00:32.341269 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 01:00:32.342523 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 01:00:32.343529 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 01:00:32.344662 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 01:00:32.346988 (MainThread): Parsing macros/catalog.sql
2020-04-28 01:00:32.349394 (MainThread): Parsing macros/relations.sql
2020-04-28 01:00:32.350795 (MainThread): Parsing macros/adapters.sql
2020-04-28 01:00:32.368042 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 01:00:32.386363 (MainThread): Partial parsing not enabled
2020-04-28 01:00:32.416368 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 01:00:32.416506 (MainThread): Opening a new connection, currently in state init
2020-04-28 01:00:32.432999 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 01:00:32.433112 (MainThread): Opening a new connection, currently in state init
2020-04-28 01:00:32.437381 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 01:00:32.437478 (MainThread): Opening a new connection, currently in state init
2020-04-28 01:00:32.563944 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 01:00:32.565400 (MainThread): 
2020-04-28 01:00:32.565672 (MainThread): Acquiring new postgres connection "master".
2020-04-28 01:00:32.565756 (MainThread): Opening a new connection, currently in state init
2020-04-28 01:00:32.570091 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 01:00:32.570196 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 01:00:32.666109 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 01:00:32.666242 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 01:00:33.206200 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.54 seconds
2020-04-28 01:00:33.226835 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 01:00:33.226971 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 01:00:33.228766 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 01:00:33.228893 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 01:00:33.271414 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 01:00:33.271710 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 01:00:33.271885 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 01:00:33.412061 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.14 seconds
2020-04-28 01:00:33.417105 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 01:00:33.477924 (MainThread): Using postgres connection "master".
2020-04-28 01:00:33.478093 (MainThread): On master: BEGIN
2020-04-28 01:00:33.836369 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 01:00:33.836648 (MainThread): Using postgres connection "master".
2020-04-28 01:00:33.836806 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 01:00:34.113307 (MainThread): SQL status: SELECT in 0.28 seconds
2020-04-28 01:00:34.188440 (MainThread): On master: ROLLBACK
2020-04-28 01:00:34.227860 (MainThread): Using postgres connection "master".
2020-04-28 01:00:34.228265 (MainThread): On master: BEGIN
2020-04-28 01:00:34.306848 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 01:00:34.307141 (MainThread): On master: COMMIT
2020-04-28 01:00:34.307314 (MainThread): Using postgres connection "master".
2020-04-28 01:00:34.307466 (MainThread): On master: COMMIT
2020-04-28 01:00:34.346464 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 01:00:34.346996 (MainThread): 18:00:34 | Concurrency: 1 threads (target='dev')
2020-04-28 01:00:34.347246 (MainThread): 18:00:34 | 
2020-04-28 01:00:34.350713 (Thread-1): Began running node model.order_history.customers
2020-04-28 01:00:34.351037 (Thread-1): 18:00:34 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 01:00:34.351522 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 01:00:34.351711 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 01:00:34.351923 (Thread-1): Compiling model.order_history.customers
2020-04-28 01:00:34.376686 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 01:00:34.377217 (Thread-1): finished collecting timing info
2020-04-28 01:00:34.415090 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.415254 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 01:00:34.499992 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 01:00:34.504419 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.504581 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 01:00:34.549048 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 01:00:34.551929 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 01:00:34.552577 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.552738 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 01:00:34.593895 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 01:00:34.594188 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.594361 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,
        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN amount_gross ELSE NULL END) AS total_revenue
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 01:00:34.677728 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-28 01:00:34.682529 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.682675 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 01:00:34.729575 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 01:00:34.732240 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.732359 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 01:00:34.774758 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 01:00:34.776740 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 01:00:34.776930 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.777085 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 01:00:35.108684 (Thread-1): SQL status: COMMIT in 0.33 seconds
2020-04-28 01:00:35.111560 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:35.111723 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 01:00:35.502020 (Thread-1): SQL status: DROP VIEW in 0.39 seconds
2020-04-28 01:00:35.505852 (Thread-1): finished collecting timing info
2020-04-28 01:00:35.506655 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66ae1a8d-89e0-4f95-a1c1-016f3cb0c386', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120cb490>]}
2020-04-28 01:00:35.506919 (Thread-1): 18:00:35 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 1.16s]
2020-04-28 01:00:35.507076 (Thread-1): Finished running node model.order_history.customers
2020-04-28 01:00:35.582340 (MainThread): Using postgres connection "master".
2020-04-28 01:00:35.582705 (MainThread): On master: BEGIN
2020-04-28 01:00:35.630398 (MainThread): SQL status: BEGIN in 0.05 seconds
2020-04-28 01:00:35.630867 (MainThread): On master: COMMIT
2020-04-28 01:00:35.631081 (MainThread): Using postgres connection "master".
2020-04-28 01:00:35.631233 (MainThread): On master: COMMIT
2020-04-28 01:00:35.678686 (MainThread): SQL status: COMMIT in 0.05 seconds
2020-04-28 01:00:35.679570 (MainThread): 18:00:35 | 
2020-04-28 01:00:35.679803 (MainThread): 18:00:35 | Finished running 1 view model in 3.11s.
2020-04-28 01:00:35.680000 (MainThread): Connection 'master' was left open.
2020-04-28 01:00:35.680150 (MainThread): On master: Close
2020-04-28 01:00:35.680538 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 01:00:35.680696 (MainThread): On model.order_history.customers: Close
2020-04-28 01:00:35.685565 (MainThread): 
2020-04-28 01:00:35.685764 (MainThread): Completed successfully
2020-04-28 01:00:35.685919 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 01:00:35.686137 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122cb9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111fe8410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120cbf90>]}
2020-04-28 01:00:35.686369 (MainThread): Flushing usage events
2020-04-28 04:49:26.571429 (MainThread): Running with dbt=0.16.1
2020-04-28 04:49:26.691625 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 04:49:26.692821 (MainThread): Tracking: tracking
2020-04-28 04:49:26.702314 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e73810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060cced0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e732d0>]}
2020-04-28 04:49:26.727110 (MainThread): Partial parsing not enabled
2020-04-28 04:49:26.731236 (MainThread): Parsing macros/core.sql
2020-04-28 04:49:26.738747 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 04:49:26.748262 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 04:49:26.751257 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 04:49:26.770335 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 04:49:26.804482 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 04:49:26.826231 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 04:49:26.828835 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 04:49:26.835938 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 04:49:26.849168 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 04:49:26.856639 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 04:49:26.863664 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 04:49:26.869363 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 04:49:26.871040 (MainThread): Parsing macros/etc/query.sql
2020-04-28 04:49:26.872787 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 04:49:26.875169 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 04:49:26.878005 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 04:49:26.888106 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 04:49:26.890807 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 04:49:26.893472 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 04:49:26.936782 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 04:49:26.938868 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 04:49:26.940671 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 04:49:26.942462 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 04:49:26.945467 (MainThread): Parsing macros/catalog.sql
2020-04-28 04:49:26.948671 (MainThread): Parsing macros/relations.sql
2020-04-28 04:49:26.951496 (MainThread): Parsing macros/adapters.sql
2020-04-28 04:49:26.974407 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 04:49:26.992574 (MainThread): Partial parsing not enabled
2020-04-28 04:49:27.020031 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:49:27.020159 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.038351 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:27.038496 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.043207 (MainThread): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:27.043310 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.051463 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:27.051588 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.055740 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:49:27.055827 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.189308 (MainThread): Found 5 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 04:49:27.192632 (MainThread): 
2020-04-28 04:49:27.193017 (MainThread): Acquiring new postgres connection "master".
2020-04-28 04:49:27.193133 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.210017 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 04:49:27.210147 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 04:49:27.309317 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 04:49:27.309459 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 04:49:27.917107 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.61 seconds
2020-04-28 04:49:27.948972 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:49:27.949207 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 04:49:27.951030 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:49:27.951161 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 04:49:27.989881 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:27.990061 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:49:27.990161 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 04:49:28.094416 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-28 04:49:28.098795 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 04:49:28.164517 (MainThread): Using postgres connection "master".
2020-04-28 04:49:28.164691 (MainThread): On master: BEGIN
2020-04-28 04:49:28.806596 (MainThread): SQL status: BEGIN in 0.64 seconds
2020-04-28 04:49:28.807026 (MainThread): Using postgres connection "master".
2020-04-28 04:49:28.807288 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 04:49:28.965581 (MainThread): SQL status: SELECT in 0.16 seconds
2020-04-28 04:49:29.047561 (MainThread): On master: ROLLBACK
2020-04-28 04:49:29.117631 (MainThread): Using postgres connection "master".
2020-04-28 04:49:29.118037 (MainThread): On master: BEGIN
2020-04-28 04:49:29.256621 (MainThread): SQL status: BEGIN in 0.14 seconds
2020-04-28 04:49:29.256851 (MainThread): On master: COMMIT
2020-04-28 04:49:29.256974 (MainThread): Using postgres connection "master".
2020-04-28 04:49:29.257085 (MainThread): On master: COMMIT
2020-04-28 04:49:29.329743 (MainThread): SQL status: COMMIT in 0.07 seconds
2020-04-28 04:49:29.330188 (MainThread): 21:49:29 | Concurrency: 1 threads (target='dev')
2020-04-28 04:49:29.330359 (MainThread): 21:49:29 | 
2020-04-28 04:49:29.333866 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 04:49:29.334096 (Thread-1): 21:49:29 | 1 of 5 START view model data_science.stg_flash....................... [RUN]
2020-04-28 04:49:29.334452 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.334565 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 04:49:29.334683 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 04:49:29.353015 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 04:49:29.353684 (Thread-1): finished collecting timing info
2020-04-28 04:49:29.394834 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.395013 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 04:49:29.474584 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 04:49:29.479053 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.479208 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:49:29.518361 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 04:49:29.521474 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 04:49:29.522084 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.522244 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 04:49:29.561543 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:29.561976 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.562235 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 04:49:29.624400 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:49:29.628595 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.628750 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 04:49:29.667872 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:49:29.669199 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:49:29.669335 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.669442 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:49:29.838252 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:49:29.842638 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.842856 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:49:30.009056 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:49:30.012362 (Thread-1): finished collecting timing info
2020-04-28 04:49:30.013133 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ff02455-6753-4c91-b6d6-a0c39d1bec07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066fbf90>]}
2020-04-28 04:49:30.013403 (Thread-1): 21:49:30 | 1 of 5 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.68s]
2020-04-28 04:49:30.013560 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 04:49:30.013723 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 04:49:30.013882 (Thread-1): 21:49:30 | 2 of 5 START view model data_science.stg_order....................... [RUN]
2020-04-28 04:49:30.014280 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.014405 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 04:49:30.014518 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 04:49:30.020700 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 04:49:30.021135 (Thread-1): finished collecting timing info
2020-04-28 04:49:30.028840 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.028993 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 04:49:30.197334 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:49:30.201539 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.201692 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:49:30.420960 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:49:30.423993 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 04:49:30.424611 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.424779 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 04:49:30.466099 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:30.466538 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.466812 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 04:49:30.523037 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:49:30.527320 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.527478 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 04:49:30.567057 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:49:30.569005 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:49:30.569204 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.569363 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:49:30.746224 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 04:49:30.748262 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.748382 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:49:30.952286 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 04:49:30.956554 (Thread-1): finished collecting timing info
2020-04-28 04:49:30.957398 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ff02455-6753-4c91-b6d6-a0c39d1bec07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106377290>]}
2020-04-28 04:49:30.957703 (Thread-1): 21:49:30 | 2 of 5 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.94s]
2020-04-28 04:49:30.957882 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 04:49:30.958105 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 04:49:30.958342 (Thread-1): 21:49:30 | 3 of 5 START view model data_science.stg_customers................... [RUN]
2020-04-28 04:49:30.959003 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:30.959333 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 04:49:30.959540 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 04:49:30.965977 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 04:49:30.966473 (Thread-1): finished collecting timing info
2020-04-28 04:49:30.974142 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:30.974277 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 04:49:31.163869 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 04:49:31.168150 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.168310 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:49:31.348503 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:49:31.353008 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 04:49:31.353691 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.353852 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 04:49:31.392772 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:31.392978 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.393094 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 04:49:31.445963 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:49:31.480465 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.480647 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 04:49:31.523840 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:49:31.528008 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.528162 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 04:49:31.570769 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:49:31.572701 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:49:31.572898 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.573056 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:49:31.746090 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:49:31.749003 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.749190 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:49:31.931014 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:49:31.935308 (Thread-1): finished collecting timing info
2020-04-28 04:49:31.936308 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ff02455-6753-4c91-b6d6-a0c39d1bec07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106744f10>]}
2020-04-28 04:49:31.936614 (Thread-1): 21:49:31 | 3 of 5 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.98s]
2020-04-28 04:49:31.936795 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 04:49:31.937043 (Thread-1): Began running node model.order_history.stg_order_flash
2020-04-28 04:49:31.937396 (Thread-1): 21:49:31 | 4 of 5 START view model data_science.stg_order_flash................. [RUN]
2020-04-28 04:49:31.937782 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:31.937922 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 04:49:31.938062 (Thread-1): Compiling model.order_history.stg_order_flash
2020-04-28 04:49:31.947178 (Thread-1): Writing injected SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:49:31.947632 (Thread-1): finished collecting timing info
2020-04-28 04:49:31.955059 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:31.955188 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" cascade
2020-04-28 04:49:32.127441 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:49:32.130142 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:32.130268 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:49:32.299935 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:49:32.302944 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:49:32.303582 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:32.303749 (Thread-1): On model.order_history.stg_order_flash: BEGIN
2020-04-28 04:49:32.343906 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:32.344345 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:32.344620 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */

  create view "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final;
  );

2020-04-28 04:49:32.384451 (Thread-1): Postgres error: syntax error at or near "ON"
LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
                                   ^

2020-04-28 04:49:32.384866 (Thread-1): On model.order_history.stg_order_flash: ROLLBACK
2020-04-28 04:49:32.424333 (Thread-1): finished collecting timing info
2020-04-28 04:49:32.424926 (Thread-1): Database Error in model stg_order_flash (models/staging/stg_order_flash.sql)
  syntax error at or near "ON"
  LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
                                     ^
  compiled SQL at target/run/order_history/staging/stg_order_flash.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "ON"
LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
                                   ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model stg_order_flash (models/staging/stg_order_flash.sql)
  syntax error at or near "ON"
  LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
                                     ^
  compiled SQL at target/run/order_history/staging/stg_order_flash.sql
2020-04-28 04:49:32.444265 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ff02455-6753-4c91-b6d6-a0c39d1bec07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106250910>]}
2020-04-28 04:49:32.444529 (Thread-1): 21:49:32 | 4 of 5 ERROR creating view model data_science.stg_order_flash........ [ERROR in 0.51s]
2020-04-28 04:49:32.444671 (Thread-1): Finished running node model.order_history.stg_order_flash
2020-04-28 04:49:32.445064 (Thread-1): Began running node model.order_history.customers
2020-04-28 04:49:32.445239 (Thread-1): 21:49:32 | 5 of 5 SKIP relation data_science.customers.......................... [SKIP]
2020-04-28 04:49:32.445374 (Thread-1): Finished running node model.order_history.customers
2020-04-28 04:49:32.519535 (MainThread): Using postgres connection "master".
2020-04-28 04:49:32.519860 (MainThread): On master: BEGIN
2020-04-28 04:49:32.590234 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 04:49:32.590638 (MainThread): On master: COMMIT
2020-04-28 04:49:32.590934 (MainThread): Using postgres connection "master".
2020-04-28 04:49:32.591104 (MainThread): On master: COMMIT
2020-04-28 04:49:32.658528 (MainThread): SQL status: COMMIT in 0.07 seconds
2020-04-28 04:49:32.659350 (MainThread): 21:49:32 | 
2020-04-28 04:49:32.659592 (MainThread): 21:49:32 | Finished running 5 view models in 5.47s.
2020-04-28 04:49:32.659786 (MainThread): Connection 'master' was left open.
2020-04-28 04:49:32.659940 (MainThread): On master: Close
2020-04-28 04:49:32.660331 (MainThread): Connection 'model.order_history.stg_order_flash' was left open.
2020-04-28 04:49:32.660496 (MainThread): On model.order_history.stg_order_flash: Close
2020-04-28 04:49:32.678453 (MainThread): 
2020-04-28 04:49:32.678762 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 04:49:32.678916 (MainThread): 
2020-04-28 04:49:32.679055 (MainThread): Database Error in model stg_order_flash (models/staging/stg_order_flash.sql)
2020-04-28 04:49:32.679180 (MainThread):   syntax error at or near "ON"
2020-04-28 04:49:32.679295 (MainThread):   LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
2020-04-28 04:49:32.679405 (MainThread):                                      ^
2020-04-28 04:49:32.679513 (MainThread):   compiled SQL at target/run/order_history/staging/stg_order_flash.sql
2020-04-28 04:49:32.679634 (MainThread): 
Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
2020-04-28 04:49:32.679834 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064cd950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10616c590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10616cf50>]}
2020-04-28 04:49:32.680055 (MainThread): Flushing usage events
2020-04-28 04:52:33.911322 (MainThread): Running with dbt=0.16.1
2020-04-28 04:52:34.000782 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 04:52:34.002196 (MainThread): Tracking: tracking
2020-04-28 04:52:34.009742 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8e21d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb05e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8e2150>]}
2020-04-28 04:52:34.034923 (MainThread): Partial parsing not enabled
2020-04-28 04:52:34.037486 (MainThread): Parsing macros/core.sql
2020-04-28 04:52:34.043730 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 04:52:34.053036 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 04:52:34.055345 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 04:52:34.074149 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 04:52:34.108221 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 04:52:34.130247 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 04:52:34.132381 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 04:52:34.138852 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 04:52:34.151649 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 04:52:34.158650 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 04:52:34.167724 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 04:52:34.176709 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 04:52:34.178136 (MainThread): Parsing macros/etc/query.sql
2020-04-28 04:52:34.180128 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 04:52:34.182224 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 04:52:34.184723 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 04:52:34.194554 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 04:52:34.197513 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 04:52:34.200090 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 04:52:34.253156 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 04:52:34.255162 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 04:52:34.256379 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 04:52:34.257815 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 04:52:34.260453 (MainThread): Parsing macros/catalog.sql
2020-04-28 04:52:34.263034 (MainThread): Parsing macros/relations.sql
2020-04-28 04:52:34.264572 (MainThread): Parsing macros/adapters.sql
2020-04-28 04:52:34.282232 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 04:52:34.300581 (MainThread): Partial parsing not enabled
2020-04-28 04:52:34.328867 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:52:34.328980 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.346331 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:34.346477 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.351543 (MainThread): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:34.351679 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.359088 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:34.359227 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.364247 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:52:34.364385 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.505443 (MainThread): Found 5 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 04:52:34.507901 (MainThread): 
2020-04-28 04:52:34.508239 (MainThread): Acquiring new postgres connection "master".
2020-04-28 04:52:34.508334 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.525831 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 04:52:34.525949 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 04:52:34.630947 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 04:52:34.631144 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 04:52:35.191965 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.56 seconds
2020-04-28 04:52:35.217844 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:52:35.217968 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 04:52:35.219545 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:52:35.219661 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 04:52:35.257785 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:35.258213 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:52:35.258487 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 04:52:35.357299 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-28 04:52:35.361248 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 04:52:35.423695 (MainThread): Using postgres connection "master".
2020-04-28 04:52:35.423841 (MainThread): On master: BEGIN
2020-04-28 04:52:35.827992 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-28 04:52:35.828422 (MainThread): Using postgres connection "master".
2020-04-28 04:52:35.828692 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 04:52:35.958204 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 04:52:36.041678 (MainThread): On master: ROLLBACK
2020-04-28 04:52:36.140868 (MainThread): Using postgres connection "master".
2020-04-28 04:52:36.141287 (MainThread): On master: BEGIN
2020-04-28 04:52:36.226232 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 04:52:36.226695 (MainThread): On master: COMMIT
2020-04-28 04:52:36.227011 (MainThread): Using postgres connection "master".
2020-04-28 04:52:36.227171 (MainThread): On master: COMMIT
2020-04-28 04:52:36.269409 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 04:52:36.270298 (MainThread): 21:52:36 | Concurrency: 1 threads (target='dev')
2020-04-28 04:52:36.270546 (MainThread): 21:52:36 | 
2020-04-28 04:52:36.273399 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 04:52:36.273671 (Thread-1): 21:52:36 | 1 of 5 START view model data_science.stg_flash....................... [RUN]
2020-04-28 04:52:36.274086 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.274222 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 04:52:36.274366 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 04:52:36.291125 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 04:52:36.291663 (Thread-1): finished collecting timing info
2020-04-28 04:52:36.333866 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.334351 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 04:52:36.412753 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 04:52:36.415507 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.415624 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:52:36.454170 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 04:52:36.457277 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 04:52:36.458923 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.459160 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 04:52:36.498427 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:36.498866 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.499131 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 04:52:36.554636 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:52:36.560723 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.560892 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 04:52:36.604214 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:36.609453 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.609606 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 04:52:36.648848 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:36.650810 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:52:36.651011 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.651172 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:52:37.669009 (Thread-1): SQL status: COMMIT in 1.02 seconds
2020-04-28 04:52:37.672116 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:37.672266 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:52:37.894104 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:52:37.898396 (Thread-1): finished collecting timing info
2020-04-28 04:52:37.899273 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef39810>]}
2020-04-28 04:52:37.899597 (Thread-1): 21:52:37 | 1 of 5 OK created view model data_science.stg_flash.................. [CREATE VIEW in 1.63s]
2020-04-28 04:52:37.899825 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 04:52:37.900056 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 04:52:37.900492 (Thread-1): 21:52:37 | 2 of 5 START view model data_science.stg_order....................... [RUN]
2020-04-28 04:52:37.901015 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:52:37.901178 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 04:52:37.901339 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 04:52:37.908065 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 04:52:37.908556 (Thread-1): finished collecting timing info
2020-04-28 04:52:37.916154 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:37.916288 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 04:52:38.096589 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:52:38.099148 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.099264 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:52:38.267985 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:38.271050 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 04:52:38.271726 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.271882 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 04:52:38.310640 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:38.311057 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.311316 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 04:52:38.366245 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:52:38.372476 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.372626 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 04:52:38.413450 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:38.417318 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.417505 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 04:52:38.456298 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:38.458255 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:52:38.458443 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.458596 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:52:38.627041 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:52:38.630504 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.630656 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:52:38.804075 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:38.808364 (Thread-1): finished collecting timing info
2020-04-28 04:52:38.809204 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f10fa90>]}
2020-04-28 04:52:38.809504 (Thread-1): 21:52:38 | 2 of 5 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.91s]
2020-04-28 04:52:38.809678 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 04:52:38.809904 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 04:52:38.810296 (Thread-1): 21:52:38 | 3 of 5 START view model data_science.stg_customers................... [RUN]
2020-04-28 04:52:38.810857 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:38.811056 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 04:52:38.811225 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 04:52:38.817202 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 04:52:38.818547 (Thread-1): finished collecting timing info
2020-04-28 04:52:38.826504 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:38.826622 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 04:52:38.991728 (Thread-1): SQL status: DROP VIEW in 0.16 seconds
2020-04-28 04:52:39.026027 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.026214 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:52:39.192097 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:39.194919 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 04:52:39.195510 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.195670 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 04:52:39.237375 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:39.237789 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.238074 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 04:52:39.289058 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:52:39.295264 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.295414 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 04:52:39.337137 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:39.341466 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.341622 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 04:52:39.380880 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:39.382809 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:52:39.383003 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.383157 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:52:39.552710 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:52:39.556246 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.556397 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:52:39.729653 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:39.733854 (Thread-1): finished collecting timing info
2020-04-28 04:52:39.734679 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef39b10>]}
2020-04-28 04:52:39.734979 (Thread-1): 21:52:39 | 3 of 5 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.92s]
2020-04-28 04:52:39.735155 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 04:52:39.735388 (Thread-1): Began running node model.order_history.stg_order_flash
2020-04-28 04:52:39.735755 (Thread-1): 21:52:39 | 4 of 5 START view model data_science.stg_order_flash................. [RUN]
2020-04-28 04:52:39.736259 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:39.736450 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 04:52:39.736605 (Thread-1): Compiling model.order_history.stg_order_flash
2020-04-28 04:52:39.746452 (Thread-1): Writing injected SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:52:39.746935 (Thread-1): finished collecting timing info
2020-04-28 04:52:39.754560 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:39.754729 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" cascade
2020-04-28 04:52:39.923617 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:39.927737 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:39.927889 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:52:40.104898 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:52:40.109137 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:52:40.109710 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.109867 (Thread-1): On model.order_history.stg_order_flash: BEGIN
2020-04-28 04:52:40.148331 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:40.148751 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.149013 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */

  create view "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 04:52:40.203315 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:52:40.207314 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.207473 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
alter table "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" rename to "stg_order_flash"
2020-04-28 04:52:40.246314 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:40.248271 (Thread-1): On model.order_history.stg_order_flash: COMMIT
2020-04-28 04:52:40.248465 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.248620 (Thread-1): On model.order_history.stg_order_flash: COMMIT
2020-04-28 04:52:40.420719 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:52:40.424220 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.424365 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:52:40.638822 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 04:52:40.643007 (Thread-1): finished collecting timing info
2020-04-28 04:52:40.643833 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecf5310>]}
2020-04-28 04:52:40.644135 (Thread-1): 21:52:40 | 4 of 5 OK created view model data_science.stg_order_flash............ [CREATE VIEW in 0.91s]
2020-04-28 04:52:40.644311 (Thread-1): Finished running node model.order_history.stg_order_flash
2020-04-28 04:52:40.644783 (Thread-1): Began running node model.order_history.customers
2020-04-28 04:52:40.645044 (Thread-1): 21:52:40 | 5 of 5 START view model data_science.customers....................... [RUN]
2020-04-28 04:52:40.645395 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:52:40.645522 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order_flash).
2020-04-28 04:52:40.645648 (Thread-1): Compiling model.order_history.customers
2020-04-28 04:52:40.654764 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 04:52:40.656260 (Thread-1): finished collecting timing info
2020-04-28 04:52:40.663575 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:52:40.663709 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 04:52:40.839769 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:52:40.843876 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:52:40.844025 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 04:52:41.011896 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:41.014256 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 04:52:41.016935 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:52:41.017212 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 04:52:41.055787 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:41.056207 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:52:41.056470 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."stg_order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,
        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN amount_gross ELSE NULL END) AS total_revenue
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 04:52:41.098903 (Thread-1): Postgres error: syntax error at or near "COUNT"
LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
                 ^

2020-04-28 04:52:41.099319 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 04:52:41.138134 (Thread-1): finished collecting timing info
2020-04-28 04:52:41.139171 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "COUNT"
  LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
                   ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "COUNT"
LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
                 ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "COUNT"
  LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
                   ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 04:52:41.157095 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecf5310>]}
2020-04-28 04:52:41.157470 (Thread-1): 21:52:41 | 5 of 5 ERROR creating view model data_science.customers.............. [ERROR in 0.51s]
2020-04-28 04:52:41.157660 (Thread-1): Finished running node model.order_history.customers
2020-04-28 04:52:41.198759 (MainThread): Using postgres connection "master".
2020-04-28 04:52:41.198942 (MainThread): On master: BEGIN
2020-04-28 04:52:41.242139 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:41.242465 (MainThread): On master: COMMIT
2020-04-28 04:52:41.242590 (MainThread): Using postgres connection "master".
2020-04-28 04:52:41.242694 (MainThread): On master: COMMIT
2020-04-28 04:52:41.284665 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 04:52:41.285185 (MainThread): 21:52:41 | 
2020-04-28 04:52:41.285383 (MainThread): 21:52:41 | Finished running 5 view models in 6.78s.
2020-04-28 04:52:41.285586 (MainThread): Connection 'master' was left open.
2020-04-28 04:52:41.285748 (MainThread): On master: Close
2020-04-28 04:52:41.286213 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 04:52:41.286414 (MainThread): On model.order_history.customers: Close
2020-04-28 04:52:41.309701 (MainThread): 
2020-04-28 04:52:41.309906 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 04:52:41.310042 (MainThread): 
2020-04-28 04:52:41.310170 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 04:52:41.310322 (MainThread):   syntax error at or near "COUNT"
2020-04-28 04:52:41.310437 (MainThread):   LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
2020-04-28 04:52:41.310534 (MainThread):                    ^
2020-04-28 04:52:41.310626 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 04:52:41.310728 (MainThread): 
Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
2020-04-28 04:52:41.310901 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef32990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1b0090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee42210>]}
2020-04-28 04:52:41.311097 (MainThread): Flushing usage events
2020-04-28 04:52:58.972352 (MainThread): Running with dbt=0.16.1
2020-04-28 04:52:59.065419 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 04:52:59.066837 (MainThread): Tracking: tracking
2020-04-28 04:52:59.075322 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd2c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd2e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd2750>]}
2020-04-28 04:52:59.101546 (MainThread): Partial parsing not enabled
2020-04-28 04:52:59.105361 (MainThread): Parsing macros/core.sql
2020-04-28 04:52:59.111831 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 04:52:59.121711 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 04:52:59.124463 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 04:52:59.144980 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 04:52:59.178629 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 04:52:59.199937 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 04:52:59.202259 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 04:52:59.211352 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 04:52:59.225802 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 04:52:59.232997 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 04:52:59.240156 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 04:52:59.245558 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 04:52:59.246704 (MainThread): Parsing macros/etc/query.sql
2020-04-28 04:52:59.247992 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 04:52:59.249814 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 04:52:59.252329 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 04:52:59.261845 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 04:52:59.264038 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 04:52:59.265628 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 04:52:59.313512 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 04:52:59.316781 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 04:52:59.318934 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 04:52:59.321042 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 04:52:59.324360 (MainThread): Parsing macros/catalog.sql
2020-04-28 04:52:59.327605 (MainThread): Parsing macros/relations.sql
2020-04-28 04:52:59.329471 (MainThread): Parsing macros/adapters.sql
2020-04-28 04:52:59.348583 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 04:52:59.366782 (MainThread): Partial parsing not enabled
2020-04-28 04:52:59.394340 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:52:59.394449 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.411688 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:59.411841 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.416957 (MainThread): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:59.417094 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.424432 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:59.424570 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.429501 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:52:59.429638 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.560226 (MainThread): Found 5 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 04:52:59.563130 (MainThread): 
2020-04-28 04:52:59.563622 (MainThread): Acquiring new postgres connection "master".
2020-04-28 04:52:59.563710 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.580505 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 04:52:59.580643 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 04:52:59.686870 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 04:52:59.687044 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 04:53:00.139681 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.45 seconds
2020-04-28 04:53:00.169506 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:53:00.169731 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 04:53:00.171554 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:53:00.171680 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 04:53:00.209236 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:00.209658 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:53:00.209920 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 04:53:00.312639 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-28 04:53:00.318110 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 04:53:00.381476 (MainThread): Using postgres connection "master".
2020-04-28 04:53:00.381628 (MainThread): On master: BEGIN
2020-04-28 04:53:00.751125 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-28 04:53:00.751578 (MainThread): Using postgres connection "master".
2020-04-28 04:53:00.751843 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 04:53:00.904200 (MainThread): SQL status: SELECT in 0.15 seconds
2020-04-28 04:53:00.988116 (MainThread): On master: ROLLBACK
2020-04-28 04:53:01.028650 (MainThread): Using postgres connection "master".
2020-04-28 04:53:01.028884 (MainThread): On master: BEGIN
2020-04-28 04:53:01.116355 (MainThread): SQL status: BEGIN in 0.09 seconds
2020-04-28 04:53:01.116816 (MainThread): On master: COMMIT
2020-04-28 04:53:01.117116 (MainThread): Using postgres connection "master".
2020-04-28 04:53:01.117275 (MainThread): On master: COMMIT
2020-04-28 04:53:01.157906 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 04:53:01.158769 (MainThread): 21:53:01 | Concurrency: 1 threads (target='dev')
2020-04-28 04:53:01.159036 (MainThread): 21:53:01 | 
2020-04-28 04:53:01.162095 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 04:53:01.162359 (Thread-1): 21:53:01 | 1 of 5 START view model data_science.stg_flash....................... [RUN]
2020-04-28 04:53:01.162779 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.162922 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 04:53:01.163066 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 04:53:01.179860 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 04:53:01.180414 (Thread-1): finished collecting timing info
2020-04-28 04:53:01.222066 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.222248 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 04:53:01.297992 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 04:53:01.301373 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.301520 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:53:01.339376 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 04:53:01.341172 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 04:53:01.341637 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.341746 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 04:53:01.379517 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:01.379947 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.380218 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 04:53:01.436083 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:53:01.442385 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.442542 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 04:53:01.506304 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 04:53:01.511493 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.511659 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 04:53:01.553503 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:01.555445 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:53:01.555644 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.555808 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:53:01.766861 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-28 04:53:01.770347 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.770515 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:53:02.079153 (Thread-1): SQL status: DROP VIEW in 0.31 seconds
2020-04-28 04:53:02.083469 (Thread-1): finished collecting timing info
2020-04-28 04:53:02.084334 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c291550>]}
2020-04-28 04:53:02.084661 (Thread-1): 21:53:02 | 1 of 5 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.92s]
2020-04-28 04:53:02.084850 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 04:53:02.085039 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 04:53:02.085220 (Thread-1): 21:53:02 | 2 of 5 START view model data_science.stg_order....................... [RUN]
2020-04-28 04:53:02.085558 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.085689 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 04:53:02.085817 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 04:53:02.091868 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 04:53:02.092370 (Thread-1): finished collecting timing info
2020-04-28 04:53:02.099725 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.099852 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 04:53:02.345858 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-28 04:53:02.348759 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.348903 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:53:02.571075 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:53:02.574123 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 04:53:02.574711 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.574868 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 04:53:02.612961 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:02.613250 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.613442 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 04:53:02.668116 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:53:02.673038 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.673180 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 04:53:02.713860 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:02.716563 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.716680 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 04:53:02.755724 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:02.757702 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:53:02.757898 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.758057 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:53:02.989558 (Thread-1): SQL status: COMMIT in 0.23 seconds
2020-04-28 04:53:02.993078 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.993238 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:53:03.216661 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:53:03.220967 (Thread-1): finished collecting timing info
2020-04-28 04:53:03.221817 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c638d90>]}
2020-04-28 04:53:03.222124 (Thread-1): 21:53:03 | 2 of 5 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.14s]
2020-04-28 04:53:03.222306 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 04:53:03.222490 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 04:53:03.222670 (Thread-1): 21:53:03 | 3 of 5 START view model data_science.stg_customers................... [RUN]
2020-04-28 04:53:03.223336 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.223590 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 04:53:03.223925 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 04:53:03.230346 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 04:53:03.230775 (Thread-1): finished collecting timing info
2020-04-28 04:53:03.239462 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.239592 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 04:53:03.421144 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:53:03.452266 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.452451 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:53:03.630486 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:53:03.633313 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 04:53:03.633870 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.634030 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 04:53:03.676491 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:03.676952 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.677131 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 04:53:03.728750 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:53:03.734650 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.734844 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 04:53:03.773809 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:03.778080 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.778235 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 04:53:03.819047 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:03.820976 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:53:03.821184 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.821346 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:53:04.040723 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 04:53:04.043607 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:04.043763 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:53:04.248911 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 04:53:04.253201 (Thread-1): finished collecting timing info
2020-04-28 04:53:04.254072 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdb4310>]}
2020-04-28 04:53:04.254393 (Thread-1): 21:53:04 | 3 of 5 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.03s]
2020-04-28 04:53:04.254585 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 04:53:04.254800 (Thread-1): Began running node model.order_history.stg_order_flash
2020-04-28 04:53:04.255198 (Thread-1): 21:53:04 | 4 of 5 START view model data_science.stg_order_flash................. [RUN]
2020-04-28 04:53:04.255662 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.255848 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 04:53:04.255998 (Thread-1): Compiling model.order_history.stg_order_flash
2020-04-28 04:53:04.264605 (Thread-1): Writing injected SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:53:04.265037 (Thread-1): finished collecting timing info
2020-04-28 04:53:04.271710 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.271823 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" cascade
2020-04-28 04:53:04.472203 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 04:53:04.476331 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.476489 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:53:04.668235 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 04:53:04.672443 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:53:04.672988 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.673143 (Thread-1): On model.order_history.stg_order_flash: BEGIN
2020-04-28 04:53:04.711434 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:04.711871 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.712155 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */

  create view "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 04:53:04.767286 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:53:04.770653 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.770829 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
alter table "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" rename to "stg_order_flash"
2020-04-28 04:53:04.809386 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:04.810558 (Thread-1): On model.order_history.stg_order_flash: COMMIT
2020-04-28 04:53:04.810674 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.810769 (Thread-1): On model.order_history.stg_order_flash: COMMIT
2020-04-28 04:53:05.009004 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 04:53:05.012613 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:05.012768 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:53:06.344925 (Thread-1): SQL status: DROP VIEW in 1.33 seconds
2020-04-28 04:53:06.348854 (Thread-1): finished collecting timing info
2020-04-28 04:53:06.349723 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2fa490>]}
2020-04-28 04:53:06.350033 (Thread-1): 21:53:06 | 4 of 5 OK created view model data_science.stg_order_flash............ [CREATE VIEW in 2.09s]
2020-04-28 04:53:06.350215 (Thread-1): Finished running node model.order_history.stg_order_flash
2020-04-28 04:53:06.350790 (Thread-1): Began running node model.order_history.customers
2020-04-28 04:53:06.351026 (Thread-1): 21:53:06 | 5 of 5 START view model data_science.customers....................... [RUN]
2020-04-28 04:53:06.351514 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:53:06.351678 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order_flash).
2020-04-28 04:53:06.351802 (Thread-1): Compiling model.order_history.customers
2020-04-28 04:53:06.360950 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 04:53:06.361375 (Thread-1): finished collecting timing info
2020-04-28 04:53:06.368754 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.368886 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 04:53:06.597098 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 04:53:06.601213 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.601366 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 04:53:06.821377 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:53:06.823883 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 04:53:06.824446 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.824609 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 04:53:06.887804 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-04-28 04:53:06.888259 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.888445 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."stg_order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,
        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN amount_gross ELSE NULL END) AS total_revenue
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 04:53:06.945467 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:53:06.949683 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.949836 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 04:53:06.988784 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:06.990719 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 04:53:06.990915 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.991075 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 04:53:07.166872 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 04:53:07.169780 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:07.169934 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 04:53:07.374796 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 04:53:07.379084 (Thread-1): finished collecting timing info
2020-04-28 04:53:07.379938 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c303fd0>]}
2020-04-28 04:53:07.380248 (Thread-1): 21:53:07 | 5 of 5 OK created view model data_science.customers.................. [CREATE VIEW in 1.03s]
2020-04-28 04:53:07.380429 (Thread-1): Finished running node model.order_history.customers
2020-04-28 04:53:07.410613 (MainThread): Using postgres connection "master".
2020-04-28 04:53:07.410806 (MainThread): On master: BEGIN
2020-04-28 04:53:07.451821 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:07.452274 (MainThread): On master: COMMIT
2020-04-28 04:53:07.452451 (MainThread): Using postgres connection "master".
2020-04-28 04:53:07.452609 (MainThread): On master: COMMIT
2020-04-28 04:53:07.492253 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 04:53:07.492745 (MainThread): 21:53:07 | 
2020-04-28 04:53:07.492901 (MainThread): 21:53:07 | Finished running 5 view models in 7.93s.
2020-04-28 04:53:07.493033 (MainThread): Connection 'master' was left open.
2020-04-28 04:53:07.493172 (MainThread): On master: Close
2020-04-28 04:53:07.493443 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 04:53:07.493584 (MainThread): On model.order_history.customers: Close
2020-04-28 04:53:07.516331 (MainThread): 
2020-04-28 04:53:07.516604 (MainThread): Completed successfully
2020-04-28 04:53:07.516818 (MainThread): 
Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
2020-04-28 04:53:07.517109 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c21d590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c464c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2df150>]}
2020-04-28 04:53:07.517421 (MainThread): Flushing usage events
2020-04-28 19:37:21.417876 (MainThread): Running with dbt=0.16.1
2020-04-28 19:37:21.544262 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:37:21.545574 (MainThread): Tracking: tracking
2020-04-28 19:37:21.555735 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ef9090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111253f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ef9bd0>]}
2020-04-28 19:37:21.582147 (MainThread): Partial parsing not enabled
2020-04-28 19:37:21.586210 (MainThread): Parsing macros/core.sql
2020-04-28 19:37:21.594076 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:37:21.608354 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:37:21.611817 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:37:21.631541 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:37:21.667295 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:37:21.689838 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:37:21.692636 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:37:21.700004 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:37:21.715602 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:37:21.723931 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:37:21.731296 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:37:21.736848 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:37:21.738643 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:37:21.740473 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:37:21.742857 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:37:21.745748 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:37:21.755349 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:37:21.758054 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:37:21.760994 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:37:21.805341 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:37:21.807574 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:37:21.809181 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:37:21.810964 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:37:21.813913 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:37:21.816993 (MainThread): Parsing macros/relations.sql
2020-04-28 19:37:21.819280 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:37:21.836374 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:37:21.853867 (MainThread): Partial parsing not enabled
2020-04-28 19:37:21.883536 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:37:21.883667 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.901322 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:21.901455 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.907118 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:21.907218 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.913219 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:37:21.913354 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.918227 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:37:21.918347 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.923765 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:21.923892 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.930937 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:37:21.931076 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.980811 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113eb250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114bd950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114bdb90>]}
2020-04-28 19:37:21.981030 (MainThread): Flushing usage events
2020-04-28 19:37:22.314756 (MainThread): Connection 'model.order_history.order_flash' was properly closed.
2020-04-28 19:37:22.315005 (MainThread): Encountered an error:
2020-04-28 19:37:22.315203 (MainThread): Compilation Error in model customers (models/customers.sql)
  Model 'model.order_history.customers' depends on a node named 'stg_order_flash' which was not found or is disabled
2020-04-28 19:37:22.327604 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 302, in process_manifest
    process_refs(manifest, project_name)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 553, in process_refs
    _process_refs_for_node(manifest, current_project, node)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 534, in _process_refs_for_node
    disabled=(isinstance(target_model, Disabled))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/utils.py", line 338, in invalid_ref_fail_unless_test
    target_model_package)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 488, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model customers (models/customers.sql)
  Model 'model.order_history.customers' depends on a node named 'stg_order_flash' which was not found or is disabled

2020-04-28 19:37:48.064891 (MainThread): Running with dbt=0.16.1
2020-04-28 19:37:48.131069 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:37:48.132060 (MainThread): Tracking: tracking
2020-04-28 19:37:48.139422 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cbbf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f2e9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f12a90>]}
2020-04-28 19:37:48.170468 (MainThread): Partial parsing not enabled
2020-04-28 19:37:48.172663 (MainThread): Parsing macros/core.sql
2020-04-28 19:37:48.178258 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:37:48.187927 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:37:48.189905 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:37:48.208092 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:37:48.242097 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:37:48.263975 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:37:48.265963 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:37:48.272503 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:37:48.285730 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:37:48.293001 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:37:48.299538 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:37:48.304785 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:37:48.305785 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:37:48.306908 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:37:48.308643 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:37:48.310871 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:37:48.320186 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:37:48.322272 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:37:48.323383 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:37:48.364673 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:37:48.365885 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:37:48.366809 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:37:48.367901 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:37:48.370572 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:37:48.373590 (MainThread): Parsing macros/relations.sql
2020-04-28 19:37:48.375091 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:37:48.392954 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:37:48.411335 (MainThread): Partial parsing not enabled
2020-04-28 19:37:48.445772 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:37:48.445901 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.462443 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:48.462549 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.466572 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:48.466659 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.471037 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:37:48.471124 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.475052 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:37:48.475139 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.479748 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:48.479841 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.484779 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:37:48.484866 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.625248 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:37:48.628916 (MainThread): 
2020-04-28 19:37:48.629278 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:37:48.629374 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.652326 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:37:48.652477 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:37:48.755029 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:37:48.755247 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:37:49.300749 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.55 seconds
2020-04-28 19:37:49.339804 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:37:49.340026 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:37:49.341715 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:37:49.341845 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:37:49.379145 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:49.379566 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:37:49.379827 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:37:49.488748 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 19:37:49.496496 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:37:49.572624 (MainThread): Using postgres connection "master".
2020-04-28 19:37:49.572780 (MainThread): On master: BEGIN
2020-04-28 19:37:49.915104 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-04-28 19:37:49.915525 (MainThread): Using postgres connection "master".
2020-04-28 19:37:49.915791 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:37:50.055429 (MainThread): SQL status: SELECT in 0.14 seconds
2020-04-28 19:37:50.131172 (MainThread): On master: ROLLBACK
2020-04-28 19:37:50.168890 (MainThread): Using postgres connection "master".
2020-04-28 19:37:50.169066 (MainThread): On master: BEGIN
2020-04-28 19:37:50.243867 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 19:37:50.244313 (MainThread): On master: COMMIT
2020-04-28 19:37:50.244596 (MainThread): Using postgres connection "master".
2020-04-28 19:37:50.244772 (MainThread): On master: COMMIT
2020-04-28 19:37:50.282101 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:37:50.282981 (MainThread): 12:37:50 | Concurrency: 1 threads (target='dev')
2020-04-28 19:37:50.283232 (MainThread): 12:37:50 | 
2020-04-28 19:37:50.287180 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:37:50.287404 (Thread-1): 12:37:50 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:37:50.287742 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.287860 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:37:50.287982 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:37:50.303902 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:37:50.304427 (Thread-1): finished collecting timing info
2020-04-28 19:37:50.343726 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.343927 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:37:50.419723 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:37:50.424707 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.424861 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:37:50.462416 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:37:50.464639 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:37:50.465314 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.465473 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:37:50.501734 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:50.501962 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.502090 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:37:50.549080 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:37:50.555507 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.555663 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:37:50.603720 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:37:50.607947 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.608113 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:37:50.646565 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:50.648518 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:37:50.648715 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.648876 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:37:52.730335 (Thread-1): SQL status: COMMIT in 2.08 seconds
2020-04-28 19:37:52.733776 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:52.733937 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:37:53.750218 (Thread-1): SQL status: DROP VIEW in 1.02 seconds
2020-04-28 19:37:53.754486 (Thread-1): finished collecting timing info
2020-04-28 19:37:53.755343 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091c5e10>]}
2020-04-28 19:37:53.755663 (Thread-1): 12:37:53 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 3.47s]
2020-04-28 19:37:53.755854 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:37:53.756038 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:37:53.756227 (Thread-1): 12:37:53 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:37:53.756965 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:53.757213 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:37:53.757383 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:37:53.763804 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:37:53.765344 (Thread-1): finished collecting timing info
2020-04-28 19:37:53.772903 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:53.773037 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:37:53.943414 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:53.947617 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:53.947769 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:37:54.143988 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:37:54.146392 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:37:54.147727 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.147891 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:37:54.184810 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:54.185092 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.185266 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:37:54.246775 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:37:54.253046 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.253200 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:37:54.294728 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:54.298495 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.298645 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:37:54.336588 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:54.338605 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:37:54.338810 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.338984 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:37:54.517687 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:37:54.521068 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.521218 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:37:54.715033 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:37:54.719253 (Thread-1): finished collecting timing info
2020-04-28 19:37:54.720090 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090bf090>]}
2020-04-28 19:37:54.720392 (Thread-1): 12:37:54 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.96s]
2020-04-28 19:37:54.720571 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:37:54.720754 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:37:54.720934 (Thread-1): 12:37:54 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:37:54.721271 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:37:54.721401 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:37:54.721531 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:37:54.758789 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:37:54.759313 (Thread-1): finished collecting timing info
2020-04-28 19:37:54.767219 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:54.767373 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:37:54.937783 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:54.941961 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:54.942113 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:37:55.109742 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:55.112712 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:37:55.113392 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.113547 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:37:55.180278 (Thread-1): SQL status: BEGIN in 0.07 seconds
2020-04-28 19:37:55.180479 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.180591 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:37:55.229302 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:37:55.233292 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.233407 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:37:55.273043 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:55.277312 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.277467 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:37:55.314890 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:55.316892 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:37:55.317086 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.317243 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:37:55.488586 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:37:55.491943 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.492100 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:37:55.679063 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:37:55.683254 (Thread-1): finished collecting timing info
2020-04-28 19:37:55.684100 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f7f510>]}
2020-04-28 19:37:55.684405 (Thread-1): 12:37:55 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.96s]
2020-04-28 19:37:55.684585 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:37:55.684772 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:37:55.685128 (Thread-1): 12:37:55 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:37:55.686012 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:37:55.686180 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:37:55.686316 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:37:55.693641 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:37:55.694081 (Thread-1): finished collecting timing info
2020-04-28 19:37:55.701298 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:55.701424 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:37:55.895729 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:37:55.898251 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:55.898367 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:37:56.939860 (Thread-1): SQL status: DROP VIEW in 1.04 seconds
2020-04-28 19:37:56.942940 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:37:56.943572 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:56.943726 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:37:56.983548 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:56.983976 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:56.984241 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:37:57.039341 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:37:57.043552 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:57.043704 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:37:57.082373 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:57.084302 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:37:57.084499 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:57.084658 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:37:57.272887 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-28 19:37:57.274956 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:57.275080 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:37:57.449524 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:57.453924 (Thread-1): finished collecting timing info
2020-04-28 19:37:57.454759 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090fad10>]}
2020-04-28 19:37:57.455064 (Thread-1): 12:37:57 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 1.77s]
2020-04-28 19:37:57.455243 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:37:57.455427 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:37:57.455828 (Thread-1): 12:37:57 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:37:57.456361 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.456533 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:37:57.456655 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:37:57.464558 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:37:57.465128 (Thread-1): finished collecting timing info
2020-04-28 19:37:57.472685 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.472815 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:37:57.707795 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 19:37:57.711952 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.712107 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:37:57.880885 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:57.884225 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:37:57.884879 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.885042 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:37:57.922378 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:57.922660 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.922842 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
)

brokers as (
    SELECT email
    FROM analytics.yield_manager_partners
)

final as (
    SELECT *
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.email
)
  );

2020-04-28 19:37:57.961444 (Thread-1): Postgres error: syntax error at or near "brokers"
LINE 8: brokers as (
        ^

2020-04-28 19:37:57.961872 (Thread-1): On model.order_history.customer_broker: ROLLBACK
2020-04-28 19:37:57.999453 (Thread-1): finished collecting timing info
2020-04-28 19:37:58.000240 (Thread-1): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  syntax error at or near "brokers"
  LINE 8: brokers as (
          ^
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "brokers"
LINE 8: brokers as (
        ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  syntax error at or near "brokers"
  LINE 8: brokers as (
          ^
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:37:58.022988 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f67f10>]}
2020-04-28 19:37:58.023262 (Thread-1): 12:37:58 | 5 of 7 ERROR creating view model data_science.customer_broker........ [ERROR in 0.57s]
2020-04-28 19:37:58.023419 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:37:58.023579 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:37:58.023733 (Thread-1): 12:37:58 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:37:58.024180 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.024294 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:37:58.024404 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:37:58.033232 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:37:58.033613 (Thread-1): finished collecting timing info
2020-04-28 19:37:58.040468 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.040581 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:37:58.138471 (Thread-1): SQL status: DROP VIEW in 0.10 seconds
2020-04-28 19:37:58.143583 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.143770 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:37:58.181895 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:37:58.184941 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:37:58.185560 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.185716 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:37:58.223480 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:58.223719 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.223840 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:37:58.270615 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:37:58.273963 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.274099 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:37:58.323273 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:37:58.325186 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:37:58.325382 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.325541 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:37:58.603608 (Thread-1): SQL status: COMMIT in 0.28 seconds
2020-04-28 19:37:58.606357 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.606514 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:37:58.794952 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:37:58.799248 (Thread-1): finished collecting timing info
2020-04-28 19:37:58.800085 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f58610>]}
2020-04-28 19:37:58.800390 (Thread-1): 12:37:58 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.78s]
2020-04-28 19:37:58.800569 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:37:58.800997 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:37:58.801198 (Thread-1): 12:37:58 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:37:58.801653 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:37:58.801849 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:37:58.801984 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:37:58.811451 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:37:58.811875 (Thread-1): finished collecting timing info
2020-04-28 19:37:58.819460 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:37:58.819598 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:37:59.001854 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:37:59.007472 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:37:59.007629 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:38:00.019858 (Thread-1): SQL status: DROP VIEW in 1.01 seconds
2020-04-28 19:38:00.022449 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:38:00.023416 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.023647 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:38:00.205313 (Thread-1): SQL status: BEGIN in 0.18 seconds
2020-04-28 19:38:00.205753 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.205949 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:38:00.298640 (Thread-1): SQL status: CREATE VIEW in 0.09 seconds
2020-04-28 19:38:00.302358 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.302502 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:38:00.500005 (Thread-1): SQL status: ALTER TABLE in 0.20 seconds
2020-04-28 19:38:00.501832 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:38:00.502035 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.502196 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:38:00.798681 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-28 19:38:00.801691 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.801857 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:38:01.339641 (Thread-1): SQL status: DROP VIEW in 0.54 seconds
2020-04-28 19:38:01.343869 (Thread-1): finished collecting timing info
2020-04-28 19:38:01.344714 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f62fd0>]}
2020-04-28 19:38:01.345016 (Thread-1): 12:38:01 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 2.54s]
2020-04-28 19:38:01.345196 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:38:01.364462 (MainThread): Using postgres connection "master".
2020-04-28 19:38:01.364732 (MainThread): On master: BEGIN
2020-04-28 19:38:01.499170 (MainThread): SQL status: BEGIN in 0.13 seconds
2020-04-28 19:38:01.499625 (MainThread): On master: COMMIT
2020-04-28 19:38:01.499912 (MainThread): Using postgres connection "master".
2020-04-28 19:38:01.500065 (MainThread): On master: COMMIT
2020-04-28 19:38:01.536841 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:38:01.537760 (MainThread): 12:38:01 | 
2020-04-28 19:38:01.537998 (MainThread): 12:38:01 | Finished running 7 view models in 12.91s.
2020-04-28 19:38:01.538191 (MainThread): Connection 'master' was left open.
2020-04-28 19:38:01.538347 (MainThread): On master: Close
2020-04-28 19:38:01.538749 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:38:01.538914 (MainThread): On model.order_history.customers: Close
2020-04-28 19:38:01.560247 (MainThread): 
2020-04-28 19:38:01.560453 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 19:38:01.560593 (MainThread): 
2020-04-28 19:38:01.560727 (MainThread): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
2020-04-28 19:38:01.560849 (MainThread):   syntax error at or near "brokers"
2020-04-28 19:38:01.560960 (MainThread):   LINE 8: brokers as (
2020-04-28 19:38:01.561067 (MainThread):           ^
2020-04-28 19:38:01.561172 (MainThread):   compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:38:01.561291 (MainThread): 
Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
2020-04-28 19:38:01.561482 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095979d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109191910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090e5310>]}
2020-04-28 19:38:01.561694 (MainThread): Flushing usage events
2020-04-28 19:38:25.552664 (MainThread): Running with dbt=0.16.1
2020-04-28 19:38:25.626596 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:38:25.627674 (MainThread): Tracking: tracking
2020-04-28 19:38:25.635348 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036f0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10371d0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036f0d50>]}
2020-04-28 19:38:25.658152 (MainThread): Partial parsing not enabled
2020-04-28 19:38:25.660342 (MainThread): Parsing macros/core.sql
2020-04-28 19:38:25.665864 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:38:25.675480 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:38:25.677399 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:38:25.696493 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:38:25.730070 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:38:25.751118 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:38:25.753009 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:38:25.759336 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:38:25.772100 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:38:25.779571 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:38:25.786445 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:38:25.791824 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:38:25.792777 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:38:25.793855 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:38:25.795533 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:38:25.797605 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:38:25.806829 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:38:25.808826 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:38:25.809891 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:38:25.850976 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:38:25.852140 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:38:25.853073 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:38:25.854189 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:38:25.856613 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:38:25.858948 (MainThread): Parsing macros/relations.sql
2020-04-28 19:38:25.860307 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:38:25.878045 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:38:25.902342 (MainThread): Partial parsing not enabled
2020-04-28 19:38:25.930309 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:38:25.930427 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.946607 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:25.946702 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.950629 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:25.950718 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.955026 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:38:25.955130 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.959019 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:38:25.959105 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.963557 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:25.963652 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.968900 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:38:25.968988 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:26.103738 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:38:26.108010 (MainThread): 
2020-04-28 19:38:26.108307 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:38:26.108393 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:26.129488 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:38:26.129630 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:38:26.226808 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:38:26.226941 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:38:26.642594 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.42 seconds
2020-04-28 19:38:26.678739 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:38:26.678941 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:38:26.680559 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:38:26.680672 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:38:26.719193 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:26.719601 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:38:26.719853 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:38:26.813048 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-28 19:38:26.821480 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:38:26.896265 (MainThread): Using postgres connection "master".
2020-04-28 19:38:26.896477 (MainThread): On master: BEGIN
2020-04-28 19:38:27.242822 (MainThread): SQL status: BEGIN in 0.35 seconds
2020-04-28 19:38:27.243143 (MainThread): Using postgres connection "master".
2020-04-28 19:38:27.243244 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:38:27.371749 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 19:38:27.448396 (MainThread): On master: ROLLBACK
2020-04-28 19:38:27.486283 (MainThread): Using postgres connection "master".
2020-04-28 19:38:27.486673 (MainThread): On master: BEGIN
2020-04-28 19:38:27.561808 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 19:38:27.562096 (MainThread): On master: COMMIT
2020-04-28 19:38:27.562257 (MainThread): Using postgres connection "master".
2020-04-28 19:38:27.562401 (MainThread): On master: COMMIT
2020-04-28 19:38:27.600496 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:38:27.601374 (MainThread): 12:38:27 | Concurrency: 1 threads (target='dev')
2020-04-28 19:38:27.601621 (MainThread): 12:38:27 | 
2020-04-28 19:38:27.604254 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:38:27.604526 (Thread-1): 12:38:27 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:38:27.604910 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.605049 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:38:27.605187 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:38:27.620301 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:38:27.620712 (Thread-1): finished collecting timing info
2020-04-28 19:38:27.658655 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.658844 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:38:27.736305 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:38:27.741308 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.741465 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:38:27.782009 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:38:27.784910 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:38:27.785557 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.785715 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:38:27.824186 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:27.824601 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.824858 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:38:27.879076 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:38:27.883797 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.883944 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:38:27.922606 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:27.925249 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.925364 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:38:27.967178 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:27.969102 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:38:27.969292 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.969445 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:38:28.892788 (Thread-1): SQL status: COMMIT in 0.92 seconds
2020-04-28 19:38:28.895094 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:28.895239 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:38:29.120029 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 19:38:29.124306 (Thread-1): finished collecting timing info
2020-04-28 19:38:29.125154 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d34e50>]}
2020-04-28 19:38:29.125463 (Thread-1): 12:38:29 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.52s]
2020-04-28 19:38:29.125644 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:38:29.125877 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:38:29.126151 (Thread-1): 12:38:29 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:38:29.126987 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.127114 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:38:29.127228 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:38:29.133481 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:38:29.133942 (Thread-1): finished collecting timing info
2020-04-28 19:38:29.141547 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.141674 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:38:29.327352 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:38:29.331544 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.331693 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:38:29.803068 (Thread-1): SQL status: DROP VIEW in 0.47 seconds
2020-04-28 19:38:29.806026 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:38:29.806602 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.806759 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:38:29.845721 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:29.846127 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.846376 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:38:29.900239 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:38:29.905615 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.905752 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:38:29.944597 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:29.947482 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.947619 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:38:29.986916 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:29.988873 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:38:29.989066 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.989221 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:38:30.161669 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:38:30.166353 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:30.166502 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:38:30.351274 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:38:30.355603 (Thread-1): finished collecting timing info
2020-04-28 19:38:30.356448 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa7350>]}
2020-04-28 19:38:30.356751 (Thread-1): 12:38:30 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 1.23s]
2020-04-28 19:38:30.356927 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:38:30.357152 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:38:30.357514 (Thread-1): 12:38:30 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:38:30.357959 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:38:30.358144 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:38:30.358329 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:38:30.396213 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:38:30.396732 (Thread-1): finished collecting timing info
2020-04-28 19:38:30.404287 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:30.404425 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:38:32.331892 (Thread-1): SQL status: DROP VIEW in 1.93 seconds
2020-04-28 19:38:32.334377 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:32.334489 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:38:33.266299 (Thread-1): SQL status: DROP VIEW in 0.93 seconds
2020-04-28 19:38:33.269271 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:38:33.269909 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.270067 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:38:33.309198 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:33.309402 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.309516 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:38:33.367132 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:38:33.373270 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.373428 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:38:33.435343 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 19:38:33.439534 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.439702 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:38:33.478284 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:33.479618 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:38:33.479747 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.479855 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:38:33.934716 (Thread-1): SQL status: COMMIT in 0.45 seconds
2020-04-28 19:38:33.938072 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.938219 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:38:37.698160 (Thread-1): SQL status: DROP VIEW in 3.76 seconds
2020-04-28 19:38:37.702507 (Thread-1): finished collecting timing info
2020-04-28 19:38:37.703359 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fc1510>]}
2020-04-28 19:38:37.703664 (Thread-1): 12:38:37 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 7.35s]
2020-04-28 19:38:37.703914 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:38:37.704125 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:38:37.704457 (Thread-1): 12:38:37 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:38:37.705157 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:38:37.705338 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:38:37.705486 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:38:37.712550 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:38:37.712968 (Thread-1): finished collecting timing info
2020-04-28 19:38:37.719672 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:37.719784 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:38:38.510889 (Thread-1): SQL status: DROP VIEW in 0.79 seconds
2020-04-28 19:38:38.514611 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:38.514763 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:38:38.950659 (Thread-1): SQL status: DROP VIEW in 0.44 seconds
2020-04-28 19:38:38.953703 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:38:38.954315 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:38.954462 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:38:38.993659 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:38.993849 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:38.993957 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:38:39.047913 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:38:39.054048 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:39.054198 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:38:39.093080 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:39.097361 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:39.097511 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:38:39.147992 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:38:39.149892 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:38:39.150085 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:39.150240 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:38:40.468642 (Thread-1): SQL status: COMMIT in 1.32 seconds
2020-04-28 19:38:40.472194 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:40.472346 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:38:40.713074 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-28 19:38:40.715642 (Thread-1): finished collecting timing info
2020-04-28 19:38:40.716266 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b2fd90>]}
2020-04-28 19:38:40.716490 (Thread-1): 12:38:40 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 3.01s]
2020-04-28 19:38:40.716621 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:38:40.716768 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:38:40.717022 (Thread-1): 12:38:40 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:38:40.717397 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:40.717514 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:38:40.717616 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:38:40.724341 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:38:40.724750 (Thread-1): finished collecting timing info
2020-04-28 19:38:40.731233 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:40.731343 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:38:40.956632 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 19:38:40.961885 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:40.962040 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:38:41.130462 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:38:41.133532 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:38:41.134162 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:41.134314 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:38:41.177678 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:41.177961 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:41.178125 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT *
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.email
)
  );

2020-04-28 19:38:41.217559 (Thread-1): Postgres error: syntax error at or near ")"
LINE 17:   );
           ^

2020-04-28 19:38:41.218009 (Thread-1): On model.order_history.customer_broker: ROLLBACK
2020-04-28 19:38:41.256941 (Thread-1): finished collecting timing info
2020-04-28 19:38:41.257996 (Thread-1): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  syntax error at or near ")"
  LINE 17:   );
             ^
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near ")"
LINE 17:   );
           ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  syntax error at or near ")"
  LINE 17:   );
             ^
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:38:41.260905 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103940bd0>]}
2020-04-28 19:38:41.261197 (Thread-1): 12:38:41 | 5 of 7 ERROR creating view model data_science.customer_broker........ [ERROR in 0.54s]
2020-04-28 19:38:41.261378 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:38:41.261564 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:38:41.261746 (Thread-1): 12:38:41 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:38:41.262089 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.262222 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:38:41.262351 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:38:41.271669 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:38:41.272089 (Thread-1): finished collecting timing info
2020-04-28 19:38:41.279547 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.279690 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:38:41.357267 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:38:41.361192 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.361354 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:38:41.400141 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:38:41.402906 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:38:41.403477 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.403658 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:38:41.441510 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:41.441793 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.441958 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:38:41.493722 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:38:41.497765 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.497929 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:38:41.536518 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:41.538236 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:38:41.538431 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.538589 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:38:41.722558 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:38:41.724818 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.724960 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:38:41.903852 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:38:41.908114 (Thread-1): finished collecting timing info
2020-04-28 19:38:41.908964 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1039ec7d0>]}
2020-04-28 19:38:41.909269 (Thread-1): 12:38:41 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.65s]
2020-04-28 19:38:41.909449 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:38:41.909952 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:38:41.910203 (Thread-1): 12:38:41 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:38:41.910637 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:38:41.910764 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:38:41.910885 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:38:41.920482 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:38:41.920924 (Thread-1): finished collecting timing info
2020-04-28 19:38:41.929638 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:41.929847 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:38:42.097725 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:38:42.100336 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.100457 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:38:42.268367 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:38:42.271191 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:38:42.271873 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.272024 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:38:42.310917 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:42.311333 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.311615 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:38:42.370407 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:38:42.374811 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.374964 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:38:42.414536 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:42.415636 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:38:42.415764 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.415863 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:38:42.611622 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 19:38:42.614871 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.615023 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:38:42.813331 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:38:42.817388 (Thread-1): finished collecting timing info
2020-04-28 19:38:42.818235 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b2c610>]}
2020-04-28 19:38:42.818536 (Thread-1): 12:38:42 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 0.91s]
2020-04-28 19:38:42.818717 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:38:42.856429 (MainThread): Using postgres connection "master".
2020-04-28 19:38:42.856787 (MainThread): On master: BEGIN
2020-04-28 19:38:42.894748 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:42.894978 (MainThread): On master: COMMIT
2020-04-28 19:38:42.895092 (MainThread): Using postgres connection "master".
2020-04-28 19:38:42.895197 (MainThread): On master: COMMIT
2020-04-28 19:38:42.932281 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:38:42.933353 (MainThread): 12:38:42 | 
2020-04-28 19:38:42.933689 (MainThread): 12:38:42 | Finished running 7 view models in 16.83s.
2020-04-28 19:38:42.933983 (MainThread): Connection 'master' was left open.
2020-04-28 19:38:42.934196 (MainThread): On master: Close
2020-04-28 19:38:42.934692 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:38:42.934905 (MainThread): On model.order_history.customers: Close
2020-04-28 19:38:42.956208 (MainThread): 
2020-04-28 19:38:42.956442 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 19:38:42.956576 (MainThread): 
2020-04-28 19:38:42.956717 (MainThread): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
2020-04-28 19:38:42.956833 (MainThread):   syntax error at or near ")"
2020-04-28 19:38:42.956939 (MainThread):   LINE 17:   );
2020-04-28 19:38:42.957042 (MainThread):              ^
2020-04-28 19:38:42.957142 (MainThread):   compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:38:42.957254 (MainThread): 
Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
2020-04-28 19:38:42.957437 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b28950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa7e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c2a750>]}
2020-04-28 19:38:42.957643 (MainThread): Flushing usage events
2020-04-28 19:44:06.175445 (MainThread): Running with dbt=0.16.1
2020-04-28 19:44:06.252186 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:44:06.253405 (MainThread): Tracking: tracking
2020-04-28 19:44:06.259262 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d062ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2e7e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2e7f10>]}
2020-04-28 19:44:06.279470 (MainThread): Partial parsing not enabled
2020-04-28 19:44:06.281471 (MainThread): Parsing macros/core.sql
2020-04-28 19:44:06.286831 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:44:06.295475 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:44:06.297301 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:44:06.315610 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:44:06.349013 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:44:06.370492 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:44:06.372442 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:44:06.378956 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:44:06.392028 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:44:06.399060 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:44:06.405484 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:44:06.410344 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:44:06.411364 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:44:06.412436 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:44:06.414087 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:44:06.417490 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:44:06.427408 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:44:06.429432 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:44:06.430514 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:44:06.472704 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:44:06.474505 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:44:06.475577 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:44:06.477397 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:44:06.480113 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:44:06.482597 (MainThread): Parsing macros/relations.sql
2020-04-28 19:44:06.484622 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:44:06.507071 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:44:06.526089 (MainThread): Partial parsing not enabled
2020-04-28 19:44:06.556086 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:44:06.556211 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.573524 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:06.573641 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.577769 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:06.577864 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.582824 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:44:06.582920 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.587020 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:44:06.587117 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.592172 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:06.592267 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.597877 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:44:06.597991 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.745882 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:44:06.750072 (MainThread): 
2020-04-28 19:44:06.750477 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:44:06.750617 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.773215 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:44:06.773420 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:44:06.857130 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:44:06.857263 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:44:07.438743 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.58 seconds
2020-04-28 19:44:07.468295 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:44:07.468413 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:44:07.469705 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:44:07.469796 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:44:07.511966 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:07.512389 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:44:07.512650 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:44:07.634465 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-28 19:44:07.642760 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:44:07.714554 (MainThread): Using postgres connection "master".
2020-04-28 19:44:07.714707 (MainThread): On master: BEGIN
2020-04-28 19:44:08.075389 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 19:44:08.075789 (MainThread): Using postgres connection "master".
2020-04-28 19:44:08.075922 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:44:08.206074 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 19:44:08.282103 (MainThread): On master: ROLLBACK
2020-04-28 19:44:08.322576 (MainThread): Using postgres connection "master".
2020-04-28 19:44:08.322986 (MainThread): On master: BEGIN
2020-04-28 19:44:08.402541 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 19:44:08.402995 (MainThread): On master: COMMIT
2020-04-28 19:44:08.403303 (MainThread): Using postgres connection "master".
2020-04-28 19:44:08.403468 (MainThread): On master: COMMIT
2020-04-28 19:44:08.442500 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:44:08.442942 (MainThread): 12:44:08 | Concurrency: 1 threads (target='dev')
2020-04-28 19:44:08.443110 (MainThread): 12:44:08 | 
2020-04-28 19:44:08.445687 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:44:08.445909 (Thread-1): 12:44:08 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:44:08.446256 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.446376 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:44:08.446509 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:44:08.469001 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:44:08.469514 (Thread-1): finished collecting timing info
2020-04-28 19:44:08.506200 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.506352 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:44:08.582223 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:44:08.586874 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.587033 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:44:08.625097 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:44:08.627971 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:44:08.628666 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.628813 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:44:08.666875 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:08.667299 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.667536 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:44:08.720658 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:08.727065 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.727218 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:44:08.768471 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:08.771486 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.771625 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:44:08.809906 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:08.810971 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:44:08.811096 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.811192 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:44:08.984972 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:08.988480 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.988637 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:44:09.216111 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 19:44:09.220424 (Thread-1): finished collecting timing info
2020-04-28 19:44:09.221274 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6bf590>]}
2020-04-28 19:44:09.221588 (Thread-1): 12:44:09 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.77s]
2020-04-28 19:44:09.221772 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:44:09.221954 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:44:09.222133 (Thread-1): 12:44:09 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:44:09.222466 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.222593 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:44:09.222721 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:44:09.228722 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:44:09.229325 (Thread-1): finished collecting timing info
2020-04-28 19:44:09.236774 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.236898 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:44:09.423631 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:44:09.427861 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.428011 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:44:09.596843 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:09.599638 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:44:09.600205 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.600376 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:44:09.639600 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:09.640013 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.640264 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:44:09.687724 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:09.694071 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.694227 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:44:09.732829 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:09.737221 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.737370 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:44:09.799411 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 19:44:09.800581 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:44:09.800704 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.800803 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:44:10.020135 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 19:44:10.024913 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:10.025059 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:44:10.203794 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:44:10.207216 (Thread-1): finished collecting timing info
2020-04-28 19:44:10.207908 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d616dd0>]}
2020-04-28 19:44:10.208153 (Thread-1): 12:44:10 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.99s]
2020-04-28 19:44:10.208294 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:44:10.208444 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:44:10.208652 (Thread-1): 12:44:10 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:44:10.209303 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.209705 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:44:10.209890 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:44:10.247965 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:44:10.248455 (Thread-1): finished collecting timing info
2020-04-28 19:44:10.255827 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.255957 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:44:10.422210 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:10.424767 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.424889 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:44:10.589661 (Thread-1): SQL status: DROP VIEW in 0.16 seconds
2020-04-28 19:44:10.592632 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:44:10.593265 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.593408 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:44:10.631691 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:10.632105 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.632356 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:44:10.680641 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:10.686788 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.686938 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:44:10.725272 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:10.729367 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.729515 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:44:10.768134 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:10.770270 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:44:10.770461 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.770614 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:44:10.941420 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:10.944802 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.944947 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:44:11.120438 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:44:11.124531 (Thread-1): finished collecting timing info
2020-04-28 19:44:11.125375 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d616dd0>]}
2020-04-28 19:44:11.125675 (Thread-1): 12:44:11 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.92s]
2020-04-28 19:44:11.125849 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:44:11.126082 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:44:11.126454 (Thread-1): 12:44:11 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:44:11.127030 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.127196 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:44:11.127325 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:44:11.134784 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:44:11.135223 (Thread-1): finished collecting timing info
2020-04-28 19:44:11.143095 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.143248 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:44:11.312859 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:11.316983 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.317129 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:44:11.484192 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:11.486158 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:44:11.486609 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.486733 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:44:11.525037 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:11.525215 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.525310 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:44:11.573355 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:11.579604 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.579757 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:44:11.620013 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:11.624314 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.624464 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:44:11.663292 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:11.665221 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:44:11.665411 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.665560 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:44:11.833405 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:11.835748 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.835887 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:44:12.053323 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 19:44:12.057565 (Thread-1): finished collecting timing info
2020-04-28 19:44:12.058404 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9249d0>]}
2020-04-28 19:44:12.058699 (Thread-1): 12:44:12 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 0.93s]
2020-04-28 19:44:12.058872 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:44:12.059108 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:44:12.059436 (Thread-1): 12:44:12 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:44:12.059913 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.060143 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:44:12.060327 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:44:12.068017 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:44:12.068444 (Thread-1): finished collecting timing info
2020-04-28 19:44:12.075857 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.075978 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:44:12.287148 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:44:12.292402 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.292547 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:44:12.537107 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-28 19:44:12.539426 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:44:12.539889 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.540007 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:44:12.578022 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:12.578435 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.578686 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT *
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.email
)
select * from final
  );

2020-04-28 19:44:12.620939 (Thread-1): Postgres error: column "email" duplicated

2020-04-28 19:44:12.621355 (Thread-1): On model.order_history.customer_broker: ROLLBACK
2020-04-28 19:44:12.659060 (Thread-1): finished collecting timing info
2020-04-28 19:44:12.660085 (Thread-1): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  column "email" duplicated
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.DuplicateColumn: column "email" duplicated


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  column "email" duplicated
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:44:12.663033 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2fda90>]}
2020-04-28 19:44:12.663320 (Thread-1): 12:44:12 | 5 of 7 ERROR creating view model data_science.customer_broker........ [ERROR in 0.60s]
2020-04-28 19:44:12.663493 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:44:12.663734 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:44:12.664093 (Thread-1): 12:44:12 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:44:12.664486 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.664632 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:44:12.664766 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:44:12.674093 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:44:12.675241 (Thread-1): finished collecting timing info
2020-04-28 19:44:12.682679 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.682810 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:44:12.759735 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:44:12.763915 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.764073 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:44:12.802663 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:44:12.805567 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:44:12.806116 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.806267 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:44:12.843722 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:12.843913 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.844022 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:44:12.901889 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:44:12.904438 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.904549 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:44:12.943359 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:12.945273 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:44:12.945458 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.945610 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:44:13.118134 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:13.121492 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:13.121662 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:44:13.288945 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:13.293202 (Thread-1): finished collecting timing info
2020-04-28 19:44:13.294038 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d344b50>]}
2020-04-28 19:44:13.294334 (Thread-1): 12:44:13 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.63s]
2020-04-28 19:44:13.294509 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:44:13.294919 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:44:13.295130 (Thread-1): 12:44:13 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:44:13.295484 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:44:13.295611 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:44:13.295736 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:44:13.304713 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:44:13.306195 (Thread-1): finished collecting timing info
2020-04-28 19:44:13.315128 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.315282 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:44:13.481452 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:13.485609 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.485756 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:44:13.656656 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:13.659483 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:44:13.660149 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.660303 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:44:13.698171 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:13.698607 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.698890 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:44:13.752581 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:13.757016 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.757164 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:44:13.797772 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:13.799708 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:44:13.799897 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.800052 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:44:13.969533 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:13.972898 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.973045 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:44:14.144194 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:14.148436 (Thread-1): finished collecting timing info
2020-04-28 19:44:14.149276 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d472d50>]}
2020-04-28 19:44:14.149573 (Thread-1): 12:44:14 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 0.85s]
2020-04-28 19:44:14.149745 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:44:14.194507 (MainThread): Using postgres connection "master".
2020-04-28 19:44:14.194857 (MainThread): On master: BEGIN
2020-04-28 19:44:14.234526 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:14.234784 (MainThread): On master: COMMIT
2020-04-28 19:44:14.234928 (MainThread): Using postgres connection "master".
2020-04-28 19:44:14.235061 (MainThread): On master: COMMIT
2020-04-28 19:44:14.274374 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:44:14.275051 (MainThread): 12:44:14 | 
2020-04-28 19:44:14.275283 (MainThread): 12:44:14 | Finished running 7 view models in 7.52s.
2020-04-28 19:44:14.275438 (MainThread): Connection 'master' was left open.
2020-04-28 19:44:14.275557 (MainThread): On master: Close
2020-04-28 19:44:14.275867 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:44:14.275990 (MainThread): On model.order_history.customers: Close
2020-04-28 19:44:14.295719 (MainThread): 
2020-04-28 19:44:14.295928 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 19:44:14.296055 (MainThread): 
2020-04-28 19:44:14.296164 (MainThread): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
2020-04-28 19:44:14.296264 (MainThread):   column "email" duplicated
2020-04-28 19:44:14.296356 (MainThread):   compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:44:14.296459 (MainThread): 
Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
2020-04-28 19:44:14.296628 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5c2a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4bae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d33a910>]}
2020-04-28 19:44:14.296811 (MainThread): Flushing usage events
2020-04-28 19:45:23.546959 (MainThread): Running with dbt=0.16.1
2020-04-28 19:45:23.610533 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:45:23.611285 (MainThread): Tracking: tracking
2020-04-28 19:45:23.617144 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbffed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c9b64d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c9b6a50>]}
2020-04-28 19:45:23.643166 (MainThread): Partial parsing not enabled
2020-04-28 19:45:23.645747 (MainThread): Parsing macros/core.sql
2020-04-28 19:45:23.651016 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:45:23.659371 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:45:23.661182 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:45:23.678898 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:45:23.711754 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:45:23.733152 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:45:23.735076 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:45:23.741440 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:45:23.754203 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:45:23.761209 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:45:23.767664 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:45:23.772694 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:45:23.773919 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:45:23.775038 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:45:23.776765 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:45:23.778900 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:45:23.788147 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:45:23.790193 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:45:23.791296 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:45:23.832351 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:45:23.833491 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:45:23.834406 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:45:23.835491 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:45:23.837705 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:45:23.841255 (MainThread): Parsing macros/relations.sql
2020-04-28 19:45:23.842717 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:45:23.860439 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:45:23.877832 (MainThread): Partial parsing not enabled
2020-04-28 19:45:23.904679 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:45:23.904779 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.921105 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:23.921219 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.925180 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:23.925270 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.930438 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:45:23.930543 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.935348 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:45:23.935453 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.940461 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:23.940585 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.948448 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:45:23.948645 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:24.089847 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:45:24.093385 (MainThread): 
2020-04-28 19:45:24.093825 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:45:24.093918 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:24.115320 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:45:24.115530 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:45:24.198831 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:45:24.198971 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:45:24.716833 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.52 seconds
2020-04-28 19:45:24.748914 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:45:24.749054 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:45:24.750525 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:45:24.750630 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:45:24.792547 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:24.792996 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:45:24.793296 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:45:24.911065 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-28 19:45:24.917601 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:45:24.984638 (MainThread): Using postgres connection "master".
2020-04-28 19:45:24.984764 (MainThread): On master: BEGIN
2020-04-28 19:45:25.378905 (MainThread): SQL status: BEGIN in 0.39 seconds
2020-04-28 19:45:25.379649 (MainThread): Using postgres connection "master".
2020-04-28 19:45:25.379826 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:45:25.563854 (MainThread): SQL status: SELECT in 0.18 seconds
2020-04-28 19:45:25.650117 (MainThread): On master: ROLLBACK
2020-04-28 19:45:25.691645 (MainThread): Using postgres connection "master".
2020-04-28 19:45:25.692058 (MainThread): On master: BEGIN
2020-04-28 19:45:25.776018 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 19:45:25.776486 (MainThread): On master: COMMIT
2020-04-28 19:45:25.776785 (MainThread): Using postgres connection "master".
2020-04-28 19:45:25.776945 (MainThread): On master: COMMIT
2020-04-28 19:45:25.820066 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:45:25.820952 (MainThread): 12:45:25 | Concurrency: 1 threads (target='dev')
2020-04-28 19:45:25.821199 (MainThread): 12:45:25 | 
2020-04-28 19:45:25.823527 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:45:25.823825 (Thread-1): 12:45:25 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:45:25.824233 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:25.824380 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:45:25.824534 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:45:25.841627 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:45:25.842114 (Thread-1): finished collecting timing info
2020-04-28 19:45:25.882758 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:25.882926 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:45:25.965824 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:45:25.969105 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:25.969235 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:45:26.011162 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:45:26.014037 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:45:26.014706 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.014863 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:45:26.055832 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:26.056128 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.056303 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:45:26.118262 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:26.124613 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.124774 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:45:26.170948 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:26.175137 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.175294 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:45:26.217869 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:26.219887 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:45:26.220090 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.220256 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:45:26.396125 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:45:26.399504 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.399673 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:45:26.603845 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:45:26.607390 (Thread-1): finished collecting timing info
2020-04-28 19:45:26.608218 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cff9990>]}
2020-04-28 19:45:26.608505 (Thread-1): 12:45:26 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.78s]
2020-04-28 19:45:26.608672 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:45:26.608842 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:45:26.609121 (Thread-1): 12:45:26 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:45:26.609562 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:26.609697 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:45:26.609816 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:45:26.615992 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:45:26.616426 (Thread-1): finished collecting timing info
2020-04-28 19:45:26.624062 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:26.624210 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:45:26.794650 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:45:26.798786 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:26.798947 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:45:26.975625 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:26.977600 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:45:26.978122 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:26.978258 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:45:27.020053 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:27.020450 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.020666 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:45:27.084131 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:27.089621 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.089752 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:45:27.132306 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:27.136768 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.136926 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:45:27.183322 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:27.185297 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:45:27.185502 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.185663 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:45:27.356961 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:45:27.359922 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.360041 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:45:27.543408 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:27.547610 (Thread-1): finished collecting timing info
2020-04-28 19:45:27.548452 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce2af10>]}
2020-04-28 19:45:27.548759 (Thread-1): 12:45:27 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.94s]
2020-04-28 19:45:27.548940 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:45:27.549237 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:45:27.549687 (Thread-1): 12:45:27 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:45:27.550215 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:45:27.550346 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:45:27.550468 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:45:27.588965 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:45:27.589513 (Thread-1): finished collecting timing info
2020-04-28 19:45:27.597260 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:27.597401 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:45:27.782895 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:45:27.786965 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:27.787122 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:45:29.035749 (Thread-1): SQL status: DROP VIEW in 1.25 seconds
2020-04-28 19:45:29.038805 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:45:29.039444 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:29.039601 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:45:29.081142 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:29.081420 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:29.081568 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:45:31.833280 (Thread-1): SQL status: CREATE VIEW in 2.75 seconds
2020-04-28 19:45:31.839474 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:31.839635 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:45:31.888574 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:31.892836 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:31.892993 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:45:31.940283 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:31.942421 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:45:31.942625 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:31.942790 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:45:32.119216 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:45:32.122585 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:32.122745 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:45:32.304277 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:32.307241 (Thread-1): finished collecting timing info
2020-04-28 19:45:32.307959 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce2af10>]}
2020-04-28 19:45:32.308222 (Thread-1): 12:45:32 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 4.76s]
2020-04-28 19:45:32.308378 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:45:32.308543 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:45:32.308963 (Thread-1): 12:45:32 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:45:32.309459 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.309597 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:45:32.309724 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:45:32.316924 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:45:32.317416 (Thread-1): finished collecting timing info
2020-04-28 19:45:32.324885 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.325071 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:45:32.563404 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-28 19:45:32.567382 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.567539 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:45:32.786954 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 19:45:32.789923 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:45:32.790531 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.790691 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:45:32.833330 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:32.833765 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.834043 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:45:32.891413 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:32.897396 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.897557 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:45:32.945071 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:32.949346 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.949502 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:45:32.992171 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:32.993429 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:45:32.993580 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.993694 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:45:33.167532 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:45:33.171069 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:33.171224 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:45:33.352340 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:33.356416 (Thread-1): finished collecting timing info
2020-04-28 19:45:33.357523 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cca4dd0>]}
2020-04-28 19:45:33.357887 (Thread-1): 12:45:33 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 1.05s]
2020-04-28 19:45:33.358118 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:45:33.358354 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:45:33.358769 (Thread-1): 12:45:33 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:45:33.359278 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.359472 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:45:33.359751 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:45:33.368159 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:45:33.368569 (Thread-1): finished collecting timing info
2020-04-28 19:45:33.375165 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.375284 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:45:33.549758 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:45:33.554994 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.555145 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:45:33.900629 (Thread-1): SQL status: DROP VIEW in 0.35 seconds
2020-04-28 19:45:33.903627 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:45:33.904221 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.904375 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:45:33.949087 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:33.949511 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.949772 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.email
)
select * from final
  );

2020-04-28 19:45:33.994719 (Thread-1): Postgres error: column reference "email" is ambiguous

2020-04-28 19:45:33.995140 (Thread-1): On model.order_history.customer_broker: ROLLBACK
2020-04-28 19:45:34.052386 (Thread-1): finished collecting timing info
2020-04-28 19:45:34.053066 (Thread-1): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  column reference "email" is ambiguous
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.AmbiguousColumn: column reference "email" is ambiguous


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  column reference "email" is ambiguous
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:45:34.055619 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ccdbc90>]}
2020-04-28 19:45:34.055846 (Thread-1): 12:45:34 | 5 of 7 ERROR creating view model data_science.customer_broker........ [ERROR in 0.70s]
2020-04-28 19:45:34.055982 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:45:34.056120 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:45:34.056365 (Thread-1): 12:45:34 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:45:34.056662 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.056759 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:45:34.056854 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:45:34.064680 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:45:34.065057 (Thread-1): finished collecting timing info
2020-04-28 19:45:34.071575 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.071742 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:45:34.155110 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:45:34.159315 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.159465 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:45:34.201339 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:45:34.204374 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:45:34.205000 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.205154 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:45:34.246977 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:34.247396 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.247661 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:45:34.309855 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:34.313942 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.314110 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:45:34.355937 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:34.357111 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:45:34.357242 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.357345 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:45:34.534523 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:45:34.537891 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.538044 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:45:34.713230 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:34.715885 (Thread-1): finished collecting timing info
2020-04-28 19:45:34.716545 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cca1810>]}
2020-04-28 19:45:34.716790 (Thread-1): 12:45:34 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.66s]
2020-04-28 19:45:34.716922 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:45:34.717281 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:45:34.717449 (Thread-1): 12:45:34 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:45:34.717724 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:45:34.717820 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:45:34.717916 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:45:34.725884 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:45:34.726259 (Thread-1): finished collecting timing info
2020-04-28 19:45:34.733937 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:34.734084 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:45:34.912432 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:34.916554 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:34.916709 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:45:35.112354 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:45:35.115152 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:45:35.115802 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.115951 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:45:35.157029 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:35.157450 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.157714 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:45:35.221953 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:35.226370 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.226524 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:45:35.268792 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:35.270716 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:45:35.270910 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.271065 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:45:35.555459 (Thread-1): SQL status: COMMIT in 0.28 seconds
2020-04-28 19:45:35.558698 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.558853 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:45:37.819568 (Thread-1): SQL status: DROP VIEW in 2.26 seconds
2020-04-28 19:45:37.822526 (Thread-1): finished collecting timing info
2020-04-28 19:45:37.823267 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc78bd0>]}
2020-04-28 19:45:37.823531 (Thread-1): 12:45:37 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 3.11s]
2020-04-28 19:45:37.823685 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:45:37.858665 (MainThread): Using postgres connection "master".
2020-04-28 19:45:37.858894 (MainThread): On master: BEGIN
2020-04-28 19:45:37.900985 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:37.901237 (MainThread): On master: COMMIT
2020-04-28 19:45:37.901347 (MainThread): Using postgres connection "master".
2020-04-28 19:45:37.901449 (MainThread): On master: COMMIT
2020-04-28 19:45:37.942834 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:45:37.943684 (MainThread): 12:45:37 | 
2020-04-28 19:45:37.943926 (MainThread): 12:45:37 | Finished running 7 view models in 13.85s.
2020-04-28 19:45:37.944120 (MainThread): Connection 'master' was left open.
2020-04-28 19:45:37.944274 (MainThread): On master: Close
2020-04-28 19:45:37.944668 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:45:37.944832 (MainThread): On model.order_history.customers: Close
2020-04-28 19:45:37.966809 (MainThread): 
2020-04-28 19:45:37.967033 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 19:45:37.967178 (MainThread): 
2020-04-28 19:45:37.967311 (MainThread): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
2020-04-28 19:45:37.967433 (MainThread):   column reference "email" is ambiguous
2020-04-28 19:45:37.967547 (MainThread):   compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:45:37.967670 (MainThread): 
Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
2020-04-28 19:45:37.967866 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce8fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce95310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce8ba90>]}
2020-04-28 19:45:37.968077 (MainThread): Flushing usage events
2020-04-28 19:46:02.694113 (MainThread): Running with dbt=0.16.1
2020-04-28 19:46:02.759550 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:46:02.760456 (MainThread): Tracking: tracking
2020-04-28 19:46:02.765588 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee5e390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee5ca50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ebd36d0>]}
2020-04-28 19:46:02.783630 (MainThread): Partial parsing not enabled
2020-04-28 19:46:02.785430 (MainThread): Parsing macros/core.sql
2020-04-28 19:46:02.789968 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:46:02.798226 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:46:02.800039 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:46:02.818196 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:46:02.851595 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:46:02.872914 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:46:02.874844 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:46:02.881216 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:46:02.893989 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:46:02.900974 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:46:02.907451 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:46:02.912278 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:46:02.913280 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:46:02.914543 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:46:02.916190 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:46:02.918225 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:46:02.927181 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:46:02.929139 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:46:02.930182 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:46:02.971579 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:46:02.972771 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:46:02.973924 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:46:02.975646 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:46:02.978376 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:46:02.981703 (MainThread): Parsing macros/relations.sql
2020-04-28 19:46:02.983228 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:46:03.005929 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:46:03.024389 (MainThread): Partial parsing not enabled
2020-04-28 19:46:03.051905 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:46:03.052010 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.067843 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:03.067939 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.071863 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:03.071953 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.076232 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:46:03.076320 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.080249 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:46:03.080335 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.084888 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:03.084993 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.090527 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:46:03.090625 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.223387 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:46:03.226978 (MainThread): 
2020-04-28 19:46:03.227414 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:46:03.227499 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.248389 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:46:03.248784 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:46:03.342101 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:46:03.342242 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:46:03.766262 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.42 seconds
2020-04-28 19:46:03.801255 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:46:03.801377 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:46:03.803018 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:46:03.803136 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:46:03.860360 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.06 seconds
2020-04-28 19:46:03.860788 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:46:03.861057 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:46:03.954190 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-28 19:46:03.960328 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:46:04.024057 (MainThread): Using postgres connection "master".
2020-04-28 19:46:04.024190 (MainThread): On master: BEGIN
2020-04-28 19:46:04.365433 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-04-28 19:46:04.366169 (MainThread): Using postgres connection "master".
2020-04-28 19:46:04.366375 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:46:04.494207 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 19:46:04.573240 (MainThread): On master: ROLLBACK
2020-04-28 19:46:04.610903 (MainThread): Using postgres connection "master".
2020-04-28 19:46:04.611315 (MainThread): On master: BEGIN
2020-04-28 19:46:04.684848 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 19:46:04.685038 (MainThread): On master: COMMIT
2020-04-28 19:46:04.685142 (MainThread): Using postgres connection "master".
2020-04-28 19:46:04.685231 (MainThread): On master: COMMIT
2020-04-28 19:46:04.721777 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:46:04.722672 (MainThread): 12:46:04 | Concurrency: 1 threads (target='dev')
2020-04-28 19:46:04.722920 (MainThread): 12:46:04 | 
2020-04-28 19:46:04.725149 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:46:04.725418 (Thread-1): 12:46:04 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:46:04.725801 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.725936 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:46:04.726083 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:46:04.741209 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:46:04.741622 (Thread-1): finished collecting timing info
2020-04-28 19:46:04.778669 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.778834 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:46:04.864669 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-28 19:46:04.869659 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.869814 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:46:04.910108 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:46:04.913027 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:46:04.913686 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.913840 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:46:04.953321 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:04.953555 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.953691 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:46:05.009511 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:46:05.013733 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:05.013858 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:46:05.054655 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:05.058968 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:05.059124 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:46:05.099793 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:05.101751 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:46:05.101949 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:05.102110 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:46:05.280446 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:46:05.283912 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:05.284076 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:46:05.517048 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 19:46:05.521327 (Thread-1): finished collecting timing info
2020-04-28 19:46:05.522189 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10effe7d0>]}
2020-04-28 19:46:05.522504 (Thread-1): 12:46:05 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.80s]
2020-04-28 19:46:05.522692 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:46:05.522877 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:46:05.523060 (Thread-1): 12:46:05 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:46:05.523518 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.523802 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:46:05.524099 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:46:05.530294 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:46:05.530724 (Thread-1): finished collecting timing info
2020-04-28 19:46:05.538235 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.538384 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:46:05.713845 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:05.717979 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.718142 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:46:05.890051 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:05.893109 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:46:05.893717 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.893876 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:46:05.934429 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:05.934862 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.935129 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:46:05.990578 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:46:05.995501 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.995693 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:46:06.036421 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:06.039886 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:06.040016 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:46:06.086477 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:46:06.088492 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:46:06.088688 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:06.088847 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:46:06.310166 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 19:46:06.313704 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:06.313844 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:46:06.498662 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:06.502906 (Thread-1): finished collecting timing info
2020-04-28 19:46:06.503754 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10efe9310>]}
2020-04-28 19:46:06.504063 (Thread-1): 12:46:06 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.98s]
2020-04-28 19:46:06.504248 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:46:06.504491 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:46:06.504871 (Thread-1): 12:46:06 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:46:06.505366 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:46:06.505538 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:46:06.505714 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:46:06.542806 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:46:06.543404 (Thread-1): finished collecting timing info
2020-04-28 19:46:06.551633 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:06.551838 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:46:08.018626 (Thread-1): SQL status: DROP VIEW in 1.47 seconds
2020-04-28 19:46:08.022726 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:08.022881 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:46:08.201924 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:08.204899 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:46:08.205548 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:08.205704 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:46:08.246115 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:08.246323 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:08.246440 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:46:09.230747 (Thread-1): SQL status: CREATE VIEW in 0.98 seconds
2020-04-28 19:46:09.236901 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:09.237054 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:46:09.283087 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:46:09.287236 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:09.287436 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:46:09.329383 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:09.331519 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:46:09.331717 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:09.331880 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:46:09.505000 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:46:09.508380 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:09.508548 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:46:09.686736 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:09.690985 (Thread-1): finished collecting timing info
2020-04-28 19:46:09.691837 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eed5c50>]}
2020-04-28 19:46:09.692144 (Thread-1): 12:46:09 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 3.19s]
2020-04-28 19:46:09.692324 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:46:09.692509 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:46:09.692944 (Thread-1): 12:46:09 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:46:09.693654 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:46:09.693810 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:46:09.693980 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:46:09.700928 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:46:09.701342 (Thread-1): finished collecting timing info
2020-04-28 19:46:09.708188 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:09.708306 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:46:09.878071 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:09.882185 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:09.882335 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:46:10.058534 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:10.061588 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:46:10.062219 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.062387 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:46:10.102812 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:10.103244 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.103524 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:46:10.155248 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:46:10.159593 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.159719 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:46:10.201578 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:10.204218 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.204332 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:46:10.245306 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:10.246345 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:46:10.246461 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.246554 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:46:10.417436 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:46:10.420982 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.421139 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:46:10.596802 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:10.599676 (Thread-1): finished collecting timing info
2020-04-28 19:46:10.600404 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f0b5750>]}
2020-04-28 19:46:10.600661 (Thread-1): 12:46:10 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 0.91s]
2020-04-28 19:46:10.600811 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:46:10.600964 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:46:10.601203 (Thread-1): 12:46:10 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:46:10.601508 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:10.601614 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:46:10.601717 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:46:10.608842 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:46:10.609273 (Thread-1): finished collecting timing info
2020-04-28 19:46:10.616515 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:10.616728 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:46:10.787275 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:10.792552 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:10.792713 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:46:11.007862 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:46:11.010935 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:46:11.011553 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.011710 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:46:11.052240 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:11.052668 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.052938 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-28 19:46:11.107733 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:46:11.111995 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.112151 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-28 19:46:11.153079 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:11.155043 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 19:46:11.155237 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.155397 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 19:46:11.338627 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:46:11.341915 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.342066 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:46:11.514284 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:11.518516 (Thread-1): finished collecting timing info
2020-04-28 19:46:11.519364 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee63310>]}
2020-04-28 19:46:11.519671 (Thread-1): 12:46:11 | 5 of 7 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.92s]
2020-04-28 19:46:11.519853 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:46:11.520036 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:46:11.520465 (Thread-1): 12:46:11 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:46:11.521007 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.521199 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:46:11.521337 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:46:11.530361 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:46:11.530793 (Thread-1): finished collecting timing info
2020-04-28 19:46:11.538095 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.538233 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:46:11.707667 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:11.711880 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.712032 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:46:11.893800 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:11.896777 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:46:11.897401 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.897563 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:46:11.938078 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:11.938529 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.938821 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:46:11.995086 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:46:11.997677 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.997794 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:46:12.038499 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:12.040362 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:46:12.040570 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:12.040733 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:46:12.240888 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 19:46:12.244260 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:12.244415 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:46:12.418496 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:12.422733 (Thread-1): finished collecting timing info
2020-04-28 19:46:12.423577 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f00f390>]}
2020-04-28 19:46:12.423884 (Thread-1): 12:46:12 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.90s]
2020-04-28 19:46:12.424065 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:46:12.424525 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:46:12.424743 (Thread-1): 12:46:12 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:46:12.425143 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:46:12.425283 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:46:12.425409 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:46:12.435525 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:46:12.435932 (Thread-1): finished collecting timing info
2020-04-28 19:46:12.443218 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.443358 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:46:12.640270 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:46:12.643147 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.643286 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:46:12.814798 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:12.817770 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:46:12.818383 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.818541 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:46:12.858854 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:12.859286 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.859555 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:46:12.920391 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:46:12.924665 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.924848 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:46:12.978335 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:46:12.979536 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:46:12.979669 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.979774 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:46:13.149259 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:46:13.152664 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:13.152808 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:46:13.321715 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:13.324698 (Thread-1): finished collecting timing info
2020-04-28 19:46:13.325437 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f157ad0>]}
2020-04-28 19:46:13.325695 (Thread-1): 12:46:13 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 0.90s]
2020-04-28 19:46:13.325844 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:46:13.378631 (MainThread): Using postgres connection "master".
2020-04-28 19:46:13.378946 (MainThread): On master: BEGIN
2020-04-28 19:46:13.417960 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:13.418411 (MainThread): On master: COMMIT
2020-04-28 19:46:13.418688 (MainThread): Using postgres connection "master".
2020-04-28 19:46:13.418867 (MainThread): On master: COMMIT
2020-04-28 19:46:13.455978 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:46:13.456902 (MainThread): 12:46:13 | 
2020-04-28 19:46:13.457135 (MainThread): 12:46:13 | Finished running 7 view models in 10.23s.
2020-04-28 19:46:13.457322 (MainThread): Connection 'master' was left open.
2020-04-28 19:46:13.457470 (MainThread): On master: Close
2020-04-28 19:46:13.457866 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:46:13.458024 (MainThread): On model.order_history.customers: Close
2020-04-28 19:46:13.480051 (MainThread): 
2020-04-28 19:46:13.480276 (MainThread): Completed successfully
2020-04-28 19:46:13.480425 (MainThread): 
Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
2020-04-28 19:46:13.480626 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ebe8790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f3e79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef0f090>]}
2020-04-28 19:46:13.480840 (MainThread): Flushing usage events
2020-04-28 19:48:19.861947 (MainThread): Running with dbt=0.16.1
2020-04-28 19:48:19.928666 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:48:19.929699 (MainThread): Tracking: tracking
2020-04-28 19:48:19.934931 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c4bf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c74310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c74c10>]}
2020-04-28 19:48:19.957433 (MainThread): Partial parsing not enabled
2020-04-28 19:48:19.961002 (MainThread): Parsing macros/core.sql
2020-04-28 19:48:19.969689 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:48:19.980078 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:48:19.982038 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:48:20.000417 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:48:20.034664 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:48:20.056621 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:48:20.058644 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:48:20.065223 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:48:20.078431 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:48:20.085587 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:48:20.092176 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:48:20.097383 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:48:20.098401 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:48:20.100115 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:48:20.102072 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:48:20.104467 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:48:20.115100 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:48:20.117267 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:48:20.118380 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:48:20.159399 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:48:20.160595 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:48:20.161530 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:48:20.162636 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:48:20.164891 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:48:20.167270 (MainThread): Parsing macros/relations.sql
2020-04-28 19:48:20.168631 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:48:20.185511 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:48:20.203108 (MainThread): Partial parsing not enabled
2020-04-28 19:48:20.229492 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:48:20.229591 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.244753 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:20.244841 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.248918 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:20.249005 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.253260 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:48:20.253345 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.257098 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:48:20.257181 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.261949 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:20.262047 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.266903 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:48:20.266995 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.412896 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:48:20.417582 (MainThread): 
2020-04-28 19:48:20.417895 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:48:20.418014 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.439895 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:48:20.440035 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:48:20.522191 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:48:20.522324 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:48:20.965065 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.44 seconds
2020-04-28 19:48:20.999443 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:48:20.999581 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:48:21.001153 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:48:21.001269 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:48:21.416658 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.42 seconds
2020-04-28 19:48:21.417076 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:48:21.417327 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:48:21.660794 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.24 seconds
2020-04-28 19:48:21.669891 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:48:21.745686 (MainThread): Using postgres connection "master".
2020-04-28 19:48:21.745848 (MainThread): On master: BEGIN
2020-04-28 19:48:22.114600 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-28 19:48:22.114820 (MainThread): Using postgres connection "master".
2020-04-28 19:48:22.114953 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:48:24.218129 (MainThread): SQL status: SELECT in 2.10 seconds
2020-04-28 19:48:24.294575 (MainThread): On master: ROLLBACK
2020-04-28 19:48:24.335860 (MainThread): Using postgres connection "master".
2020-04-28 19:48:24.336263 (MainThread): On master: BEGIN
2020-04-28 19:48:24.422451 (MainThread): SQL status: BEGIN in 0.09 seconds
2020-04-28 19:48:24.422789 (MainThread): On master: COMMIT
2020-04-28 19:48:24.422982 (MainThread): Using postgres connection "master".
2020-04-28 19:48:24.423173 (MainThread): On master: COMMIT
2020-04-28 19:48:24.463008 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:48:24.463454 (MainThread): 12:48:24 | Concurrency: 1 threads (target='dev')
2020-04-28 19:48:24.463629 (MainThread): 12:48:24 | 
2020-04-28 19:48:24.465502 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:48:24.465910 (Thread-1): 12:48:24 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:48:24.466284 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.466416 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:48:24.466543 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:48:24.482836 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:48:24.483479 (Thread-1): finished collecting timing info
2020-04-28 19:48:24.525082 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.525234 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:48:24.609716 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:48:24.614778 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.614930 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:48:24.655585 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:48:24.658523 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:48:24.659174 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.659327 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:48:24.699440 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:24.699840 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.700106 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:48:24.760767 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:24.767121 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.767272 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:48:24.812598 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:48:24.815617 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.815759 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:48:24.859876 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:24.861730 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:48:24.861927 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.862086 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:48:25.129006 (Thread-1): SQL status: COMMIT in 0.27 seconds
2020-04-28 19:48:25.132281 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:25.132452 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:48:25.608688 (Thread-1): SQL status: DROP VIEW in 0.48 seconds
2020-04-28 19:48:25.612985 (Thread-1): finished collecting timing info
2020-04-28 19:48:25.613841 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10447eb50>]}
2020-04-28 19:48:25.614162 (Thread-1): 12:48:25 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.15s]
2020-04-28 19:48:25.614350 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:48:25.614613 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:48:25.615022 (Thread-1): 12:48:25 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:48:25.615652 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:25.615821 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:48:25.616008 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:48:25.622088 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:48:25.622541 (Thread-1): finished collecting timing info
2020-04-28 19:48:25.629908 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:25.630036 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:48:25.799323 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:25.803552 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:25.803711 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:48:25.975498 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:25.978569 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:48:25.979175 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:25.979337 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:48:26.019876 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:26.020175 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.020350 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:48:26.082822 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:26.089098 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.089254 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:48:26.130126 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:26.134475 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.134631 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:48:26.178903 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:26.180108 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:48:26.180240 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.180345 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:48:26.350796 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:48:26.355538 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.355692 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:48:26.543384 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:48:26.545948 (Thread-1): finished collecting timing info
2020-04-28 19:48:26.546574 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10451eb10>]}
2020-04-28 19:48:26.546801 (Thread-1): 12:48:26 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.93s]
2020-04-28 19:48:26.546935 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:48:26.547080 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:48:26.547335 (Thread-1): 12:48:26 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:48:26.547684 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.547798 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:48:26.547902 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:48:26.582748 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:48:26.583235 (Thread-1): finished collecting timing info
2020-04-28 19:48:26.591226 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.591439 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:48:26.776841 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:48:26.780948 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.781103 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:48:26.954118 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:26.957108 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:48:26.957718 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.957876 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:48:26.997142 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:26.997568 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.997831 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:48:27.053397 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:27.059525 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:27.059682 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:48:27.102157 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:27.106459 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:27.106616 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:48:27.146388 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:27.148483 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:48:27.148679 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:27.148837 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:48:27.319941 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:48:27.323320 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:27.323474 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:48:27.500577 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:48:27.504047 (Thread-1): finished collecting timing info
2020-04-28 19:48:27.504878 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103eeb650>]}
2020-04-28 19:48:27.505172 (Thread-1): 12:48:27 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.96s]
2020-04-28 19:48:27.505346 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:48:27.505542 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:48:27.505777 (Thread-1): 12:48:27 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:48:27.506422 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.506572 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:48:27.506702 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:48:27.514397 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:48:27.514862 (Thread-1): finished collecting timing info
2020-04-28 19:48:27.522497 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.522650 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:48:27.696138 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:27.700290 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.700444 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:48:27.917236 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 19:48:27.918965 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:48:27.919381 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.919493 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:48:27.958565 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:27.958993 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.959262 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:48:28.010639 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:48:28.016020 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:28.016177 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:48:28.058788 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:28.063175 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:28.063329 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:48:28.109392 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:48:28.111216 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:48:28.111418 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:28.111579 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:48:28.317864 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-28 19:48:28.321225 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:28.321384 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:48:28.496773 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:48:28.499765 (Thread-1): finished collecting timing info
2020-04-28 19:48:28.500641 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10412a310>]}
2020-04-28 19:48:28.500967 (Thread-1): 12:48:28 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 0.99s]
2020-04-28 19:48:28.501128 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:48:28.501297 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:48:28.501551 (Thread-1): 12:48:28 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:48:28.501885 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.502003 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:48:28.502117 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:48:28.510646 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:48:28.511181 (Thread-1): finished collecting timing info
2020-04-28 19:48:28.519367 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.519551 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:48:28.692350 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:28.697750 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.697906 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:48:28.867120 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:28.869088 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:48:28.869607 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.869735 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:48:28.908462 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:28.908646 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.908746 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    broker_email,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-28 19:48:28.961234 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:48:28.965323 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.965482 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-28 19:48:29.005233 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:29.007156 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 19:48:29.007352 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:29.007510 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 19:48:29.176227 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:48:29.178538 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:29.178672 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:48:29.385218 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:48:29.388945 (Thread-1): finished collecting timing info
2020-04-28 19:48:29.389760 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104062310>]}
2020-04-28 19:48:29.390022 (Thread-1): 12:48:29 | 5 of 7 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.89s]
2020-04-28 19:48:29.390179 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:48:29.390339 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:48:29.390497 (Thread-1): 12:48:29 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:48:29.390918 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.391036 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:48:29.391147 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:48:29.399959 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:48:29.400379 (Thread-1): finished collecting timing info
2020-04-28 19:48:29.408072 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.408223 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:48:29.587853 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:48:29.592376 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.592550 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:48:29.785502 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:48:29.787421 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:48:29.787900 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.788024 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:48:29.827283 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:29.827471 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.827573 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:48:29.882873 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:29.887132 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.887285 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:48:29.927605 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:29.929556 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:48:29.929755 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.929915 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:48:30.099456 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:48:30.102845 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:30.102996 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:48:30.308956 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:48:30.313159 (Thread-1): finished collecting timing info
2020-04-28 19:48:30.313994 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041a9a50>]}
2020-04-28 19:48:30.314296 (Thread-1): 12:48:30 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.92s]
2020-04-28 19:48:30.314476 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:48:30.314918 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:48:30.315140 (Thread-1): 12:48:30 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:48:30.315489 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:48:30.315666 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:48:30.315832 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:48:30.325997 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:48:30.326407 (Thread-1): finished collecting timing info
2020-04-28 19:48:30.333677 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:30.333814 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:48:30.778695 (Thread-1): SQL status: DROP VIEW in 0.44 seconds
2020-04-28 19:48:30.782519 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:30.782718 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:48:30.993952 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:48:30.996695 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:48:30.997372 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:30.997528 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:48:31.059682 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-04-28 19:48:31.060137 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:31.060313 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:48:31.118989 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:31.122144 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:31.122407 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:48:31.162662 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:31.163933 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:48:31.164092 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:31.164191 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:48:31.341501 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:48:31.345050 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:31.345209 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:48:31.872539 (Thread-1): SQL status: DROP VIEW in 0.53 seconds
2020-04-28 19:48:31.876610 (Thread-1): finished collecting timing info
2020-04-28 19:48:31.877456 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104448850>]}
2020-04-28 19:48:31.877761 (Thread-1): 12:48:31 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 1.56s]
2020-04-28 19:48:31.877944 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:48:31.948037 (MainThread): Using postgres connection "master".
2020-04-28 19:48:31.948382 (MainThread): On master: BEGIN
2020-04-28 19:48:31.989082 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:31.989538 (MainThread): On master: COMMIT
2020-04-28 19:48:31.989828 (MainThread): Using postgres connection "master".
2020-04-28 19:48:31.989983 (MainThread): On master: COMMIT
2020-04-28 19:48:32.030554 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:48:32.031494 (MainThread): 12:48:32 | 
2020-04-28 19:48:32.031733 (MainThread): 12:48:32 | Finished running 7 view models in 11.61s.
2020-04-28 19:48:32.031926 (MainThread): Connection 'master' was left open.
2020-04-28 19:48:32.032081 (MainThread): On master: Close
2020-04-28 19:48:32.032530 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:48:32.032702 (MainThread): On model.order_history.customers: Close
2020-04-28 19:48:32.054468 (MainThread): 
2020-04-28 19:48:32.054682 (MainThread): Completed successfully
2020-04-28 19:48:32.054828 (MainThread): 
Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
2020-04-28 19:48:32.055027 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104141d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041bd610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104099350>]}
2020-04-28 19:48:32.055251 (MainThread): Flushing usage events
2020-04-28 20:55:15.567732 (MainThread): Running with dbt=0.16.1
2020-04-28 20:55:15.642929 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 20:55:15.643745 (MainThread): Tracking: tracking
2020-04-28 20:55:15.649325 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10909f110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092e8bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086e6d50>]}
2020-04-28 20:55:15.668859 (MainThread): Partial parsing not enabled
2020-04-28 20:55:15.672321 (MainThread): Parsing macros/core.sql
2020-04-28 20:55:15.677106 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 20:55:15.685510 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 20:55:15.687371 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 20:55:15.705919 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 20:55:15.740088 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 20:55:15.762003 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 20:55:15.764511 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 20:55:15.771766 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 20:55:15.785533 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 20:55:15.792797 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 20:55:15.800095 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 20:55:15.805244 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 20:55:15.806181 (MainThread): Parsing macros/etc/query.sql
2020-04-28 20:55:15.807571 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 20:55:15.809545 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 20:55:15.811990 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 20:55:15.822007 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 20:55:15.824056 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 20:55:15.825106 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 20:55:15.866280 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 20:55:15.867944 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 20:55:15.869188 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 20:55:15.870733 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 20:55:15.872954 (MainThread): Parsing macros/catalog.sql
2020-04-28 20:55:15.875240 (MainThread): Parsing macros/relations.sql
2020-04-28 20:55:15.876572 (MainThread): Parsing macros/adapters.sql
2020-04-28 20:55:15.894869 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 20:55:15.919034 (MainThread): Partial parsing not enabled
2020-04-28 20:55:15.946944 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:55:15.947061 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.962511 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 20:55:15.962598 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.966500 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 20:55:15.966584 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.970865 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 20:55:15.970953 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.974894 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 20:55:15.974982 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.980293 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 20:55:15.980399 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.985560 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 20:55:15.985653 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:16.130038 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 20:55:16.133363 (MainThread): 
2020-04-28 20:55:16.133757 (MainThread): Acquiring new postgres connection "master".
2020-04-28 20:55:16.133843 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:16.138768 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 20:55:16.138924 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 20:55:16.224565 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 20:55:16.224744 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 20:55:16.783610 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.56 seconds
2020-04-28 20:55:16.820701 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:55:16.820936 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 20:55:16.822562 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:55:16.822673 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 20:55:16.864723 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:55:16.865173 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:55:16.865349 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 20:55:16.978642 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 20:55:16.987788 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 20:55:17.079473 (MainThread): Using postgres connection "master".
2020-04-28 20:55:17.079624 (MainThread): On master: BEGIN
2020-04-28 20:55:17.414736 (MainThread): SQL status: BEGIN in 0.33 seconds
2020-04-28 20:55:17.415151 (MainThread): Using postgres connection "master".
2020-04-28 20:55:17.415419 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 20:55:17.543593 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 20:55:17.630501 (MainThread): On master: ROLLBACK
2020-04-28 20:55:17.667128 (MainThread): Using postgres connection "master".
2020-04-28 20:55:17.667529 (MainThread): On master: BEGIN
2020-04-28 20:55:17.741542 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 20:55:17.741991 (MainThread): On master: COMMIT
2020-04-28 20:55:17.742298 (MainThread): Using postgres connection "master".
2020-04-28 20:55:17.742465 (MainThread): On master: COMMIT
2020-04-28 20:55:17.778817 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 20:55:17.779701 (MainThread): 13:55:17 | Concurrency: 1 threads (target='dev')
2020-04-28 20:55:17.779944 (MainThread): 13:55:17 | 
2020-04-28 20:55:17.782070 (Thread-1): Began running node model.order_history.customers
2020-04-28 20:55:17.782335 (Thread-1): 13:55:17 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 20:55:17.782900 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:55:17.783042 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 20:55:17.783184 (Thread-1): Compiling model.order_history.customers
2020-04-28 20:55:17.802766 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 20:55:17.803295 (Thread-1): finished collecting timing info
2020-04-28 20:55:17.842551 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:55:17.842715 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 20:55:17.926970 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 20:55:17.930500 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:55:17.930681 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 20:55:17.972835 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 20:55:17.976709 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 20:55:17.978627 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:55:17.978986 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 20:55:18.020685 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:55:18.021129 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:55:18.021421 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,

        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        CASE WHEN yield_manager_partners.email is not null THEN 1 ELSE 0 END AS is_broker,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 20:55:18.073093 (Thread-1): Postgres error: relation "yield_manager_partners" does not exist

2020-04-28 20:55:18.073504 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 20:55:18.115656 (Thread-1): finished collecting timing info
2020-04-28 20:55:18.116691 (Thread-1): Database Error in model customers (models/customers.sql)
  relation "yield_manager_partners" does not exist
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "yield_manager_partners" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  relation "yield_manager_partners" does not exist
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 20:55:18.119731 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8bd1e3d3-af73-479a-b71a-5d75e9ea201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098c3590>]}
2020-04-28 20:55:18.120034 (Thread-1): 13:55:18 | 1 of 1 ERROR creating view model data_science.customers.............. [ERROR in 0.34s]
2020-04-28 20:55:18.120212 (Thread-1): Finished running node model.order_history.customers
2020-04-28 20:55:18.195817 (MainThread): Using postgres connection "master".
2020-04-28 20:55:18.196140 (MainThread): On master: BEGIN
2020-04-28 20:55:18.233372 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:55:18.233605 (MainThread): On master: COMMIT
2020-04-28 20:55:18.233738 (MainThread): Using postgres connection "master".
2020-04-28 20:55:18.233857 (MainThread): On master: COMMIT
2020-04-28 20:55:18.269525 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 20:55:18.270130 (MainThread): 13:55:18 | 
2020-04-28 20:55:18.270355 (MainThread): 13:55:18 | Finished running 1 view model in 2.14s.
2020-04-28 20:55:18.270555 (MainThread): Connection 'master' was left open.
2020-04-28 20:55:18.270709 (MainThread): On master: Close
2020-04-28 20:55:18.271081 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 20:55:18.271247 (MainThread): On model.order_history.customers: Close
2020-04-28 20:55:18.275879 (MainThread): 
2020-04-28 20:55:18.276046 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 20:55:18.276166 (MainThread): 
2020-04-28 20:55:18.276279 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 20:55:18.276381 (MainThread):   relation "yield_manager_partners" does not exist
2020-04-28 20:55:18.276477 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 20:55:18.276575 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2020-04-28 20:55:18.276747 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109602050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093a2110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098acad0>]}
2020-04-28 20:55:18.276933 (MainThread): Flushing usage events
2020-04-28 20:56:09.488253 (MainThread): Running with dbt=0.16.1
2020-04-28 20:56:09.561554 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 20:56:09.562404 (MainThread): Tracking: tracking
2020-04-28 20:56:09.569014 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a2da50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a36fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a36a50>]}
2020-04-28 20:56:09.596215 (MainThread): Partial parsing not enabled
2020-04-28 20:56:09.598110 (MainThread): Parsing macros/core.sql
2020-04-28 20:56:09.603232 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 20:56:09.611956 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 20:56:09.613816 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 20:56:09.633024 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 20:56:09.668764 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 20:56:09.691667 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 20:56:09.693673 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 20:56:09.700660 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 20:56:09.714136 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 20:56:09.721239 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 20:56:09.727872 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 20:56:09.733165 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 20:56:09.734182 (MainThread): Parsing macros/etc/query.sql
2020-04-28 20:56:09.735318 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 20:56:09.737084 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 20:56:09.739327 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 20:56:09.748879 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 20:56:09.751016 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 20:56:09.752165 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 20:56:09.795252 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 20:56:09.796475 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 20:56:09.797439 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 20:56:09.798578 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 20:56:09.800893 (MainThread): Parsing macros/catalog.sql
2020-04-28 20:56:09.803473 (MainThread): Parsing macros/relations.sql
2020-04-28 20:56:09.804888 (MainThread): Parsing macros/adapters.sql
2020-04-28 20:56:09.823590 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 20:56:09.841643 (MainThread): Partial parsing not enabled
2020-04-28 20:56:09.869755 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:56:09.869864 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.886959 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 20:56:09.887088 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.891649 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 20:56:09.891755 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.896629 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 20:56:09.896738 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.901472 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 20:56:09.901619 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.908161 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 20:56:09.908300 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.915663 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 20:56:09.915775 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.958557 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d5d5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e6f110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e6f890>]}
2020-04-28 20:56:09.958764 (MainThread): Flushing usage events
2020-04-28 20:56:10.266963 (MainThread): Connection 'model.order_history.order_flash' was properly closed.
2020-04-28 20:56:10.267194 (MainThread): Encountered an error:
2020-04-28 20:56:10.267398 (MainThread): Compilation Error in model customers (models/customers.sql)
  Model 'model.order_history.customers' depends on a node named 'stg_customer_broker' which was not found or is disabled
2020-04-28 20:56:10.269580 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 302, in process_manifest
    process_refs(manifest, project_name)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 553, in process_refs
    _process_refs_for_node(manifest, current_project, node)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 534, in _process_refs_for_node
    disabled=(isinstance(target_model, Disabled))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/utils.py", line 338, in invalid_ref_fail_unless_test
    target_model_package)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 488, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model customers (models/customers.sql)
  Model 'model.order_history.customers' depends on a node named 'stg_customer_broker' which was not found or is disabled

2020-04-28 20:56:20.018625 (MainThread): Running with dbt=0.16.1
2020-04-28 20:56:20.082011 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 20:56:20.082799 (MainThread): Tracking: tracking
2020-04-28 20:56:20.087726 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fdce10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d48690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d84810>]}
2020-04-28 20:56:20.105919 (MainThread): Partial parsing not enabled
2020-04-28 20:56:20.107752 (MainThread): Parsing macros/core.sql
2020-04-28 20:56:20.112305 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 20:56:20.120259 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 20:56:20.122004 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 20:56:20.139880 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 20:56:20.172677 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 20:56:20.193376 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 20:56:20.195295 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 20:56:20.201480 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 20:56:20.214178 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 20:56:20.220886 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 20:56:20.227044 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 20:56:20.231931 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 20:56:20.232871 (MainThread): Parsing macros/etc/query.sql
2020-04-28 20:56:20.233934 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 20:56:20.235583 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 20:56:20.237667 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 20:56:20.247472 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 20:56:20.249517 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 20:56:20.250590 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 20:56:20.300748 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 20:56:20.302038 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 20:56:20.303040 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 20:56:20.304229 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 20:56:20.306652 (MainThread): Parsing macros/catalog.sql
2020-04-28 20:56:20.309081 (MainThread): Parsing macros/relations.sql
2020-04-28 20:56:20.310492 (MainThread): Parsing macros/adapters.sql
2020-04-28 20:56:20.328185 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 20:56:20.346051 (MainThread): Partial parsing not enabled
2020-04-28 20:56:20.373349 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:56:20.373444 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.389357 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 20:56:20.389451 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.393394 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 20:56:20.393483 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.397794 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 20:56:20.397882 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.401777 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 20:56:20.401864 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.406309 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 20:56:20.406406 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.411330 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 20:56:20.411420 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.546536 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 20:56:20.550515 (MainThread): 
2020-04-28 20:56:20.550842 (MainThread): Acquiring new postgres connection "master".
2020-04-28 20:56:20.550930 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.555332 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 20:56:20.555515 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 20:56:20.648127 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 20:56:20.648258 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 20:56:21.067699 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.42 seconds
2020-04-28 20:56:21.099522 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:56:21.099659 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 20:56:21.101105 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:56:21.101208 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 20:56:21.140395 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:56:21.140819 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:56:21.141087 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 20:56:21.255303 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 20:56:21.263258 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 20:56:21.328758 (MainThread): Using postgres connection "master".
2020-04-28 20:56:21.328880 (MainThread): On master: BEGIN
2020-04-28 20:56:21.726160 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-28 20:56:21.726576 (MainThread): Using postgres connection "master".
2020-04-28 20:56:21.726845 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 20:56:21.858287 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 20:56:21.937615 (MainThread): On master: ROLLBACK
2020-04-28 20:56:21.976872 (MainThread): Using postgres connection "master".
2020-04-28 20:56:21.977037 (MainThread): On master: BEGIN
2020-04-28 20:56:22.054044 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 20:56:22.054494 (MainThread): On master: COMMIT
2020-04-28 20:56:22.054789 (MainThread): Using postgres connection "master".
2020-04-28 20:56:22.054970 (MainThread): On master: COMMIT
2020-04-28 20:56:22.092849 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 20:56:22.093727 (MainThread): 13:56:22 | Concurrency: 1 threads (target='dev')
2020-04-28 20:56:22.093973 (MainThread): 13:56:22 | 
2020-04-28 20:56:22.096238 (Thread-1): Began running node model.order_history.customers
2020-04-28 20:56:22.096495 (Thread-1): 13:56:22 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 20:56:22.097068 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:56:22.097193 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 20:56:22.097323 (Thread-1): Compiling model.order_history.customers
2020-04-28 20:56:22.116402 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 20:56:22.116959 (Thread-1): finished collecting timing info
2020-04-28 20:56:22.155322 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.155481 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 20:56:22.234245 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 20:56:22.238569 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.238725 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 20:56:22.277430 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 20:56:22.279954 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 20:56:22.280438 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.280561 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 20:56:22.318694 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:56:22.318873 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.318971 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,

        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 20:56:22.376459 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 20:56:22.382724 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.382888 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 20:56:22.423256 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 20:56:22.427543 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.427700 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 20:56:22.494469 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2020-04-28 20:56:22.496428 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 20:56:22.496624 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.496784 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 20:56:22.665981 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 20:56:22.667917 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.668042 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 20:56:22.842050 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 20:56:22.846271 (Thread-1): finished collecting timing info
2020-04-28 20:56:22.847123 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7656adda-eb75-4c0b-be1e-b8034901c4d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051d0090>]}
2020-04-28 20:56:22.847434 (Thread-1): 13:56:22 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 0.75s]
2020-04-28 20:56:22.847622 (Thread-1): Finished running node model.order_history.customers
2020-04-28 20:56:22.921722 (MainThread): Using postgres connection "master".
2020-04-28 20:56:22.921970 (MainThread): On master: BEGIN
2020-04-28 20:56:22.960899 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:56:22.961218 (MainThread): On master: COMMIT
2020-04-28 20:56:22.961400 (MainThread): Using postgres connection "master".
2020-04-28 20:56:22.961575 (MainThread): On master: COMMIT
2020-04-28 20:56:22.999429 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 20:56:23.000335 (MainThread): 13:56:23 | 
2020-04-28 20:56:23.000575 (MainThread): 13:56:23 | Finished running 1 view model in 2.45s.
2020-04-28 20:56:23.000778 (MainThread): Connection 'master' was left open.
2020-04-28 20:56:23.000933 (MainThread): On master: Close
2020-04-28 20:56:23.001316 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 20:56:23.001476 (MainThread): On model.order_history.customers: Close
2020-04-28 20:56:23.008068 (MainThread): 
2020-04-28 20:56:23.008308 (MainThread): Completed successfully
2020-04-28 20:56:23.008485 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 20:56:23.008729 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053add90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10558f2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105583750>]}
2020-04-28 20:56:23.008976 (MainThread): Flushing usage events
2020-04-28 21:55:27.126061 (MainThread): Running with dbt=0.16.1
2020-04-28 21:55:27.201708 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 21:55:27.202590 (MainThread): Tracking: tracking
2020-04-28 21:55:27.211663 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045941d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d7b1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104594a90>]}
2020-04-28 21:55:27.233693 (MainThread): Partial parsing not enabled
2020-04-28 21:55:27.237039 (MainThread): Parsing macros/core.sql
2020-04-28 21:55:27.241903 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 21:55:27.250046 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 21:55:27.251847 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 21:55:27.273505 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 21:55:27.309918 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 21:55:27.331362 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 21:55:27.333784 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 21:55:27.341004 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 21:55:27.354799 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 21:55:27.362694 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 21:55:27.370017 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 21:55:27.375115 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 21:55:27.376091 (MainThread): Parsing macros/etc/query.sql
2020-04-28 21:55:27.377515 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 21:55:27.379253 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 21:55:27.381384 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 21:55:27.390833 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 21:55:27.392886 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 21:55:27.393983 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 21:55:27.436070 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 21:55:27.437713 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 21:55:27.439102 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 21:55:27.440697 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 21:55:27.443326 (MainThread): Parsing macros/catalog.sql
2020-04-28 21:55:27.445827 (MainThread): Parsing macros/relations.sql
2020-04-28 21:55:27.447338 (MainThread): Parsing macros/adapters.sql
2020-04-28 21:55:27.464924 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 21:55:27.481969 (MainThread): Partial parsing not enabled
2020-04-28 21:55:27.508261 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 21:55:27.508360 (MainThread): Opening a new connection, currently in state init
2020-04-28 21:55:27.523801 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:27.523892 (MainThread): Opening a new connection, currently in state init
2020-04-28 21:55:27.527681 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:27.527766 (MainThread): Opening a new connection, currently in state init
2020-04-28 21:55:27.531880 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 21:55:27.531965 (MainThread): Opening a new connection, currently in state init
2020-04-28 21:55:27.535716 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 21:55:27.535802 (MainThread): Opening a new connection, currently in state init
2020-04-28 21:55:27.540014 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:27.540103 (MainThread): Opening a new connection, currently in state init
2020-04-28 21:55:27.544784 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 21:55:27.544870 (MainThread): Opening a new connection, currently in state init
2020-04-28 21:55:27.690984 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 21:55:27.694070 (MainThread): 
2020-04-28 21:55:27.694416 (MainThread): Acquiring new postgres connection "master".
2020-04-28 21:55:27.694509 (MainThread): Opening a new connection, currently in state init
2020-04-28 21:55:27.716043 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 21:55:27.716240 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 21:55:27.799052 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 21:55:27.799191 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 21:55:28.338020 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.54 seconds
2020-04-28 21:55:28.370510 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 21:55:28.370686 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 21:55:28.372279 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 21:55:28.372389 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 21:55:28.412805 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:28.413228 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 21:55:28.413497 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 21:55:28.526193 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 21:55:28.535252 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 21:55:28.610031 (MainThread): Using postgres connection "master".
2020-04-28 21:55:28.610242 (MainThread): On master: BEGIN
2020-04-28 21:55:28.997077 (MainThread): SQL status: BEGIN in 0.39 seconds
2020-04-28 21:55:28.997503 (MainThread): Using postgres connection "master".
2020-04-28 21:55:28.997769 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 21:55:29.131217 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 21:55:29.212071 (MainThread): On master: ROLLBACK
2020-04-28 21:55:29.251349 (MainThread): Using postgres connection "master".
2020-04-28 21:55:29.251785 (MainThread): On master: BEGIN
2020-04-28 21:55:29.381769 (MainThread): SQL status: BEGIN in 0.13 seconds
2020-04-28 21:55:29.382254 (MainThread): On master: COMMIT
2020-04-28 21:55:29.382554 (MainThread): Using postgres connection "master".
2020-04-28 21:55:29.382714 (MainThread): On master: COMMIT
2020-04-28 21:55:29.421820 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 21:55:29.422713 (MainThread): 14:55:29 | Concurrency: 1 threads (target='dev')
2020-04-28 21:55:29.422966 (MainThread): 14:55:29 | 
2020-04-28 21:55:29.425384 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 21:55:29.425853 (Thread-1): 14:55:29 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 21:55:29.426234 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:29.426375 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 21:55:29.426517 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 21:55:29.442966 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 21:55:29.443660 (Thread-1): finished collecting timing info
2020-04-28 21:55:29.483746 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:29.483912 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 21:55:29.565281 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 21:55:29.570296 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:29.570450 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 21:55:29.610067 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 21:55:29.611955 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 21:55:29.613180 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:29.613328 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 21:55:29.652285 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:29.652477 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:29.652584 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 21:55:29.708452 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 21:55:29.713732 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:29.713878 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 21:55:29.756993 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:29.761367 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:29.761524 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 21:55:29.802738 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:29.804641 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 21:55:29.804839 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:29.805000 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 21:55:30.002540 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 21:55:30.004632 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 21:55:30.004758 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 21:55:30.198817 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 21:55:30.203166 (Thread-1): finished collecting timing info
2020-04-28 21:55:30.204038 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '941f3098-13fd-4815-b5fc-04b096341fa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b76cd0>]}
2020-04-28 21:55:30.204352 (Thread-1): 14:55:30 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.78s]
2020-04-28 21:55:30.204536 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 21:55:30.204725 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 21:55:30.204903 (Thread-1): 14:55:30 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 21:55:30.205239 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.205370 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 21:55:30.205503 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 21:55:30.212209 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 21:55:30.212652 (Thread-1): finished collecting timing info
2020-04-28 21:55:30.220234 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.220365 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 21:55:30.398622 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 21:55:30.402779 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.402937 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 21:55:30.574186 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 21:55:30.577210 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 21:55:30.577821 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.577977 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 21:55:30.618452 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:30.618887 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.619168 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 21:55:30.674712 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 21:55:30.680891 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.681082 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 21:55:30.724795 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:30.729165 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.729321 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 21:55:30.777245 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 21:55:30.779244 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 21:55:30.779441 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.779603 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 21:55:30.950862 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 21:55:30.955556 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 21:55:30.955709 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 21:55:31.138506 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 21:55:31.142725 (Thread-1): finished collecting timing info
2020-04-28 21:55:31.143564 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '941f3098-13fd-4815-b5fc-04b096341fa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104955310>]}
2020-04-28 21:55:31.143867 (Thread-1): 14:55:31 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.94s]
2020-04-28 21:55:31.144047 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 21:55:31.144281 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 21:55:31.144624 (Thread-1): 14:55:31 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 21:55:31.145099 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 21:55:31.145291 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 21:55:31.145445 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 21:55:31.184203 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 21:55:31.186080 (Thread-1): finished collecting timing info
2020-04-28 21:55:31.194164 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 21:55:31.194325 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 21:55:31.755692 (Thread-1): SQL status: DROP VIEW in 0.56 seconds
2020-04-28 21:55:31.759782 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 21:55:31.759935 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 21:55:32.201301 (Thread-1): SQL status: DROP VIEW in 0.44 seconds
2020-04-28 21:55:32.204348 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 21:55:32.205037 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 21:55:32.205209 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 21:55:32.246394 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:32.246943 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 21:55:32.247126 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 21:55:32.306674 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 21:55:32.311638 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 21:55:32.311778 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 21:55:32.351552 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:32.354185 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 21:55:32.354298 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 21:55:32.397300 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:32.399444 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 21:55:32.399647 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 21:55:32.399810 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 21:55:32.571281 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 21:55:32.574644 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 21:55:32.574796 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 21:55:32.751734 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 21:55:32.756026 (Thread-1): finished collecting timing info
2020-04-28 21:55:32.756863 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '941f3098-13fd-4815-b5fc-04b096341fa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048598d0>]}
2020-04-28 21:55:32.757171 (Thread-1): 14:55:32 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.61s]
2020-04-28 21:55:32.757354 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 21:55:32.757589 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 21:55:32.758031 (Thread-1): 14:55:32 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 21:55:32.758662 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 21:55:32.758883 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 21:55:32.759029 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 21:55:32.765926 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 21:55:32.766361 (Thread-1): finished collecting timing info
2020-04-28 21:55:32.773540 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 21:55:32.773689 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 21:55:33.013694 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-28 21:55:33.016349 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 21:55:33.016468 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 21:55:33.469105 (Thread-1): SQL status: DROP VIEW in 0.45 seconds
2020-04-28 21:55:33.472168 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 21:55:33.472813 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 21:55:33.472970 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 21:55:33.515305 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:33.515596 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 21:55:33.515780 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 21:55:33.725221 (Thread-1): SQL status: CREATE VIEW in 0.21 seconds
2020-04-28 21:55:33.731335 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 21:55:33.731488 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 21:55:33.775896 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:33.779293 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 21:55:33.779458 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 21:55:33.820950 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:33.822889 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 21:55:33.823080 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 21:55:33.823234 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 21:55:33.997487 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 21:55:33.999571 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 21:55:33.999680 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 21:55:34.178453 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 21:55:34.182675 (Thread-1): finished collecting timing info
2020-04-28 21:55:34.183508 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '941f3098-13fd-4815-b5fc-04b096341fa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10473d190>]}
2020-04-28 21:55:34.183805 (Thread-1): 14:55:34 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 1.42s]
2020-04-28 21:55:34.183982 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 21:55:34.184216 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 21:55:34.184630 (Thread-1): 14:55:34 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 21:55:34.185174 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:34.185372 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 21:55:34.185502 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 21:55:34.193287 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 21:55:34.193727 (Thread-1): finished collecting timing info
2020-04-28 21:55:34.201327 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:34.201471 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 21:55:34.395603 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 21:55:34.400808 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:34.400955 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 21:55:34.572041 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 21:55:34.575107 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 21:55:34.576789 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:34.577020 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 21:55:34.616705 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:34.616899 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:34.617008 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-28 21:55:34.670193 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 21:55:34.674455 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:34.674606 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-28 21:55:34.718891 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:34.720825 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 21:55:34.721013 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:34.721168 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 21:55:34.894290 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 21:55:34.897693 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 21:55:34.897838 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 21:55:35.074959 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 21:55:35.078989 (Thread-1): finished collecting timing info
2020-04-28 21:55:35.079825 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '941f3098-13fd-4815-b5fc-04b096341fa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045c05d0>]}
2020-04-28 21:55:35.080124 (Thread-1): 14:55:35 | 5 of 7 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.89s]
2020-04-28 21:55:35.080302 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 21:55:35.080485 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 21:55:35.080667 (Thread-1): 14:55:35 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 21:55:35.081155 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 21:55:35.081327 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 21:55:35.081621 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 21:55:35.090455 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 21:55:35.091905 (Thread-1): finished collecting timing info
2020-04-28 21:55:35.099296 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 21:55:35.099536 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 21:55:35.320673 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 21:55:35.323243 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 21:55:35.323352 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 21:55:35.501944 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 21:55:35.504929 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 21:55:35.506613 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 21:55:35.506835 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 21:55:35.547218 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:35.547631 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 21:55:35.547898 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 21:55:35.606782 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 21:55:35.609951 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 21:55:35.610081 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 21:55:35.650612 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:35.651681 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 21:55:35.651791 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 21:55:35.651882 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 21:55:35.879364 (Thread-1): SQL status: COMMIT in 0.23 seconds
2020-04-28 21:55:35.882591 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 21:55:35.882752 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 21:55:36.062820 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 21:55:36.067034 (Thread-1): finished collecting timing info
2020-04-28 21:55:36.067864 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '941f3098-13fd-4815-b5fc-04b096341fa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048285d0>]}
2020-04-28 21:55:36.068162 (Thread-1): 14:55:36 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.99s]
2020-04-28 21:55:36.068337 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 21:55:36.068788 (Thread-1): Began running node model.order_history.customers
2020-04-28 21:55:36.069131 (Thread-1): 14:55:36 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 21:55:36.069537 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 21:55:36.069679 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 21:55:36.069785 (Thread-1): Compiling model.order_history.customers
2020-04-28 21:55:36.079784 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 21:55:36.080904 (Thread-1): finished collecting timing info
2020-04-28 21:55:36.088461 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 21:55:36.088690 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 21:55:36.260024 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 21:55:36.264127 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 21:55:36.264279 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 21:55:36.434260 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 21:55:36.437192 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 21:55:36.437778 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 21:55:36.437924 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 21:55:36.478434 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:36.478850 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 21:55:36.479107 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets
        
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 21:55:36.537232 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 21:55:36.541480 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 21:55:36.541631 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 21:55:36.585542 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 21:55:36.587466 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 21:55:36.587654 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 21:55:36.587809 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 21:55:36.759431 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 21:55:36.762812 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 21:55:36.762963 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 21:55:36.932118 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 21:55:36.936244 (Thread-1): finished collecting timing info
2020-04-28 21:55:36.937084 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '941f3098-13fd-4815-b5fc-04b096341fa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b3b650>]}
2020-04-28 21:55:36.937382 (Thread-1): 14:55:36 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 0.87s]
2020-04-28 21:55:36.937556 (Thread-1): Finished running node model.order_history.customers
2020-04-28 21:55:37.032997 (MainThread): Using postgres connection "master".
2020-04-28 21:55:37.033151 (MainThread): On master: BEGIN
2020-04-28 21:55:37.072940 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 21:55:37.073370 (MainThread): On master: COMMIT
2020-04-28 21:55:37.073631 (MainThread): Using postgres connection "master".
2020-04-28 21:55:37.073819 (MainThread): On master: COMMIT
2020-04-28 21:55:37.113082 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 21:55:37.113995 (MainThread): 14:55:37 | 
2020-04-28 21:55:37.114241 (MainThread): 14:55:37 | Finished running 7 view models in 9.42s.
2020-04-28 21:55:37.114432 (MainThread): Connection 'master' was left open.
2020-04-28 21:55:37.114582 (MainThread): On master: Close
2020-04-28 21:55:37.114963 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 21:55:37.115123 (MainThread): On model.order_history.customers: Close
2020-04-28 21:55:37.137213 (MainThread): 
2020-04-28 21:55:37.137421 (MainThread): Completed successfully
2020-04-28 21:55:37.137554 (MainThread): 
Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
2020-04-28 21:55:37.137755 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104975c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104644310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104998290>]}
2020-04-28 21:55:37.137956 (MainThread): Flushing usage events
2020-04-28 23:37:16.050863 (MainThread): Running with dbt=0.16.1
2020-04-28 23:37:16.163773 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 23:37:16.164956 (MainThread): Tracking: tracking
2020-04-28 23:37:16.173724 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112605e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112646a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112646890>]}
2020-04-28 23:37:16.196936 (MainThread): Partial parsing not enabled
2020-04-28 23:37:16.201357 (MainThread): Parsing macros/core.sql
2020-04-28 23:37:16.208251 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 23:37:16.217384 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 23:37:16.220278 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 23:37:16.239962 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 23:37:16.279711 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 23:37:16.302428 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 23:37:16.305163 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 23:37:16.313015 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 23:37:16.326876 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 23:37:16.334755 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 23:37:16.342486 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 23:37:16.348235 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 23:37:16.349816 (MainThread): Parsing macros/etc/query.sql
2020-04-28 23:37:16.351534 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 23:37:16.353938 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 23:37:16.356924 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 23:37:16.367045 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 23:37:16.369754 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 23:37:16.371965 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 23:37:16.415921 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 23:37:16.418119 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 23:37:16.419719 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 23:37:16.421587 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 23:37:16.424714 (MainThread): Parsing macros/catalog.sql
2020-04-28 23:37:16.427951 (MainThread): Parsing macros/relations.sql
2020-04-28 23:37:16.430151 (MainThread): Parsing macros/adapters.sql
2020-04-28 23:37:16.448687 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 23:37:16.467451 (MainThread): Partial parsing not enabled
2020-04-28 23:37:16.495420 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 23:37:16.495533 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:16.513560 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 23:37:16.513695 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:16.519463 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 23:37:16.519600 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:16.524827 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 23:37:16.525015 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:16.530940 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 23:37:16.531107 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:16.538331 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 23:37:16.538481 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:16.546009 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 23:37:16.546161 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:16.702211 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 23:37:16.705809 (MainThread): 
2020-04-28 23:37:16.706218 (MainThread): Acquiring new postgres connection "master".
2020-04-28 23:37:16.706306 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:16.711913 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 23:37:16.712072 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 23:37:16.797439 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 23:37:16.797575 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 23:37:17.351798 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.55 seconds
2020-04-28 23:37:17.391469 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 23:37:17.391684 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 23:37:17.393389 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 23:37:17.393509 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 23:37:17.431911 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 23:37:17.432353 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 23:37:17.432526 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 23:37:17.539786 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 23:37:17.549748 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 23:37:17.622311 (MainThread): Using postgres connection "master".
2020-04-28 23:37:17.622461 (MainThread): On master: BEGIN
2020-04-28 23:37:17.985042 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 23:37:17.985472 (MainThread): Using postgres connection "master".
2020-04-28 23:37:17.985736 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 23:37:18.122673 (MainThread): SQL status: SELECT in 0.14 seconds
2020-04-28 23:37:18.200645 (MainThread): On master: ROLLBACK
2020-04-28 23:37:18.240358 (MainThread): Using postgres connection "master".
2020-04-28 23:37:18.240526 (MainThread): On master: BEGIN
2020-04-28 23:37:18.320130 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 23:37:18.320644 (MainThread): On master: COMMIT
2020-04-28 23:37:18.320899 (MainThread): Using postgres connection "master".
2020-04-28 23:37:18.321056 (MainThread): On master: COMMIT
2020-04-28 23:37:18.360150 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 23:37:18.361063 (MainThread): 16:37:18 | Concurrency: 1 threads (target='dev')
2020-04-28 23:37:18.361310 (MainThread): 16:37:18 | 
2020-04-28 23:37:18.365162 (Thread-1): Began running node model.order_history.customers
2020-04-28 23:37:18.365391 (Thread-1): 16:37:18 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 23:37:18.365898 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 23:37:18.366019 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 23:37:18.366145 (Thread-1): Compiling model.order_history.customers
2020-04-28 23:37:18.385341 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 23:37:18.386462 (Thread-1): finished collecting timing info
2020-04-28 23:37:18.425723 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:18.425884 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 23:37:18.503358 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 23:37:18.507729 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:18.507883 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 23:37:18.546737 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 23:37:18.549283 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 23:37:18.549790 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:18.549919 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 23:37:18.587756 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 23:37:18.587942 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:18.588042 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 23:37:18.632999 (Thread-1): Postgres error: syntax error at or near "."
LINE 41:         customer_orders.count_transfers
                                ^

2020-04-28 23:37:18.633444 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 23:37:18.671902 (Thread-1): finished collecting timing info
2020-04-28 23:37:18.672964 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "."
  LINE 41:         customer_orders.count_transfers
                                  ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "."
LINE 41:         customer_orders.count_transfers
                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "."
  LINE 41:         customer_orders.count_transfers
                                  ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 23:37:18.696825 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c61d7eea-b449-46d3-9f2d-2463457991f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3b4d0>]}
2020-04-28 23:37:18.697119 (Thread-1): 16:37:18 | 1 of 1 ERROR creating view model data_science.customers.............. [ERROR in 0.33s]
2020-04-28 23:37:18.697291 (Thread-1): Finished running node model.order_history.customers
2020-04-28 23:37:18.773709 (MainThread): Using postgres connection "master".
2020-04-28 23:37:18.774053 (MainThread): On master: BEGIN
2020-04-28 23:37:18.813902 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 23:37:18.814418 (MainThread): On master: COMMIT
2020-04-28 23:37:18.814630 (MainThread): Using postgres connection "master".
2020-04-28 23:37:18.814786 (MainThread): On master: COMMIT
2020-04-28 23:37:18.853886 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 23:37:18.854595 (MainThread): 16:37:18 | 
2020-04-28 23:37:18.854862 (MainThread): 16:37:18 | Finished running 1 view model in 2.15s.
2020-04-28 23:37:18.855129 (MainThread): Connection 'master' was left open.
2020-04-28 23:37:18.855430 (MainThread): On master: Close
2020-04-28 23:37:18.855826 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 23:37:18.855954 (MainThread): On model.order_history.customers: Close
2020-04-28 23:37:18.859861 (MainThread): 
2020-04-28 23:37:18.860076 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 23:37:18.860228 (MainThread): 
2020-04-28 23:37:18.860367 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 23:37:18.860495 (MainThread):   syntax error at or near "."
2020-04-28 23:37:18.860611 (MainThread):   LINE 41:         customer_orders.count_transfers
2020-04-28 23:37:18.860725 (MainThread):                                   ^
2020-04-28 23:37:18.860837 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 23:37:18.860955 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2020-04-28 23:37:18.861207 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e57190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e57890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e575d0>]}
2020-04-28 23:37:18.861458 (MainThread): Flushing usage events
2020-04-28 23:37:45.240833 (MainThread): Running with dbt=0.16.1
2020-04-28 23:37:45.310966 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 23:37:45.311705 (MainThread): Tracking: tracking
2020-04-28 23:37:45.316579 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a514e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4fb9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a511550>]}
2020-04-28 23:37:45.343630 (MainThread): Partial parsing not enabled
2020-04-28 23:37:45.345521 (MainThread): Parsing macros/core.sql
2020-04-28 23:37:45.350217 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 23:37:45.358397 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 23:37:45.360211 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 23:37:45.378246 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 23:37:45.412132 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 23:37:45.433849 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 23:37:45.435808 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 23:37:45.442251 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 23:37:45.455348 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 23:37:45.462320 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 23:37:45.468720 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 23:37:45.474703 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 23:37:45.475726 (MainThread): Parsing macros/etc/query.sql
2020-04-28 23:37:45.476831 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 23:37:45.478663 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 23:37:45.480805 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 23:37:45.490194 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 23:37:45.492229 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 23:37:45.493371 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 23:37:45.537070 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 23:37:45.538299 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 23:37:45.539263 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 23:37:45.540409 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 23:37:45.543032 (MainThread): Parsing macros/catalog.sql
2020-04-28 23:37:45.545459 (MainThread): Parsing macros/relations.sql
2020-04-28 23:37:45.546857 (MainThread): Parsing macros/adapters.sql
2020-04-28 23:37:45.564738 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 23:37:45.586203 (MainThread): Partial parsing not enabled
2020-04-28 23:37:45.618697 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 23:37:45.618853 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:45.636954 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 23:37:45.637079 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:45.641898 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 23:37:45.642015 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:45.646864 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 23:37:45.646976 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:45.652172 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 23:37:45.652306 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:45.658184 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 23:37:45.658332 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:45.667823 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 23:37:45.668017 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:45.816377 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 23:37:45.819035 (MainThread): 
2020-04-28 23:37:45.819312 (MainThread): Acquiring new postgres connection "master".
2020-04-28 23:37:45.819400 (MainThread): Opening a new connection, currently in state init
2020-04-28 23:37:45.823753 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 23:37:45.823858 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 23:37:45.910563 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 23:37:45.910711 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 23:37:46.334637 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.42 seconds
2020-04-28 23:37:46.372210 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 23:37:46.372410 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 23:37:46.374052 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 23:37:46.374166 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 23:37:46.412473 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 23:37:46.412899 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 23:37:46.413166 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 23:37:46.503018 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-28 23:37:46.512225 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 23:37:46.584434 (MainThread): Using postgres connection "master".
2020-04-28 23:37:46.584598 (MainThread): On master: BEGIN
2020-04-28 23:37:46.930456 (MainThread): SQL status: BEGIN in 0.35 seconds
2020-04-28 23:37:46.930879 (MainThread): Using postgres connection "master".
2020-04-28 23:37:46.931154 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 23:37:47.058971 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 23:37:47.129647 (MainThread): On master: ROLLBACK
2020-04-28 23:37:47.167845 (MainThread): Using postgres connection "master".
2020-04-28 23:37:47.168252 (MainThread): On master: BEGIN
2020-04-28 23:37:47.243975 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 23:37:47.244450 (MainThread): On master: COMMIT
2020-04-28 23:37:47.244731 (MainThread): Using postgres connection "master".
2020-04-28 23:37:47.244936 (MainThread): On master: COMMIT
2020-04-28 23:37:47.282726 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 23:37:47.283609 (MainThread): 16:37:47 | Concurrency: 1 threads (target='dev')
2020-04-28 23:37:47.283850 (MainThread): 16:37:47 | 
2020-04-28 23:37:47.285944 (Thread-1): Began running node model.order_history.customers
2020-04-28 23:37:47.286179 (Thread-1): 16:37:47 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 23:37:47.286718 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 23:37:47.286843 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 23:37:47.286973 (Thread-1): Compiling model.order_history.customers
2020-04-28 23:37:47.306419 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 23:37:47.307024 (Thread-1): finished collecting timing info
2020-04-28 23:37:47.347793 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:47.347955 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 23:37:47.425880 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 23:37:47.430175 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:47.430325 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 23:37:47.469028 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 23:37:47.472865 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 23:37:47.473445 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:47.473603 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 23:37:47.511553 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 23:37:47.511979 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:47.512268 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 23:37:47.566150 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 23:37:47.572470 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:47.572632 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 23:37:47.612263 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 23:37:47.615330 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:47.615477 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 23:37:47.655438 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 23:37:47.656944 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 23:37:47.657101 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:47.657230 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 23:37:47.834176 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 23:37:47.837232 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 23:37:47.837405 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 23:37:48.029436 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 23:37:48.032536 (Thread-1): finished collecting timing info
2020-04-28 23:37:48.033357 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5da6ef19-c1fd-4e3c-9995-7c894ad3b90e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9fe6d0>]}
2020-04-28 23:37:48.033642 (Thread-1): 16:37:48 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 0.75s]
2020-04-28 23:37:48.033810 (Thread-1): Finished running node model.order_history.customers
2020-04-28 23:37:48.097247 (MainThread): Using postgres connection "master".
2020-04-28 23:37:48.097446 (MainThread): On master: BEGIN
2020-04-28 23:37:48.136221 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 23:37:48.136542 (MainThread): On master: COMMIT
2020-04-28 23:37:48.136721 (MainThread): Using postgres connection "master".
2020-04-28 23:37:48.136883 (MainThread): On master: COMMIT
2020-04-28 23:37:48.174713 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 23:37:48.175633 (MainThread): 16:37:48 | 
2020-04-28 23:37:48.175876 (MainThread): 16:37:48 | Finished running 1 view model in 2.36s.
2020-04-28 23:37:48.176080 (MainThread): Connection 'master' was left open.
2020-04-28 23:37:48.176238 (MainThread): On master: Close
2020-04-28 23:37:48.176624 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 23:37:48.176788 (MainThread): On model.order_history.customers: Close
2020-04-28 23:37:48.181671 (MainThread): 
2020-04-28 23:37:48.181862 (MainThread): Completed successfully
2020-04-28 23:37:48.182011 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 23:37:48.182221 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad28310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a92b310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a940a10>]}
2020-04-28 23:37:48.182434 (MainThread): Flushing usage events
2020-04-29 00:52:09.073365 (MainThread): Running with dbt=0.16.1
2020-04-29 00:52:09.174036 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 00:52:09.175556 (MainThread): Tracking: tracking
2020-04-29 00:52:09.184651 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a41650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107cb450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a1ea90>]}
2020-04-29 00:52:09.208057 (MainThread): Partial parsing not enabled
2020-04-29 00:52:09.212394 (MainThread): Parsing macros/core.sql
2020-04-29 00:52:09.219027 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 00:52:09.229308 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 00:52:09.231872 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 00:52:09.251871 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 00:52:09.288003 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 00:52:09.311585 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 00:52:09.314269 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 00:52:09.321736 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 00:52:09.336317 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 00:52:09.344024 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 00:52:09.351500 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 00:52:09.356895 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 00:52:09.358527 (MainThread): Parsing macros/etc/query.sql
2020-04-29 00:52:09.360244 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 00:52:09.362140 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 00:52:09.364768 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 00:52:09.375953 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 00:52:09.378449 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 00:52:09.379887 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 00:52:09.429939 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 00:52:09.431868 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 00:52:09.433300 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 00:52:09.434899 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 00:52:09.437422 (MainThread): Parsing macros/catalog.sql
2020-04-29 00:52:09.440181 (MainThread): Parsing macros/relations.sql
2020-04-29 00:52:09.441803 (MainThread): Parsing macros/adapters.sql
2020-04-29 00:52:09.461321 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 00:52:09.485037 (MainThread): Partial parsing not enabled
2020-04-29 00:52:09.516195 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 00:52:09.516341 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.534472 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:09.534614 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.539871 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:09.540012 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.546111 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 00:52:09.546264 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.551402 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 00:52:09.551549 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.556986 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:09.557100 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.563177 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 00:52:09.563290 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.570938 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 00:52:09.571052 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.725283 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 00:52:09.728727 (MainThread): 
2020-04-29 00:52:09.729062 (MainThread): Acquiring new postgres connection "master".
2020-04-29 00:52:09.729150 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:52:09.760578 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 00:52:09.760755 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 00:52:09.848116 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 00:52:09.848250 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 00:52:10.411238 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.56 seconds
2020-04-29 00:52:10.449307 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:52:10.449440 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 00:52:10.451098 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:52:10.451193 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 00:52:10.491499 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:10.491910 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:52:10.492167 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 00:52:10.584802 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-29 00:52:10.592912 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 00:52:10.672155 (MainThread): Using postgres connection "master".
2020-04-29 00:52:10.672308 (MainThread): On master: BEGIN
2020-04-29 00:52:11.070556 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-29 00:52:11.070778 (MainThread): Using postgres connection "master".
2020-04-29 00:52:11.070907 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 00:52:11.200889 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-29 00:52:11.280891 (MainThread): On master: ROLLBACK
2020-04-29 00:52:11.319213 (MainThread): Using postgres connection "master".
2020-04-29 00:52:11.319609 (MainThread): On master: BEGIN
2020-04-29 00:52:11.396110 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 00:52:11.396555 (MainThread): On master: COMMIT
2020-04-29 00:52:11.396824 (MainThread): Using postgres connection "master".
2020-04-29 00:52:11.397030 (MainThread): On master: COMMIT
2020-04-29 00:52:11.434631 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 00:52:11.435070 (MainThread): 17:52:11 | Concurrency: 1 threads (target='dev')
2020-04-29 00:52:11.435239 (MainThread): 17:52:11 | 
2020-04-29 00:52:11.438423 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 00:52:11.438775 (Thread-1): 17:52:11 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 00:52:11.439301 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:11.439499 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 00:52:11.439694 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 00:52:11.462781 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 00:52:11.463567 (Thread-1): finished collecting timing info
2020-04-29 00:52:11.501786 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:11.501949 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 00:52:11.582491 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 00:52:11.586823 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:11.586975 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 00:52:11.627644 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 00:52:11.630747 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 00:52:11.631398 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:11.631545 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 00:52:11.671564 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:11.671887 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:11.672050 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 00:52:11.732653 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:52:11.738646 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:11.738814 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 00:52:11.784033 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 00:52:11.787153 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:11.787340 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 00:52:11.827715 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:52:11.828944 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 00:52:11.829100 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:11.829223 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 00:52:12.002149 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:52:12.005700 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:52:12.005863 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 00:52:12.244235 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-29 00:52:12.248657 (Thread-1): finished collecting timing info
2020-04-29 00:52:12.249513 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bb1c214-a9a2-48c2-8a18-3dcb0a2af4c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110cbdb50>]}
2020-04-29 00:52:12.249821 (Thread-1): 17:52:12 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.81s]
2020-04-29 00:52:12.250001 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 00:52:12.250195 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 00:52:12.250375 (Thread-1): 17:52:12 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 00:52:12.250828 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 00:52:12.250985 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 00:52:12.251104 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 00:52:12.257562 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 00:52:12.257983 (Thread-1): finished collecting timing info
2020-04-29 00:52:12.265851 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:52:12.265985 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 00:52:12.448083 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:52:12.450819 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:52:12.450950 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 00:52:12.627429 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:52:12.631640 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 00:52:12.632814 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:52:12.632979 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 00:52:12.674397 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:12.674811 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:52:12.675067 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 00:52:12.738962 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:52:12.774044 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:52:12.774239 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 00:52:12.815461 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:52:12.819884 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:52:12.820059 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 00:52:12.865539 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 00:52:12.867505 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 00:52:12.867699 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:52:12.867852 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 00:52:13.074375 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-29 00:52:13.077774 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:52:13.077922 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 00:52:13.256647 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:52:13.260690 (Thread-1): finished collecting timing info
2020-04-29 00:52:13.261529 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bb1c214-a9a2-48c2-8a18-3dcb0a2af4c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111053a50>]}
2020-04-29 00:52:13.261829 (Thread-1): 17:52:13 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.01s]
2020-04-29 00:52:13.262002 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 00:52:13.262182 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 00:52:13.262362 (Thread-1): 17:52:13 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 00:52:13.263218 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:13.263390 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 00:52:13.263514 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 00:52:13.269634 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 00:52:13.270089 (Thread-1): finished collecting timing info
2020-04-29 00:52:13.277514 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:13.277648 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 00:52:13.446510 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:52:13.450564 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:13.450723 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 00:52:13.623246 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:52:13.626268 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 00:52:13.626918 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:13.627066 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 00:52:13.667497 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:13.667911 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:13.668172 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 00:52:13.726841 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:52:13.733033 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:13.733185 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 00:52:13.783475 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 00:52:13.788929 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:13.789082 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 00:52:13.832737 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:52:13.834013 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 00:52:13.834143 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:13.834246 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 00:52:14.026843 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-29 00:52:14.030265 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:52:14.030415 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 00:52:14.210970 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:52:14.213567 (Thread-1): finished collecting timing info
2020-04-29 00:52:14.214203 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bb1c214-a9a2-48c2-8a18-3dcb0a2af4c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ceae50>]}
2020-04-29 00:52:14.214473 (Thread-1): 17:52:14 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.95s]
2020-04-29 00:52:14.214623 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 00:52:14.214772 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 00:52:14.215019 (Thread-1): 17:52:14 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 00:52:14.215434 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 00:52:14.215538 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 00:52:14.215634 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 00:52:14.221042 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 00:52:14.221430 (Thread-1): finished collecting timing info
2020-04-29 00:52:14.227951 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:52:14.228064 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 00:52:14.407438 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:52:14.410270 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:52:14.410392 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 00:52:14.608811 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 00:52:14.611900 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 00:52:14.612545 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:52:14.612698 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 00:52:14.653246 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:14.653595 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:52:14.653759 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime

FROM
    ticketing.events
    INNER JOIN ticketing.zones USING (zone_unique_id)
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND lower(zone_type_description)  in ('admissions', 'premium seating')
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 00:52:14.695659 (Thread-1): Postgres error: column "zone_unique_id" specified in USING clause does not exist in left table

2020-04-29 00:52:14.696083 (Thread-1): On model.order_history.stg_events: ROLLBACK
2020-04-29 00:52:14.736633 (Thread-1): finished collecting timing info
2020-04-29 00:52:14.737675 (Thread-1): Database Error in model stg_events (models/staging/stg_events.sql)
  column "zone_unique_id" specified in USING clause does not exist in left table
  compiled SQL at target/run/order_history/staging/stg_events.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column "zone_unique_id" specified in USING clause does not exist in left table


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model stg_events (models/staging/stg_events.sql)
  column "zone_unique_id" specified in USING clause does not exist in left table
  compiled SQL at target/run/order_history/staging/stg_events.sql
2020-04-29 00:52:14.751391 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bb1c214-a9a2-48c2-8a18-3dcb0a2af4c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ceae50>]}
2020-04-29 00:52:14.751734 (Thread-1): 17:52:14 | 4 of 8 ERROR creating view model data_science.stg_events............. [ERROR in 0.54s]
2020-04-29 00:52:14.751924 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 00:52:14.752112 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 00:52:14.752296 (Thread-1): 17:52:14 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 00:52:14.752906 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 00:52:14.753053 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 00:52:14.753185 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 00:52:14.762586 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 00:52:14.764271 (Thread-1): finished collecting timing info
2020-04-29 00:52:14.772021 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:52:14.772159 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 00:52:14.853372 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 00:52:14.857878 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:52:14.858018 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 00:52:14.899004 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 00:52:14.900744 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 00:52:14.901185 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:52:14.901291 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 00:52:14.941299 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:14.941719 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:52:14.941981 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 00:52:15.016131 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-29 00:52:15.020448 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:52:15.020604 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 00:52:15.062092 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:52:15.064029 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 00:52:15.064225 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:52:15.064386 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 00:52:15.248992 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 00:52:15.250972 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:52:15.251091 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 00:52:15.468085 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 00:52:15.472361 (Thread-1): finished collecting timing info
2020-04-29 00:52:15.473213 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bb1c214-a9a2-48c2-8a18-3dcb0a2af4c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110df55d0>]}
2020-04-29 00:52:15.473513 (Thread-1): 17:52:15 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 0.72s]
2020-04-29 00:52:15.473688 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 00:52:15.473914 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 00:52:15.474143 (Thread-1): 17:52:15 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 00:52:15.474912 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:15.475047 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 00:52:15.475162 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 00:52:15.482881 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 00:52:15.483351 (Thread-1): finished collecting timing info
2020-04-29 00:52:15.490824 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:15.490949 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 00:52:15.697134 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 00:52:15.701128 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:15.701282 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 00:52:15.876312 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:52:15.879258 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 00:52:15.879850 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:15.879999 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 00:52:15.920121 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:15.920394 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:15.920591 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 00:52:15.977987 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:52:15.981873 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:15.982027 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 00:52:16.022729 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:52:16.024458 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 00:52:16.024655 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:16.024809 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 00:52:16.196908 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:52:16.200314 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:52:16.200462 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 00:52:16.372162 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:52:16.376616 (Thread-1): finished collecting timing info
2020-04-29 00:52:16.377464 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bb1c214-a9a2-48c2-8a18-3dcb0a2af4c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ddff90>]}
2020-04-29 00:52:16.377766 (Thread-1): 17:52:16 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.90s]
2020-04-29 00:52:16.377939 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 00:52:16.378119 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 00:52:16.378295 (Thread-1): 17:52:16 | 7 of 8 SKIP relation data_science.order_flash_event.................. [SKIP]
2020-04-29 00:52:16.378670 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 00:52:16.378869 (Thread-1): Began running node model.order_history.customers
2020-04-29 00:52:16.379052 (Thread-1): 17:52:16 | 8 of 8 START view model data_science.customers....................... [RUN]
2020-04-29 00:52:16.379429 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 00:52:16.379542 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 00:52:16.379651 (Thread-1): Compiling model.order_history.customers
2020-04-29 00:52:16.389821 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 00:52:16.390247 (Thread-1): finished collecting timing info
2020-04-29 00:52:16.397867 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:52:16.398026 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 00:52:16.589961 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:52:16.592782 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:52:16.592909 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 00:52:16.770825 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:52:16.773212 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 00:52:16.773762 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:52:16.773911 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 00:52:16.814135 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:16.814535 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:52:16.814785 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 00:52:16.869535 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:52:16.873639 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:52:16.873799 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 00:52:16.915107 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:52:16.916295 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 00:52:16.916428 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:52:16.916529 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 00:52:17.087979 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:52:17.091377 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:52:17.091527 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 00:52:17.263476 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:52:17.266777 (Thread-1): finished collecting timing info
2020-04-29 00:52:17.267799 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bb1c214-a9a2-48c2-8a18-3dcb0a2af4c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d45910>]}
2020-04-29 00:52:17.268126 (Thread-1): 17:52:17 | 8 of 8 OK created view model data_science.customers.................. [CREATE VIEW in 0.89s]
2020-04-29 00:52:17.268340 (Thread-1): Finished running node model.order_history.customers
2020-04-29 00:52:17.286281 (MainThread): Using postgres connection "master".
2020-04-29 00:52:17.286453 (MainThread): On master: BEGIN
2020-04-29 00:52:17.325239 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:52:17.325669 (MainThread): On master: COMMIT
2020-04-29 00:52:17.325934 (MainThread): Using postgres connection "master".
2020-04-29 00:52:17.326127 (MainThread): On master: COMMIT
2020-04-29 00:52:17.364438 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 00:52:17.365373 (MainThread): 17:52:17 | 
2020-04-29 00:52:17.365610 (MainThread): 17:52:17 | Finished running 8 view models in 7.64s.
2020-04-29 00:52:17.365802 (MainThread): Connection 'master' was left open.
2020-04-29 00:52:17.365954 (MainThread): On master: Close
2020-04-29 00:52:17.366360 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 00:52:17.366521 (MainThread): On model.order_history.customers: Close
2020-04-29 00:52:17.391241 (MainThread): 
2020-04-29 00:52:17.391450 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 00:52:17.391592 (MainThread): 
2020-04-29 00:52:17.391721 (MainThread): Database Error in model stg_events (models/staging/stg_events.sql)
2020-04-29 00:52:17.391838 (MainThread):   column "zone_unique_id" specified in USING clause does not exist in left table
2020-04-29 00:52:17.391946 (MainThread):   compiled SQL at target/run/order_history/staging/stg_events.sql
2020-04-29 00:52:17.392066 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 00:52:17.392254 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111036350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111028e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110c27990>]}
2020-04-29 00:52:17.392461 (MainThread): Flushing usage events
2020-04-29 00:54:57.252894 (MainThread): Running with dbt=0.16.1
2020-04-29 00:54:57.318200 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 00:54:57.319224 (MainThread): Tracking: tracking
2020-04-29 00:54:57.324899 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afb9e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afb9d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afc5710>]}
2020-04-29 00:54:57.352820 (MainThread): Partial parsing not enabled
2020-04-29 00:54:57.354734 (MainThread): Parsing macros/core.sql
2020-04-29 00:54:57.359491 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 00:54:57.367791 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 00:54:57.369590 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 00:54:57.387709 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 00:54:57.421519 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 00:54:57.443549 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 00:54:57.445621 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 00:54:57.452187 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 00:54:57.465165 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 00:54:57.472266 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 00:54:57.478842 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 00:54:57.483993 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 00:54:57.484992 (MainThread): Parsing macros/etc/query.sql
2020-04-29 00:54:57.486628 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 00:54:57.488628 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 00:54:57.491087 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 00:54:57.501604 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 00:54:57.503742 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 00:54:57.504879 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 00:54:57.548930 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 00:54:57.550165 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 00:54:57.551137 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 00:54:57.552278 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 00:54:57.554566 (MainThread): Parsing macros/catalog.sql
2020-04-29 00:54:57.556966 (MainThread): Parsing macros/relations.sql
2020-04-29 00:54:57.558473 (MainThread): Parsing macros/adapters.sql
2020-04-29 00:54:57.575654 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 00:54:57.593506 (MainThread): Partial parsing not enabled
2020-04-29 00:54:57.621145 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 00:54:57.621253 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.637251 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 00:54:57.637349 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.641494 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:57.641597 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.646241 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 00:54:57.646345 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.650342 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 00:54:57.650433 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.655031 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 00:54:57.655131 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.660425 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 00:54:57.660648 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.667268 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 00:54:57.667420 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.814734 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 00:54:57.818736 (MainThread): 
2020-04-29 00:54:57.819107 (MainThread): Acquiring new postgres connection "master".
2020-04-29 00:54:57.819192 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:54:57.843619 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 00:54:57.843760 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 00:54:57.928875 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 00:54:57.929011 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 00:54:58.462435 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-29 00:54:58.502166 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:54:58.502388 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 00:54:58.504537 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:54:58.504660 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 00:54:58.545632 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:54:58.546060 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:54:58.546317 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 00:54:58.639863 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-29 00:54:58.649049 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 00:54:58.728649 (MainThread): Using postgres connection "master".
2020-04-29 00:54:58.728802 (MainThread): On master: BEGIN
2020-04-29 00:54:59.109321 (MainThread): SQL status: BEGIN in 0.38 seconds
2020-04-29 00:54:59.109625 (MainThread): Using postgres connection "master".
2020-04-29 00:54:59.109813 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 00:54:59.242089 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-29 00:54:59.323647 (MainThread): On master: ROLLBACK
2020-04-29 00:54:59.391483 (MainThread): Using postgres connection "master".
2020-04-29 00:54:59.391692 (MainThread): On master: BEGIN
2020-04-29 00:54:59.471464 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 00:54:59.471913 (MainThread): On master: COMMIT
2020-04-29 00:54:59.472244 (MainThread): Using postgres connection "master".
2020-04-29 00:54:59.472412 (MainThread): On master: COMMIT
2020-04-29 00:54:59.511414 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 00:54:59.512304 (MainThread): 17:54:59 | Concurrency: 1 threads (target='dev')
2020-04-29 00:54:59.512561 (MainThread): 17:54:59 | 
2020-04-29 00:54:59.515118 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 00:54:59.515382 (Thread-1): 17:54:59 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 00:54:59.515769 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:59.515909 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 00:54:59.516056 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 00:54:59.533158 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 00:54:59.533655 (Thread-1): finished collecting timing info
2020-04-29 00:54:59.574279 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:59.574444 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 00:54:59.655019 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 00:54:59.659389 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:59.659536 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 00:54:59.699305 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 00:54:59.701640 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 00:54:59.702224 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:59.702359 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 00:54:59.741653 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:54:59.741902 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:59.742051 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 00:54:59.798476 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:54:59.804797 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:59.804967 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 00:54:59.848835 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:54:59.852945 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:59.853110 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 00:54:59.893776 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:54:59.895662 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 00:54:59.895880 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:54:59.896056 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 00:55:00.071628 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 00:55:00.074589 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:00.074768 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 00:55:00.261649 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:55:00.265974 (Thread-1): finished collecting timing info
2020-04-29 00:55:00.266826 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72d28b48-ece0-4425-9d55-d5dfde2be1bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b02c210>]}
2020-04-29 00:55:00.267139 (Thread-1): 17:55:00 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.75s]
2020-04-29 00:55:00.267332 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 00:55:00.267519 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 00:55:00.267705 (Thread-1): 17:55:00 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 00:55:00.268053 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 00:55:00.268187 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 00:55:00.268320 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 00:55:00.274540 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 00:55:00.275065 (Thread-1): finished collecting timing info
2020-04-29 00:55:00.282806 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:00.282939 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 00:55:00.453808 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:00.456514 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:00.456657 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 00:55:00.627476 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:00.631896 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 00:55:00.632486 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:00.632647 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 00:55:00.673016 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:00.673434 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:00.673699 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zones.zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 00:55:00.736673 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:55:00.772930 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:00.773100 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 00:55:00.816515 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:00.820846 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:00.820995 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 00:55:00.861699 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:00.863641 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 00:55:00.863842 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:00.863997 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 00:55:01.052775 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-29 00:55:01.055352 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:01.055510 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 00:55:01.602169 (Thread-1): SQL status: DROP VIEW in 0.55 seconds
2020-04-29 00:55:01.606521 (Thread-1): finished collecting timing info
2020-04-29 00:55:01.607340 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72d28b48-ece0-4425-9d55-d5dfde2be1bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b5d5d10>]}
2020-04-29 00:55:01.607633 (Thread-1): 17:55:01 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.34s]
2020-04-29 00:55:01.607809 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 00:55:01.608047 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 00:55:01.608545 (Thread-1): 17:55:01 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 00:55:01.609142 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:01.609282 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 00:55:01.609407 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 00:55:01.615648 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 00:55:01.616096 (Thread-1): finished collecting timing info
2020-04-29 00:55:01.623705 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:01.623833 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 00:55:01.855758 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-29 00:55:01.859855 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:01.860008 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 00:55:02.041165 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:02.044190 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 00:55:02.044867 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:02.045016 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 00:55:02.086480 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:02.086895 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:02.087147 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 00:55:02.161213 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-29 00:55:02.165791 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:02.165941 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 00:55:02.210046 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:02.215831 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:02.215994 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 00:55:02.257371 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:02.259349 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 00:55:02.259552 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:02.259712 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 00:55:02.467845 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-29 00:55:02.470043 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:02.470176 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 00:55:02.662764 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:55:02.667015 (Thread-1): finished collecting timing info
2020-04-29 00:55:02.667859 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72d28b48-ece0-4425-9d55-d5dfde2be1bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b1c6a10>]}
2020-04-29 00:55:02.668166 (Thread-1): 17:55:02 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.06s]
2020-04-29 00:55:02.668352 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 00:55:02.668586 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 00:55:02.669028 (Thread-1): 17:55:02 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 00:55:02.669773 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 00:55:02.669939 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 00:55:02.670084 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 00:55:02.676517 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 00:55:02.677000 (Thread-1): finished collecting timing info
2020-04-29 00:55:02.684418 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:02.684548 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 00:55:02.867355 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:02.871411 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:02.871567 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 00:55:03.047178 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:03.050294 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 00:55:03.050932 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:03.051084 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 00:55:03.091622 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:03.092053 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:03.092324 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime

FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 00:55:03.150394 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:55:03.155083 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:03.155240 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 00:55:03.200701 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 00:55:03.203352 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:03.203485 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 00:55:03.249046 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 00:55:03.250988 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 00:55:03.251184 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:03.251344 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 00:55:03.459204 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-29 00:55:03.462657 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:03.462810 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 00:55:03.670226 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 00:55:03.675918 (Thread-1): finished collecting timing info
2020-04-29 00:55:03.676751 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72d28b48-ece0-4425-9d55-d5dfde2be1bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2d3a50>]}
2020-04-29 00:55:03.677071 (Thread-1): 17:55:03 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 1.01s]
2020-04-29 00:55:03.677226 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 00:55:03.677431 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 00:55:03.677755 (Thread-1): 17:55:03 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 00:55:03.678172 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 00:55:03.678304 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 00:55:03.678419 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 00:55:03.687174 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 00:55:03.687609 (Thread-1): finished collecting timing info
2020-04-29 00:55:03.694596 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:03.694756 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 00:55:03.866206 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:03.869939 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:03.870158 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 00:55:04.053231 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:04.056298 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 00:55:04.056839 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:04.056998 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 00:55:04.101850 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:04.102283 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:04.102557 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 00:55:04.181103 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-29 00:55:04.184247 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:04.184387 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 00:55:04.242426 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-29 00:55:04.243493 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 00:55:04.243610 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:04.243705 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 00:55:04.417299 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:55:04.420651 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:04.420803 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 00:55:05.044692 (Thread-1): SQL status: DROP VIEW in 0.62 seconds
2020-04-29 00:55:05.048939 (Thread-1): finished collecting timing info
2020-04-29 00:55:05.049775 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72d28b48-ece0-4425-9d55-d5dfde2be1bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2d3a50>]}
2020-04-29 00:55:05.050078 (Thread-1): 17:55:05 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 1.37s]
2020-04-29 00:55:05.050261 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 00:55:05.050500 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 00:55:05.050911 (Thread-1): 17:55:05 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 00:55:05.051538 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:05.051708 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 00:55:05.051828 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 00:55:05.059538 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 00:55:05.060003 (Thread-1): finished collecting timing info
2020-04-29 00:55:05.067539 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:05.067701 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 00:55:05.263174 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 00:55:05.265692 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:05.265811 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 00:55:05.440743 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:05.445249 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 00:55:05.445835 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:05.445988 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 00:55:05.486517 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:05.486950 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:05.487225 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 00:55:05.546924 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:55:05.550329 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:05.550464 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 00:55:05.591110 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:05.592211 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 00:55:05.592337 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:05.592436 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 00:55:05.769926 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 00:55:05.773371 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:05.773527 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 00:55:05.947604 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:05.950345 (Thread-1): finished collecting timing info
2020-04-29 00:55:05.950995 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72d28b48-ece0-4425-9d55-d5dfde2be1bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3661d0>]}
2020-04-29 00:55:05.951230 (Thread-1): 17:55:05 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.90s]
2020-04-29 00:55:05.951369 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 00:55:05.951543 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 00:55:05.951818 (Thread-1): 17:55:05 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 00:55:05.952318 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:05.952428 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 00:55:05.952529 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 00:55:05.960995 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 00:55:05.961452 (Thread-1): finished collecting timing info
2020-04-29 00:55:05.968063 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:05.968179 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 00:55:06.159890 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:55:06.163399 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:06.163572 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 00:55:06.347155 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:06.350238 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 00:55:06.350849 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:06.351007 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 00:55:06.392207 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:06.392642 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:06.392906 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
final as (
    select 
    order_flash.*,
    (SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0)))/ 
    COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
    (SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ 
    COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

    FROM order_flash INNER JOIN events USING (event_unique_id)
),

select * from final
  );

2020-04-29 00:55:06.435047 (Thread-1): Postgres error: syntax error at or near "AS"
LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
                                                    ^

2020-04-29 00:55:06.435477 (Thread-1): On model.order_history.order_flash_event: ROLLBACK
2020-04-29 00:55:06.476757 (Thread-1): finished collecting timing info
2020-04-29 00:55:06.477811 (Thread-1): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "AS"
  LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
                                                      ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "AS"
LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
                                                    ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "AS"
  LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
                                                      ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 00:55:06.480770 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72d28b48-ece0-4425-9d55-d5dfde2be1bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3661d0>]}
2020-04-29 00:55:06.481066 (Thread-1): 17:55:06 | 7 of 8 ERROR creating view model data_science.order_flash_event...... [ERROR in 0.53s]
2020-04-29 00:55:06.481254 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 00:55:06.481441 (Thread-1): Began running node model.order_history.customers
2020-04-29 00:55:06.481630 (Thread-1): 17:55:06 | 8 of 8 START view model data_science.customers....................... [RUN]
2020-04-29 00:55:06.481976 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 00:55:06.482250 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash_event).
2020-04-29 00:55:06.482417 (Thread-1): Compiling model.order_history.customers
2020-04-29 00:55:06.491982 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 00:55:06.492401 (Thread-1): finished collecting timing info
2020-04-29 00:55:06.499852 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:06.499986 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 00:55:06.581925 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 00:55:06.586309 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:06.586450 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 00:55:06.626697 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 00:55:06.628455 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 00:55:06.628884 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:06.628995 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 00:55:06.668970 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:06.669401 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:06.669673 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 00:55:06.785240 (Thread-1): SQL status: CREATE VIEW in 0.12 seconds
2020-04-29 00:55:06.789505 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:06.789659 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 00:55:06.831164 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:06.833142 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 00:55:06.833341 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:06.833504 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 00:55:07.045181 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-29 00:55:07.048554 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:07.048712 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 00:55:07.226858 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:07.230963 (Thread-1): finished collecting timing info
2020-04-29 00:55:07.231818 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72d28b48-ece0-4425-9d55-d5dfde2be1bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b225dd0>]}
2020-04-29 00:55:07.232127 (Thread-1): 17:55:07 | 8 of 8 OK created view model data_science.customers.................. [CREATE VIEW in 0.75s]
2020-04-29 00:55:07.232316 (Thread-1): Finished running node model.order_history.customers
2020-04-29 00:55:07.328284 (MainThread): Using postgres connection "master".
2020-04-29 00:55:07.328451 (MainThread): On master: BEGIN
2020-04-29 00:55:07.369087 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:07.369551 (MainThread): On master: COMMIT
2020-04-29 00:55:07.369841 (MainThread): Using postgres connection "master".
2020-04-29 00:55:07.369993 (MainThread): On master: COMMIT
2020-04-29 00:55:07.410305 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 00:55:07.411126 (MainThread): 17:55:07 | 
2020-04-29 00:55:07.411379 (MainThread): 17:55:07 | Finished running 8 view models in 9.59s.
2020-04-29 00:55:07.411595 (MainThread): Connection 'master' was left open.
2020-04-29 00:55:07.411763 (MainThread): On master: Close
2020-04-29 00:55:07.412163 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 00:55:07.412339 (MainThread): On model.order_history.customers: Close
2020-04-29 00:55:07.436815 (MainThread): 
2020-04-29 00:55:07.437026 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 00:55:07.437166 (MainThread): 
2020-04-29 00:55:07.437297 (MainThread): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
2020-04-29 00:55:07.437416 (MainThread):   syntax error at or near "AS"
2020-04-29 00:55:07.437525 (MainThread):   LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
2020-04-29 00:55:07.437631 (MainThread):                                                       ^
2020-04-29 00:55:07.437738 (MainThread):   compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 00:55:07.437856 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 00:55:07.438043 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b134190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad346d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b6510>]}
2020-04-29 00:55:07.438247 (MainThread): Flushing usage events
2020-04-29 00:55:51.046212 (MainThread): Running with dbt=0.16.1
2020-04-29 00:55:51.120482 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 00:55:51.121244 (MainThread): Tracking: tracking
2020-04-29 00:55:51.126365 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac16510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae9cf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac16b10>]}
2020-04-29 00:55:51.147237 (MainThread): Partial parsing not enabled
2020-04-29 00:55:51.149362 (MainThread): Parsing macros/core.sql
2020-04-29 00:55:51.154224 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 00:55:51.163044 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 00:55:51.164929 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 00:55:51.183225 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 00:55:51.217079 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 00:55:51.238515 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 00:55:51.240461 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 00:55:51.246883 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 00:55:51.259824 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 00:55:51.266830 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 00:55:51.273338 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 00:55:51.278420 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 00:55:51.279407 (MainThread): Parsing macros/etc/query.sql
2020-04-29 00:55:51.280523 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 00:55:51.282252 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 00:55:51.284389 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 00:55:51.293647 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 00:55:51.295632 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 00:55:51.296674 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 00:55:51.340529 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 00:55:51.341700 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 00:55:51.342628 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 00:55:51.343763 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 00:55:51.345994 (MainThread): Parsing macros/catalog.sql
2020-04-29 00:55:51.348296 (MainThread): Parsing macros/relations.sql
2020-04-29 00:55:51.349721 (MainThread): Parsing macros/adapters.sql
2020-04-29 00:55:51.366743 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 00:55:51.385821 (MainThread): Partial parsing not enabled
2020-04-29 00:55:51.421504 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 00:55:51.421640 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.438355 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:51.438464 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.442582 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:51.442672 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.447069 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 00:55:51.447158 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.451191 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 00:55:51.451281 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.455880 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:51.455987 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.461654 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 00:55:51.461756 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.468701 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:51.468809 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.613127 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 00:55:51.617117 (MainThread): 
2020-04-29 00:55:51.617566 (MainThread): Acquiring new postgres connection "master".
2020-04-29 00:55:51.617665 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:55:51.641900 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 00:55:51.642038 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 00:55:51.731257 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 00:55:51.731416 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 00:55:52.177501 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.45 seconds
2020-04-29 00:55:52.216154 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:55:52.216390 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 00:55:52.218554 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:55:52.218679 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 00:55:52.258147 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:52.258596 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:55:52.258895 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 00:55:52.367103 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-29 00:55:52.376493 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 00:55:52.452152 (MainThread): Using postgres connection "master".
2020-04-29 00:55:52.452300 (MainThread): On master: BEGIN
2020-04-29 00:55:52.803548 (MainThread): SQL status: BEGIN in 0.35 seconds
2020-04-29 00:55:52.803979 (MainThread): Using postgres connection "master".
2020-04-29 00:55:52.804263 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 00:55:52.938193 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-29 00:55:53.021284 (MainThread): On master: ROLLBACK
2020-04-29 00:55:53.060426 (MainThread): Using postgres connection "master".
2020-04-29 00:55:53.060840 (MainThread): On master: BEGIN
2020-04-29 00:55:53.137801 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 00:55:53.138258 (MainThread): On master: COMMIT
2020-04-29 00:55:53.138561 (MainThread): Using postgres connection "master".
2020-04-29 00:55:53.138729 (MainThread): On master: COMMIT
2020-04-29 00:55:53.176903 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 00:55:53.177628 (MainThread): 17:55:53 | Concurrency: 1 threads (target='dev')
2020-04-29 00:55:53.177878 (MainThread): 17:55:53 | 
2020-04-29 00:55:53.180170 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 00:55:53.180428 (Thread-1): 17:55:53 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 00:55:53.180816 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.180957 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 00:55:53.181101 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 00:55:53.197379 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 00:55:53.197867 (Thread-1): finished collecting timing info
2020-04-29 00:55:53.239574 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.239738 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 00:55:53.318687 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 00:55:53.323022 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.323175 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 00:55:53.363104 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 00:55:53.366202 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 00:55:53.366827 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.366985 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 00:55:53.405875 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:53.406133 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.406268 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 00:55:53.461539 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:55:53.465651 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.465769 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 00:55:53.506283 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:53.510620 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.510775 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 00:55:53.553723 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:53.555686 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 00:55:53.555885 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.556049 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 00:55:53.725666 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:55:53.729199 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:55:53.729364 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 00:55:53.916294 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:55:53.920661 (Thread-1): finished collecting timing info
2020-04-29 00:55:53.921527 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584a330d-3bed-41d2-bb45-a38fde484103', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b246810>]}
2020-04-29 00:55:53.921846 (Thread-1): 17:55:53 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.74s]
2020-04-29 00:55:53.922039 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 00:55:53.922227 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 00:55:53.922414 (Thread-1): 17:55:53 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 00:55:53.922761 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 00:55:53.922896 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 00:55:53.923049 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 00:55:53.929316 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 00:55:53.929746 (Thread-1): finished collecting timing info
2020-04-29 00:55:53.937399 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:53.937533 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 00:55:54.106712 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:54.109669 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:54.109809 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 00:55:54.296442 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:55:54.300757 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 00:55:54.301343 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:54.301503 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 00:55:54.341247 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:54.341663 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:54.341842 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zones.zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 00:55:54.402221 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:55:54.438370 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:54.438573 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 00:55:54.482800 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:54.486766 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:54.486945 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 00:55:54.526999 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:54.528995 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 00:55:54.529202 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:54.529369 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 00:55:54.699846 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:55:54.703270 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:55:54.703426 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 00:55:54.888653 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:55:54.892931 (Thread-1): finished collecting timing info
2020-04-29 00:55:54.893770 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584a330d-3bed-41d2-bb45-a38fde484103', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b49e690>]}
2020-04-29 00:55:54.894079 (Thread-1): 17:55:54 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.97s]
2020-04-29 00:55:54.894261 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 00:55:54.894499 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 00:55:54.894933 (Thread-1): 17:55:54 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 00:55:54.895613 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:54.895756 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 00:55:54.895876 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 00:55:54.901945 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 00:55:54.902384 (Thread-1): finished collecting timing info
2020-04-29 00:55:54.909886 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:54.910020 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 00:55:55.125435 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 00:55:55.128649 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:55.128788 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 00:55:55.304476 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:55.307464 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 00:55:55.308056 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:55.308210 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 00:55:55.347896 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:55.348334 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:55.348612 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 00:55:55.403093 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:55:55.409307 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:55.409470 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 00:55:55.449730 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:55.455189 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:55.455345 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 00:55:55.495153 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:55.496331 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 00:55:55.496466 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:55.496576 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 00:55:55.669514 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:55:55.672914 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:55:55.673073 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 00:55:55.853519 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:55.857489 (Thread-1): finished collecting timing info
2020-04-29 00:55:55.858224 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584a330d-3bed-41d2-bb45-a38fde484103', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0d2510>]}
2020-04-29 00:55:55.858491 (Thread-1): 17:55:55 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.96s]
2020-04-29 00:55:55.858656 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 00:55:55.858867 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 00:55:55.859212 (Thread-1): 17:55:55 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 00:55:55.859635 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 00:55:55.859783 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 00:55:55.859911 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 00:55:55.865613 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 00:55:55.866010 (Thread-1): finished collecting timing info
2020-04-29 00:55:55.872570 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:55.872691 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 00:55:56.043996 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:56.048163 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:56.048312 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 00:55:56.216376 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:56.218112 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 00:55:56.218545 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:56.218658 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 00:55:56.258277 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:56.258718 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:56.259002 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime

FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 00:55:56.314922 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:55:56.321131 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:56.321291 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 00:55:56.361350 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:56.365648 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:56.365806 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 00:55:56.409181 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:56.411116 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 00:55:56.411306 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:56.411470 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 00:55:56.586391 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:55:56.589871 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:55:56.590030 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 00:55:56.765084 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:56.770719 (Thread-1): finished collecting timing info
2020-04-29 00:55:56.771568 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584a330d-3bed-41d2-bb45-a38fde484103', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a33d110>]}
2020-04-29 00:55:56.771882 (Thread-1): 17:55:56 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 0.91s]
2020-04-29 00:55:56.772043 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 00:55:56.772241 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 00:55:56.772577 (Thread-1): 17:55:56 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 00:55:56.773070 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 00:55:56.773265 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 00:55:56.773502 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 00:55:56.782490 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 00:55:56.782946 (Thread-1): finished collecting timing info
2020-04-29 00:55:56.790317 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:56.790455 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 00:55:56.983617 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:55:56.987720 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:56.987881 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 00:55:57.157200 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:57.160263 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 00:55:57.160883 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:57.161045 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 00:55:57.205457 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:57.205658 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:57.205772 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 00:55:57.259352 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:55:57.263615 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:57.263770 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 00:55:57.306547 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:57.308573 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 00:55:57.308779 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:57.308943 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 00:55:57.477720 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:55:57.480297 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:55:57.480447 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 00:55:57.650702 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:57.654285 (Thread-1): finished collecting timing info
2020-04-29 00:55:57.655129 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584a330d-3bed-41d2-bb45-a38fde484103', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aef6850>]}
2020-04-29 00:55:57.655397 (Thread-1): 17:55:57 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 0.88s]
2020-04-29 00:55:57.655556 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 00:55:57.655717 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 00:55:57.656013 (Thread-1): 17:55:57 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 00:55:57.656610 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:57.656868 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 00:55:57.657055 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 00:55:57.664719 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 00:55:57.665166 (Thread-1): finished collecting timing info
2020-04-29 00:55:57.672473 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:57.672603 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 00:55:57.885975 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 00:55:57.888852 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:57.888986 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 00:55:58.065970 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:55:58.070464 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 00:55:58.071068 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:58.071229 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 00:55:58.110986 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:58.111422 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:58.111706 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 00:55:58.165439 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:55:58.169228 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:58.169403 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 00:55:58.209434 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:55:58.210793 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 00:55:58.210956 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:58.211087 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 00:55:58.382248 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:55:58.385712 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:55:58.385868 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 00:55:58.582188 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 00:55:58.586590 (Thread-1): finished collecting timing info
2020-04-29 00:55:58.587585 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584a330d-3bed-41d2-bb45-a38fde484103', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b11af90>]}
2020-04-29 00:55:58.587945 (Thread-1): 17:55:58 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.93s]
2020-04-29 00:55:58.588166 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 00:55:58.588439 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 00:55:58.588823 (Thread-1): 17:55:58 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 00:55:58.589463 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:58.589640 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 00:55:58.589816 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 00:55:58.601178 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 00:55:58.601656 (Thread-1): finished collecting timing info
2020-04-29 00:55:58.608390 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:58.608513 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 00:55:58.777894 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:58.782055 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:58.782213 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 00:55:58.969714 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:55:58.972849 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 00:55:58.973456 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:58.973613 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 00:55:59.013256 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:59.013691 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:55:59.013974 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
final as (
    select 
    order_flash.*,
    (SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0)))/ 
    COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
    (SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ 
    COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

    FROM order_flash INNER JOIN events USING (event_unique_id)
),

select * from final
  );

2020-04-29 00:55:59.054817 (Thread-1): Postgres error: syntax error at or near "AS"
LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
                                                    ^

2020-04-29 00:55:59.055281 (Thread-1): On model.order_history.order_flash_event: ROLLBACK
2020-04-29 00:55:59.095021 (Thread-1): finished collecting timing info
2020-04-29 00:55:59.096067 (Thread-1): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "AS"
  LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
                                                      ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "AS"
LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
                                                    ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "AS"
  LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
                                                      ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 00:55:59.098993 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584a330d-3bed-41d2-bb45-a38fde484103', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b037a50>]}
2020-04-29 00:55:59.099292 (Thread-1): 17:55:59 | 7 of 8 ERROR creating view model data_science.order_flash_event...... [ERROR in 0.51s]
2020-04-29 00:55:59.099480 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 00:55:59.099669 (Thread-1): Began running node model.order_history.customers
2020-04-29 00:55:59.099854 (Thread-1): 17:55:59 | 8 of 8 START view model data_science.customers....................... [RUN]
2020-04-29 00:55:59.100202 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 00:55:59.100336 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash_event).
2020-04-29 00:55:59.100471 (Thread-1): Compiling model.order_history.customers
2020-04-29 00:55:59.110064 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 00:55:59.110465 (Thread-1): finished collecting timing info
2020-04-29 00:55:59.118025 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:59.118166 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 00:55:59.197354 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 00:55:59.202826 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:59.202981 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 00:55:59.242500 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 00:55:59.244624 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 00:55:59.245119 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:59.245253 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 00:55:59.284047 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:55:59.284256 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:59.284370 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 00:55:59.341309 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:55:59.344810 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:59.344958 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 00:55:59.411863 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2020-04-29 00:55:59.413057 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 00:55:59.413188 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:59.413297 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 00:55:59.587496 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:55:59.590086 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:55:59.590242 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 00:55:59.764090 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:55:59.768362 (Thread-1): finished collecting timing info
2020-04-29 00:55:59.769216 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584a330d-3bed-41d2-bb45-a38fde484103', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b025790>]}
2020-04-29 00:55:59.769531 (Thread-1): 17:55:59 | 8 of 8 OK created view model data_science.customers.................. [CREATE VIEW in 0.67s]
2020-04-29 00:55:59.769717 (Thread-1): Finished running node model.order_history.customers
2020-04-29 00:55:59.786260 (MainThread): Using postgres connection "master".
2020-04-29 00:55:59.786575 (MainThread): On master: BEGIN
2020-04-29 00:55:59.852197 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-29 00:55:59.852622 (MainThread): On master: COMMIT
2020-04-29 00:55:59.852841 (MainThread): Using postgres connection "master".
2020-04-29 00:55:59.853002 (MainThread): On master: COMMIT
2020-04-29 00:55:59.891728 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 00:55:59.892706 (MainThread): 17:55:59 | 
2020-04-29 00:55:59.892964 (MainThread): 17:55:59 | Finished running 8 view models in 8.28s.
2020-04-29 00:55:59.893161 (MainThread): Connection 'master' was left open.
2020-04-29 00:55:59.893318 (MainThread): On master: Close
2020-04-29 00:55:59.893702 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 00:55:59.893868 (MainThread): On model.order_history.customers: Close
2020-04-29 00:55:59.918713 (MainThread): 
2020-04-29 00:55:59.918977 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 00:55:59.919152 (MainThread): 
2020-04-29 00:55:59.919302 (MainThread): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
2020-04-29 00:55:59.919437 (MainThread):   syntax error at or near "AS"
2020-04-29 00:55:59.919561 (MainThread):   LINE 15:     order_ticket_unique_id  ELSE NULL END) AS average_days_s...
2020-04-29 00:55:59.919681 (MainThread):                                                       ^
2020-04-29 00:55:59.919797 (MainThread):   compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 00:55:59.919935 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 00:55:59.920167 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b070350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b199710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b199fd0>]}
2020-04-29 00:55:59.920429 (MainThread): Flushing usage events
2020-04-29 00:59:46.652173 (MainThread): Running with dbt=0.16.1
2020-04-29 00:59:46.723259 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 00:59:46.724150 (MainThread): Tracking: tracking
2020-04-29 00:59:46.729740 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0bb910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b342e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b342f10>]}
2020-04-29 00:59:46.749145 (MainThread): Partial parsing not enabled
2020-04-29 00:59:46.750972 (MainThread): Parsing macros/core.sql
2020-04-29 00:59:46.755631 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 00:59:46.763724 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 00:59:46.765513 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 00:59:46.783766 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 00:59:46.817407 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 00:59:46.838838 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 00:59:46.840758 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 00:59:46.847140 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 00:59:46.859814 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 00:59:46.866699 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 00:59:46.873065 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 00:59:46.878105 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 00:59:46.879081 (MainThread): Parsing macros/etc/query.sql
2020-04-29 00:59:46.880184 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 00:59:46.881887 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 00:59:46.883995 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 00:59:46.893134 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 00:59:46.895227 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 00:59:46.896307 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 00:59:46.939915 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 00:59:46.941387 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 00:59:46.942664 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 00:59:46.944297 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 00:59:46.946787 (MainThread): Parsing macros/catalog.sql
2020-04-29 00:59:46.949386 (MainThread): Parsing macros/relations.sql
2020-04-29 00:59:46.951593 (MainThread): Parsing macros/adapters.sql
2020-04-29 00:59:46.974295 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 00:59:46.992414 (MainThread): Partial parsing not enabled
2020-04-29 00:59:47.020646 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 00:59:47.020770 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.036854 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:47.036949 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.041177 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:47.041270 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.045624 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 00:59:47.045759 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.049786 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 00:59:47.049875 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.054488 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:47.054599 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.060279 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 00:59:47.060379 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.066923 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 00:59:47.067039 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.208052 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 00:59:47.212949 (MainThread): 
2020-04-29 00:59:47.213257 (MainThread): Acquiring new postgres connection "master".
2020-04-29 00:59:47.213352 (MainThread): Opening a new connection, currently in state init
2020-04-29 00:59:47.238007 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 00:59:47.238148 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 00:59:47.331736 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 00:59:47.331871 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 00:59:47.836439 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.50 seconds
2020-04-29 00:59:47.875760 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:59:47.875974 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 00:59:47.877945 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:59:47.878092 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 00:59:47.947772 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.07 seconds
2020-04-29 00:59:47.947972 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 00:59:47.948087 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 00:59:48.050870 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-29 00:59:48.060050 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 00:59:48.167930 (MainThread): Using postgres connection "master".
2020-04-29 00:59:48.168079 (MainThread): On master: BEGIN
2020-04-29 00:59:48.571888 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-29 00:59:48.572326 (MainThread): Using postgres connection "master".
2020-04-29 00:59:48.572603 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 00:59:48.705699 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-29 00:59:48.788293 (MainThread): On master: ROLLBACK
2020-04-29 00:59:48.829286 (MainThread): Using postgres connection "master".
2020-04-29 00:59:48.829676 (MainThread): On master: BEGIN
2020-04-29 00:59:48.913438 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 00:59:48.913738 (MainThread): On master: COMMIT
2020-04-29 00:59:48.913900 (MainThread): Using postgres connection "master".
2020-04-29 00:59:48.914036 (MainThread): On master: COMMIT
2020-04-29 00:59:48.953391 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 00:59:48.953914 (MainThread): 17:59:48 | Concurrency: 1 threads (target='dev')
2020-04-29 00:59:48.954112 (MainThread): 17:59:48 | 
2020-04-29 00:59:48.956203 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 00:59:48.956483 (Thread-1): 17:59:48 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 00:59:48.956904 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:48.957021 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 00:59:48.957143 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 00:59:48.973475 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 00:59:48.973998 (Thread-1): finished collecting timing info
2020-04-29 00:59:49.018704 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:49.018865 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 00:59:49.109130 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-29 00:59:49.113470 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:49.113621 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 00:59:49.153665 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 00:59:49.156758 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 00:59:49.157400 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:49.157548 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 00:59:49.197890 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:59:49.198325 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:49.198589 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 00:59:49.249384 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:59:49.255660 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:49.255816 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 00:59:49.296856 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:49.300477 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:49.300631 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 00:59:49.343914 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:49.345062 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 00:59:49.345195 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:49.345298 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 00:59:49.515759 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:59:49.519335 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 00:59:49.519498 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 00:59:49.746582 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-29 00:59:49.750969 (Thread-1): finished collecting timing info
2020-04-29 00:59:49.751839 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e49a3c79-5a89-4ba7-bd86-d7fbc365fe47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b5becd0>]}
2020-04-29 00:59:49.752160 (Thread-1): 17:59:49 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.79s]
2020-04-29 00:59:49.752345 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 00:59:49.752597 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 00:59:49.752934 (Thread-1): 17:59:49 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 00:59:49.753304 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 00:59:49.753458 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 00:59:49.753567 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 00:59:49.759615 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 00:59:49.760034 (Thread-1): finished collecting timing info
2020-04-29 00:59:49.767446 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:59:49.767570 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 00:59:50.494536 (Thread-1): SQL status: DROP VIEW in 0.73 seconds
2020-04-29 00:59:50.498345 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:59:50.498507 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 00:59:50.667607 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:50.670869 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 00:59:50.671387 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:59:50.671518 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 00:59:50.714583 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:59:50.714774 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:59:50.714877 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zones.zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 00:59:50.777995 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:59:50.812260 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:59:50.812452 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 00:59:50.854570 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:50.858936 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:59:50.859092 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 00:59:50.899437 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:50.901385 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 00:59:50.901582 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:59:50.901742 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 00:59:51.138857 (Thread-1): SQL status: COMMIT in 0.24 seconds
2020-04-29 00:59:51.142275 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 00:59:51.142429 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 00:59:51.316541 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:51.320992 (Thread-1): finished collecting timing info
2020-04-29 00:59:51.321838 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e49a3c79-5a89-4ba7-bd86-d7fbc365fe47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4bfb90>]}
2020-04-29 00:59:51.322143 (Thread-1): 17:59:51 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.57s]
2020-04-29 00:59:51.322324 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 00:59:51.322552 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 00:59:51.322873 (Thread-1): 17:59:51 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 00:59:51.323609 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:51.323873 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 00:59:51.324065 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 00:59:51.330196 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 00:59:51.330629 (Thread-1): finished collecting timing info
2020-04-29 00:59:51.338386 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:51.338544 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 00:59:51.533404 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:59:51.537533 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:51.537692 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 00:59:51.705646 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:51.707846 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 00:59:51.708492 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:51.708627 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 00:59:51.749207 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:59:51.749387 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:51.749486 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 00:59:51.798865 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:59:51.803109 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:51.803252 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 00:59:51.844331 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:51.849066 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:51.849203 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 00:59:51.890008 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:51.891262 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 00:59:51.891395 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:51.891498 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 00:59:52.057794 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:59:52.059739 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 00:59:52.059851 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 00:59:52.237128 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:59:52.241388 (Thread-1): finished collecting timing info
2020-04-29 00:59:52.242297 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e49a3c79-5a89-4ba7-bd86-d7fbc365fe47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b643550>]}
2020-04-29 00:59:52.242617 (Thread-1): 17:59:52 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.92s]
2020-04-29 00:59:52.242804 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 00:59:52.242991 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 00:59:52.243181 (Thread-1): 17:59:52 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 00:59:52.243527 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.243771 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 00:59:52.243927 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 00:59:52.249703 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 00:59:52.250151 (Thread-1): finished collecting timing info
2020-04-29 00:59:52.256593 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.256706 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 00:59:52.426863 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:52.431066 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.431230 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 00:59:52.605815 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:52.608300 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 00:59:52.608835 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.608999 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 00:59:52.655195 (Thread-1): SQL status: BEGIN in 0.05 seconds
2020-04-29 00:59:52.655629 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.655904 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime

FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 00:59:52.705044 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:59:52.709664 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.709820 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 00:59:52.750017 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:52.754375 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.754539 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 00:59:52.800869 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 00:59:52.802071 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 00:59:52.802221 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.802313 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 00:59:52.990477 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-29 00:59:52.994027 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 00:59:52.994185 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 00:59:54.037877 (Thread-1): SQL status: DROP VIEW in 1.04 seconds
2020-04-29 00:59:54.043334 (Thread-1): finished collecting timing info
2020-04-29 00:59:54.044140 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e49a3c79-5a89-4ba7-bd86-d7fbc365fe47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b643550>]}
2020-04-29 00:59:54.044423 (Thread-1): 17:59:54 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 1.80s]
2020-04-29 00:59:54.044581 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 00:59:54.044811 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 00:59:54.045152 (Thread-1): 17:59:54 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 00:59:54.045577 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 00:59:54.045765 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 00:59:54.045939 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 00:59:54.056185 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 00:59:54.056702 (Thread-1): finished collecting timing info
2020-04-29 00:59:54.064252 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:59:54.064417 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 00:59:54.236858 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:54.241044 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:59:54.241205 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 00:59:54.429746 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:59:54.432329 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 00:59:54.432918 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:59:54.433113 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 00:59:54.473229 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:59:54.473459 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:59:54.473596 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 00:59:54.530798 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 00:59:54.533468 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:59:54.533591 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 00:59:54.577352 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:54.579038 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 00:59:54.579238 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:59:54.579395 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 00:59:54.747474 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 00:59:54.749660 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 00:59:54.749792 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 00:59:54.921164 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:54.925415 (Thread-1): finished collecting timing info
2020-04-29 00:59:54.926271 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e49a3c79-5a89-4ba7-bd86-d7fbc365fe47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b396110>]}
2020-04-29 00:59:54.926585 (Thread-1): 17:59:54 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 0.88s]
2020-04-29 00:59:54.926770 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 00:59:54.926961 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 00:59:54.927148 (Thread-1): 17:59:54 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 00:59:54.927491 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:54.927626 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 00:59:54.927762 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 00:59:54.935698 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 00:59:54.936121 (Thread-1): finished collecting timing info
2020-04-29 00:59:54.943506 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:54.943633 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 00:59:55.133763 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:59:55.137936 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:55.138092 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 00:59:55.308506 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:55.313131 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 00:59:55.313775 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:55.313941 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 00:59:55.355085 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:59:55.355528 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:55.355818 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 00:59:55.408647 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:59:55.411938 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:55.412078 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 00:59:55.451581 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:55.452642 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 00:59:55.452763 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:55.452862 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 00:59:55.628471 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 00:59:55.631915 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 00:59:55.632069 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 00:59:55.804606 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:55.807354 (Thread-1): finished collecting timing info
2020-04-29 00:59:55.807999 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e49a3c79-5a89-4ba7-bd86-d7fbc365fe47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4cd110>]}
2020-04-29 00:59:55.808237 (Thread-1): 17:59:55 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.88s]
2020-04-29 00:59:55.808374 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 00:59:55.808585 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 00:59:55.808955 (Thread-1): 17:59:55 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 00:59:55.809416 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 00:59:55.809530 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 00:59:55.809630 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 00:59:55.817446 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 00:59:55.817838 (Thread-1): finished collecting timing info
2020-04-29 00:59:55.824476 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:59:55.824594 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 00:59:56.018178 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 00:59:56.022345 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:59:56.022499 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 00:59:56.193423 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 00:59:56.196485 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 00:59:56.197088 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:59:56.197250 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 00:59:56.237239 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:59:56.237676 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 00:59:56.237962 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
final as (
    select 
    order_flash.*,
    SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,

    SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event

    FROM order_flash INNER JOIN events USING (event_unique_id)
),

select * from final
  );

2020-04-29 00:59:56.278683 (Thread-1): Postgres error: syntax error at or near "select"
LINE 22: select * from final
         ^

2020-04-29 00:59:56.279075 (Thread-1): On model.order_history.order_flash_event: ROLLBACK
2020-04-29 00:59:56.319210 (Thread-1): finished collecting timing info
2020-04-29 00:59:56.320262 (Thread-1): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "select"
  LINE 22: select * from final
           ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "select"
LINE 22: select * from final
         ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "select"
  LINE 22: select * from final
           ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 00:59:56.323226 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e49a3c79-5a89-4ba7-bd86-d7fbc365fe47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b65ca50>]}
2020-04-29 00:59:56.323526 (Thread-1): 17:59:56 | 7 of 8 ERROR creating view model data_science.order_flash_event...... [ERROR in 0.51s]
2020-04-29 00:59:56.323710 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 00:59:56.323987 (Thread-1): Began running node model.order_history.customers
2020-04-29 00:59:56.324474 (Thread-1): 17:59:56 | 8 of 8 START view model data_science.customers....................... [RUN]
2020-04-29 00:59:56.324862 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 00:59:56.324999 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash_event).
2020-04-29 00:59:56.325129 (Thread-1): Compiling model.order_history.customers
2020-04-29 00:59:56.334828 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 00:59:56.335260 (Thread-1): finished collecting timing info
2020-04-29 00:59:56.342761 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:59:56.342907 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 00:59:56.423621 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 00:59:56.427948 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:59:56.428124 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 00:59:56.468204 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 00:59:56.469931 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 00:59:56.470366 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:59:56.470479 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 00:59:56.510013 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:59:56.510361 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:59:56.510531 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 00:59:56.564939 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 00:59:56.569235 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:59:56.569392 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 00:59:56.610958 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 00:59:56.612884 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 00:59:56.613076 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:59:56.613237 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 00:59:56.833661 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-29 00:59:56.836351 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 00:59:56.836512 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 00:59:57.015843 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 00:59:57.020106 (Thread-1): finished collecting timing info
2020-04-29 00:59:57.020954 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e49a3c79-5a89-4ba7-bd86-d7fbc365fe47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b99a810>]}
2020-04-29 00:59:57.021259 (Thread-1): 17:59:57 | 8 of 8 OK created view model data_science.customers.................. [CREATE VIEW in 0.70s]
2020-04-29 00:59:57.021439 (Thread-1): Finished running node model.order_history.customers
2020-04-29 00:59:57.062543 (MainThread): Using postgres connection "master".
2020-04-29 00:59:57.062821 (MainThread): On master: BEGIN
2020-04-29 00:59:57.106234 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 00:59:57.106463 (MainThread): On master: COMMIT
2020-04-29 00:59:57.106584 (MainThread): Using postgres connection "master".
2020-04-29 00:59:57.106691 (MainThread): On master: COMMIT
2020-04-29 00:59:57.145697 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 00:59:57.146139 (MainThread): 17:59:57 | 
2020-04-29 00:59:57.146289 (MainThread): 17:59:57 | Finished running 8 view models in 9.93s.
2020-04-29 00:59:57.146406 (MainThread): Connection 'master' was left open.
2020-04-29 00:59:57.146508 (MainThread): On master: Close
2020-04-29 00:59:57.146760 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 00:59:57.146909 (MainThread): On model.order_history.customers: Close
2020-04-29 00:59:57.167865 (MainThread): 
2020-04-29 00:59:57.168105 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 00:59:57.168242 (MainThread): 
2020-04-29 00:59:57.168404 (MainThread): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
2020-04-29 00:59:57.168586 (MainThread):   syntax error at or near "select"
2020-04-29 00:59:57.168702 (MainThread):   LINE 22: select * from final
2020-04-29 00:59:57.168806 (MainThread):            ^
2020-04-29 00:59:57.168909 (MainThread):   compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 00:59:57.169022 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 00:59:57.169203 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4fced0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b52c910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b52c5d0>]}
2020-04-29 00:59:57.169404 (MainThread): Flushing usage events
2020-04-29 01:02:29.699709 (MainThread): Running with dbt=0.16.1
2020-04-29 01:02:29.768230 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:02:29.769177 (MainThread): Tracking: tracking
2020-04-29 01:02:29.775101 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f3b290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cc5c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cbcf10>]}
2020-04-29 01:02:29.800088 (MainThread): Partial parsing not enabled
2020-04-29 01:02:29.802001 (MainThread): Parsing macros/core.sql
2020-04-29 01:02:29.806793 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:02:29.815052 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:02:29.816875 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:02:29.835508 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:02:29.872335 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:02:29.894388 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:02:29.896396 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:02:29.902969 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:02:29.915879 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:02:29.922902 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:02:29.929345 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:02:29.934478 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:02:29.935468 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:02:29.936633 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:02:29.938647 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:02:29.940810 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:02:29.950142 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:02:29.952261 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:02:29.953366 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:02:29.996049 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:02:29.997266 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:02:29.998236 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:02:29.999378 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:02:30.001648 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:02:30.004439 (MainThread): Parsing macros/relations.sql
2020-04-29 01:02:30.006066 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:02:30.024705 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:02:30.042366 (MainThread): Partial parsing not enabled
2020-04-29 01:02:30.072049 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:02:30.072250 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.094012 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:30.094144 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.099060 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:30.099152 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.103830 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:02:30.103925 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.107949 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:02:30.108040 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.112503 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:30.112596 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.117658 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:02:30.117748 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.124121 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:02:30.124224 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.267075 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:02:30.272146 (MainThread): 
2020-04-29 01:02:30.272468 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:02:30.272567 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:02:30.297398 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:02:30.297540 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:02:30.388769 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:02:30.388968 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:02:30.914112 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.52 seconds
2020-04-29 01:02:30.952064 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:02:30.952193 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:02:30.954062 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:02:30.954178 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:02:31.014290 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.06 seconds
2020-04-29 01:02:31.014727 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:02:31.014924 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:02:31.198033 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.18 seconds
2020-04-29 01:02:31.207382 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:02:31.282865 (MainThread): Using postgres connection "master".
2020-04-29 01:02:31.283023 (MainThread): On master: BEGIN
2020-04-29 01:02:31.663775 (MainThread): SQL status: BEGIN in 0.38 seconds
2020-04-29 01:02:31.664075 (MainThread): Using postgres connection "master".
2020-04-29 01:02:31.664252 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:02:31.891986 (MainThread): SQL status: SELECT in 0.23 seconds
2020-04-29 01:02:31.975452 (MainThread): On master: ROLLBACK
2020-04-29 01:02:32.019056 (MainThread): Using postgres connection "master".
2020-04-29 01:02:32.019275 (MainThread): On master: BEGIN
2020-04-29 01:02:32.101140 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 01:02:32.101606 (MainThread): On master: COMMIT
2020-04-29 01:02:32.101911 (MainThread): Using postgres connection "master".
2020-04-29 01:02:32.102072 (MainThread): On master: COMMIT
2020-04-29 01:02:32.143462 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:02:32.144112 (MainThread): 18:02:32 | Concurrency: 1 threads (target='dev')
2020-04-29 01:02:32.144362 (MainThread): 18:02:32 | 
2020-04-29 01:02:32.146877 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 01:02:32.147172 (Thread-1): 18:02:32 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 01:02:32.147577 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.147722 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:02:32.147881 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 01:02:32.165947 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 01:02:32.166533 (Thread-1): finished collecting timing info
2020-04-29 01:02:32.211814 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.212007 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 01:02:32.297904 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-29 01:02:32.302262 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.302419 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:02:32.341130 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:02:32.344277 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 01:02:32.344905 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.345057 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 01:02:32.382683 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:02:32.382946 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.383090 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 01:02:32.457820 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-29 01:02:32.464144 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.464303 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 01:02:32.507827 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:32.512248 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.512412 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 01:02:32.554123 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:32.556073 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:02:32.556275 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.556438 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:02:32.820843 (Thread-1): SQL status: COMMIT in 0.26 seconds
2020-04-29 01:02:32.824441 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:02:32.824604 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:02:33.081559 (Thread-1): SQL status: DROP VIEW in 0.26 seconds
2020-04-29 01:02:33.084333 (Thread-1): finished collecting timing info
2020-04-29 01:02:33.085031 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ad24dd5-ee0a-4e22-9d4f-397c2a201c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082eb210>]}
2020-04-29 01:02:33.085295 (Thread-1): 18:02:33 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.94s]
2020-04-29 01:02:33.085445 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 01:02:33.085589 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 01:02:33.085824 (Thread-1): 18:02:33 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 01:02:33.086111 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:02:33.086216 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 01:02:33.086317 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 01:02:33.091880 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 01:02:33.092299 (Thread-1): finished collecting timing info
2020-04-29 01:02:33.098958 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:02:33.099079 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 01:02:33.308861 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 01:02:33.313086 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:02:33.313242 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:02:35.494711 (Thread-1): SQL status: DROP VIEW in 2.18 seconds
2020-04-29 01:02:35.498987 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 01:02:35.499557 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:02:35.499714 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 01:02:35.538397 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:02:35.538837 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:02:35.539108 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zones.zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 01:02:36.812548 (Thread-1): SQL status: CREATE VIEW in 1.27 seconds
2020-04-29 01:02:36.843606 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:02:36.843792 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 01:02:36.887434 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:36.891805 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:02:36.891961 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 01:02:36.930902 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:36.932556 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:02:36.932749 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:02:36.932910 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:02:37.101823 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:02:37.104822 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:02:37.104975 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:02:37.292714 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:02:37.296474 (Thread-1): finished collecting timing info
2020-04-29 01:02:37.297311 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ad24dd5-ee0a-4e22-9d4f-397c2a201c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10854ddd0>]}
2020-04-29 01:02:37.297576 (Thread-1): 18:02:37 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 4.21s]
2020-04-29 01:02:37.297733 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 01:02:37.297893 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 01:02:37.298335 (Thread-1): 18:02:37 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 01:02:37.298785 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:37.299057 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 01:02:37.299285 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 01:02:37.305390 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 01:02:37.305837 (Thread-1): finished collecting timing info
2020-04-29 01:02:37.313339 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:37.313471 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 01:02:37.484617 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:02:37.488145 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:37.488333 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:02:37.656746 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:02:37.659742 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 01:02:37.660335 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:37.660492 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 01:02:37.698688 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:02:37.699196 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:37.699518 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 01:02:37.750334 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:02:37.756563 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:37.756716 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 01:02:37.798521 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:37.802565 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:37.802758 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 01:02:37.840306 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:37.841463 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:02:37.841581 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:37.841676 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:02:38.043898 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-29 01:02:38.047337 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:02:38.047497 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:02:38.268446 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:02:38.272295 (Thread-1): finished collecting timing info
2020-04-29 01:02:38.273156 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ad24dd5-ee0a-4e22-9d4f-397c2a201c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108250150>]}
2020-04-29 01:02:38.273466 (Thread-1): 18:02:38 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.97s]
2020-04-29 01:02:38.273648 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 01:02:38.273835 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 01:02:38.274025 (Thread-1): 18:02:38 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 01:02:38.274438 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:02:38.274570 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 01:02:38.274691 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 01:02:38.280836 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 01:02:38.281353 (Thread-1): finished collecting timing info
2020-04-29 01:02:38.288628 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:02:38.288758 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 01:02:38.512990 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:02:38.515550 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:02:38.515670 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:02:40.132411 (Thread-1): SQL status: DROP VIEW in 1.62 seconds
2020-04-29 01:02:40.134532 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 01:02:40.135091 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:02:40.135229 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 01:02:40.173208 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:02:40.173401 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:02:40.173509 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime

FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 01:02:40.225181 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:02:40.231402 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:02:40.231564 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 01:02:40.274036 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:40.278398 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:02:40.278560 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 01:02:40.317123 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:40.318930 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:02:40.319125 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:02:40.319285 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:02:40.487296 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:02:40.489600 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:02:40.489750 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:02:40.711053 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:02:40.716654 (Thread-1): finished collecting timing info
2020-04-29 01:02:40.717509 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ad24dd5-ee0a-4e22-9d4f-397c2a201c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108223f10>]}
2020-04-29 01:02:40.717889 (Thread-1): 18:02:40 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 2.44s]
2020-04-29 01:02:40.718047 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 01:02:40.718218 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 01:02:40.718383 (Thread-1): 18:02:40 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 01:02:40.718895 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:02:40.719166 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 01:02:40.719306 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 01:02:40.728277 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 01:02:40.728749 (Thread-1): finished collecting timing info
2020-04-29 01:02:40.736169 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:02:40.736298 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 01:02:40.911835 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:02:40.915267 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:02:40.915418 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:02:41.083761 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:02:41.086855 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 01:02:41.087492 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:02:41.087653 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 01:02:41.126103 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:02:41.126542 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:02:41.126833 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 01:02:41.181153 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:02:41.184777 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:02:41.184953 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 01:02:41.238755 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:02:41.240715 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:02:41.240925 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:02:41.241089 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:02:41.411887 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:02:41.415025 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:02:41.415184 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:02:41.582616 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:02:41.586842 (Thread-1): finished collecting timing info
2020-04-29 01:02:41.587701 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ad24dd5-ee0a-4e22-9d4f-397c2a201c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10854a3d0>]}
2020-04-29 01:02:41.588006 (Thread-1): 18:02:41 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 0.87s]
2020-04-29 01:02:41.588199 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 01:02:41.588444 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 01:02:41.588894 (Thread-1): 18:02:41 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 01:02:41.589273 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:41.589409 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 01:02:41.589542 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 01:02:41.597291 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 01:02:41.597674 (Thread-1): finished collecting timing info
2020-04-29 01:02:41.604262 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:41.604377 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 01:02:41.770860 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:02:41.775006 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:41.775160 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:02:41.942570 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:02:41.946871 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 01:02:41.947450 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:41.947610 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 01:02:41.985686 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:02:41.986125 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:41.986401 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 01:02:42.038514 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:02:42.042750 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:42.042906 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 01:02:42.081255 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:42.083218 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:02:42.083421 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:42.083584 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:02:42.252656 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:02:42.256088 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:02:42.256246 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:02:42.428624 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:02:42.432417 (Thread-1): finished collecting timing info
2020-04-29 01:02:42.433247 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ad24dd5-ee0a-4e22-9d4f-397c2a201c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081de590>]}
2020-04-29 01:02:42.433526 (Thread-1): 18:02:42 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.84s]
2020-04-29 01:02:42.433690 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 01:02:42.433896 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 01:02:42.434106 (Thread-1): 18:02:42 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 01:02:42.434677 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:02:42.434943 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 01:02:42.435141 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 01:02:42.444309 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 01:02:42.444780 (Thread-1): finished collecting timing info
2020-04-29 01:02:42.452353 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:02:42.452488 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 01:02:42.647074 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:02:42.651136 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:02:42.651293 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 01:02:42.836442 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:02:42.839654 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 01:02:42.840249 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:02:42.840409 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 01:02:42.896676 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-04-29 01:02:42.896868 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:02:42.896976 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
final as (
    select 
    order_flash.*,
    SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,

    SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event

    FROM order_flash INNER JOIN events USING (event_unique_id)
)

select * from final
  );

2020-04-29 01:02:42.938991 (Thread-1): Postgres error: column "event_unique_id" specified in USING clause does not exist in left table

2020-04-29 01:02:42.939432 (Thread-1): On model.order_history.order_flash_event: ROLLBACK
2020-04-29 01:02:42.977801 (Thread-1): finished collecting timing info
2020-04-29 01:02:42.978758 (Thread-1): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  column "event_unique_id" specified in USING clause does not exist in left table
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column "event_unique_id" specified in USING clause does not exist in left table


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  column "event_unique_id" specified in USING clause does not exist in left table
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 01:02:42.981612 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ad24dd5-ee0a-4e22-9d4f-397c2a201c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108127390>]}
2020-04-29 01:02:42.981905 (Thread-1): 18:02:42 | 7 of 8 ERROR creating view model data_science.order_flash_event...... [ERROR in 0.55s]
2020-04-29 01:02:42.982084 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 01:02:42.982271 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:02:42.982462 (Thread-1): 18:02:42 | 8 of 8 START view model data_science.customers....................... [RUN]
2020-04-29 01:02:42.982983 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:02:42.983135 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash_event).
2020-04-29 01:02:42.983277 (Thread-1): Compiling model.order_history.customers
2020-04-29 01:02:42.992608 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 01:02:42.993030 (Thread-1): finished collecting timing info
2020-04-29 01:02:43.000435 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:02:43.000569 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 01:02:43.078552 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:02:43.084076 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:02:43.084231 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 01:02:43.122406 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:02:43.125518 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 01:02:43.126150 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:02:43.126309 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 01:02:43.163665 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:02:43.164102 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:02:43.164376 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 01:02:43.221765 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:02:43.226231 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:02:43.226393 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 01:02:43.268096 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:02:43.270065 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 01:02:43.270267 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:02:43.270430 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 01:02:43.438291 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:02:43.441658 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:02:43.441812 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 01:02:43.739016 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-29 01:02:43.743054 (Thread-1): finished collecting timing info
2020-04-29 01:02:43.743899 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ad24dd5-ee0a-4e22-9d4f-397c2a201c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10812de10>]}
2020-04-29 01:02:43.744207 (Thread-1): 18:02:43 | 8 of 8 OK created view model data_science.customers.................. [CREATE VIEW in 0.76s]
2020-04-29 01:02:43.744388 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:02:43.843906 (MainThread): Using postgres connection "master".
2020-04-29 01:02:43.844229 (MainThread): On master: BEGIN
2020-04-29 01:02:44.035213 (MainThread): SQL status: BEGIN in 0.19 seconds
2020-04-29 01:02:44.035682 (MainThread): On master: COMMIT
2020-04-29 01:02:44.035978 (MainThread): Using postgres connection "master".
2020-04-29 01:02:44.036137 (MainThread): On master: COMMIT
2020-04-29 01:02:44.076528 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:02:44.077378 (MainThread): 18:02:44 | 
2020-04-29 01:02:44.077625 (MainThread): 18:02:44 | Finished running 8 view models in 13.80s.
2020-04-29 01:02:44.077824 (MainThread): Connection 'master' was left open.
2020-04-29 01:02:44.077984 (MainThread): On master: Close
2020-04-29 01:02:44.078381 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 01:02:44.078551 (MainThread): On model.order_history.customers: Close
2020-04-29 01:02:44.103817 (MainThread): 
2020-04-29 01:02:44.104049 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 01:02:44.104197 (MainThread): 
2020-04-29 01:02:44.104333 (MainThread): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
2020-04-29 01:02:44.104455 (MainThread):   column "event_unique_id" specified in USING clause does not exist in left table
2020-04-29 01:02:44.104568 (MainThread):   compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 01:02:44.104694 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 01:02:44.104891 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080f0d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fec410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108255890>]}
2020-04-29 01:02:44.105104 (MainThread): Flushing usage events
2020-04-29 01:10:12.520162 (MainThread): Running with dbt=0.16.1
2020-04-29 01:10:12.594083 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:10:12.595025 (MainThread): Tracking: tracking
2020-04-29 01:10:12.600388 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109581910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109806d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109806e50>]}
2020-04-29 01:10:12.626183 (MainThread): Partial parsing not enabled
2020-04-29 01:10:12.630755 (MainThread): Parsing macros/core.sql
2020-04-29 01:10:12.643392 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:10:12.660244 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:10:12.664736 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:10:12.704010 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:10:12.756357 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:10:12.786167 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:10:12.788744 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:10:12.797019 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:10:12.812674 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:10:12.821150 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:10:12.828539 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:10:12.833707 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:10:12.834701 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:10:12.836385 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:10:12.838397 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:10:12.840750 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:10:12.850545 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:10:12.852674 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:10:12.853795 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:10:12.898165 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:10:12.899438 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:10:12.900420 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:10:12.901593 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:10:12.903992 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:10:12.906575 (MainThread): Parsing macros/relations.sql
2020-04-29 01:10:12.907964 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:10:12.926247 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:10:12.946223 (MainThread): Partial parsing not enabled
2020-04-29 01:10:12.976675 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:10:12.976816 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:12.995959 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:12.996111 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:13.001252 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:13.001381 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:13.007241 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:10:13.007400 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:13.012582 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:10:13.012725 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:13.020130 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:13.020304 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:13.028967 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:10:13.029124 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:13.036480 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:10:13.036611 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:13.183160 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:10:13.186840 (MainThread): 
2020-04-29 01:10:13.187140 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:10:13.187232 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:10:13.213376 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:10:13.213653 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:10:13.312111 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:10:13.312285 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:10:13.846286 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-29 01:10:13.884898 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:10:13.885123 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:10:13.887179 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:10:13.887295 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:10:13.929574 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:10:13.929889 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:10:13.930057 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:10:14.159802 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.23 seconds
2020-04-29 01:10:14.168973 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:10:14.249210 (MainThread): Using postgres connection "master".
2020-04-29 01:10:14.249358 (MainThread): On master: BEGIN
2020-04-29 01:10:14.595320 (MainThread): SQL status: BEGIN in 0.35 seconds
2020-04-29 01:10:14.595780 (MainThread): Using postgres connection "master".
2020-04-29 01:10:14.595950 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:10:14.839366 (MainThread): SQL status: SELECT in 0.24 seconds
2020-04-29 01:10:14.920267 (MainThread): On master: ROLLBACK
2020-04-29 01:10:14.958999 (MainThread): Using postgres connection "master".
2020-04-29 01:10:14.959403 (MainThread): On master: BEGIN
2020-04-29 01:10:15.036547 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 01:10:15.036778 (MainThread): On master: COMMIT
2020-04-29 01:10:15.036903 (MainThread): Using postgres connection "master".
2020-04-29 01:10:15.037010 (MainThread): On master: COMMIT
2020-04-29 01:10:15.075095 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:10:15.075778 (MainThread): 18:10:15 | Concurrency: 1 threads (target='dev')
2020-04-29 01:10:15.076054 (MainThread): 18:10:15 | 
2020-04-29 01:10:15.078528 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 01:10:15.078912 (Thread-1): 18:10:15 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 01:10:15.079471 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.079662 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:10:15.079865 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 01:10:15.099099 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 01:10:15.099664 (Thread-1): finished collecting timing info
2020-04-29 01:10:15.139711 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.139880 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 01:10:15.224950 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:10:15.229203 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.229369 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:10:15.272465 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:10:15.275562 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 01:10:15.276184 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.276342 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 01:10:15.318439 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:10:15.318740 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.318916 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 01:10:15.400274 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-29 01:10:15.405047 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.405215 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 01:10:15.448376 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:10:15.451097 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.451246 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 01:10:15.498604 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:10:15.500555 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:10:15.500729 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.500862 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:10:15.688376 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-29 01:10:15.691543 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:10:15.691772 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:10:15.954319 (Thread-1): SQL status: DROP VIEW in 0.26 seconds
2020-04-29 01:10:15.958652 (Thread-1): finished collecting timing info
2020-04-29 01:10:15.959517 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a3a675a-9b1f-4f95-8cea-42e263397931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b9f390>]}
2020-04-29 01:10:15.959837 (Thread-1): 18:10:15 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.88s]
2020-04-29 01:10:15.960031 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 01:10:15.960221 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 01:10:15.960525 (Thread-1): 18:10:15 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 01:10:15.960893 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:10:15.961024 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 01:10:15.961158 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 01:10:15.967447 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 01:10:15.967885 (Thread-1): finished collecting timing info
2020-04-29 01:10:15.975554 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:10:15.975699 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 01:10:16.158807 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:10:16.161615 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:10:16.161748 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:10:16.336051 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:10:16.340489 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 01:10:16.341150 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:10:16.341333 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 01:10:16.383760 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:10:16.384052 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:10:16.384222 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticket_state,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 01:10:16.429805 (Thread-1): Postgres error: column "ticket_state" does not exist in order_tickets, price_codes, unnamed_join, zones, unnamed_join

2020-04-29 01:10:16.430017 (Thread-1): On model.order_history.stg_order: ROLLBACK
2020-04-29 01:10:16.471288 (Thread-1): finished collecting timing info
2020-04-29 01:10:16.471842 (Thread-1): Database Error in model stg_order (models/staging/stg_order.sql)
  column "ticket_state" does not exist in order_tickets, price_codes, unnamed_join, zones, unnamed_join
  compiled SQL at target/run/order_history/staging/stg_order.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column "ticket_state" does not exist in order_tickets, price_codes, unnamed_join, zones, unnamed_join


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model stg_order (models/staging/stg_order.sql)
  column "ticket_state" does not exist in order_tickets, price_codes, unnamed_join, zones, unnamed_join
  compiled SQL at target/run/order_history/staging/stg_order.sql
2020-04-29 01:10:16.473906 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a3a675a-9b1f-4f95-8cea-42e263397931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099d5410>]}
2020-04-29 01:10:16.474135 (Thread-1): 18:10:16 | 2 of 8 ERROR creating view model data_science.stg_order.............. [ERROR in 0.51s]
2020-04-29 01:10:16.474276 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 01:10:16.474421 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 01:10:16.474728 (Thread-1): 18:10:16 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 01:10:16.475165 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:16.475281 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 01:10:16.475385 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 01:10:16.512713 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 01:10:16.513235 (Thread-1): finished collecting timing info
2020-04-29 01:10:16.521358 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:16.521544 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 01:10:16.606726 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-29 01:10:16.610407 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:16.610641 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:10:16.652838 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:10:16.655082 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 01:10:16.655723 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:16.655863 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 01:10:16.697090 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:10:16.697329 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:16.697450 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 01:10:16.773541 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-29 01:10:16.778322 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:16.778493 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 01:10:16.821270 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:10:16.824269 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:16.824422 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 01:10:16.870765 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:10:16.872695 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:10:16.872893 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:16.873055 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:10:17.047102 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:10:17.050503 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:10:17.050659 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:10:17.242137 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:10:17.246329 (Thread-1): finished collecting timing info
2020-04-29 01:10:17.247181 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a3a675a-9b1f-4f95-8cea-42e263397931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e116d0>]}
2020-04-29 01:10:17.247484 (Thread-1): 18:10:17 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.77s]
2020-04-29 01:10:17.247666 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 01:10:17.247901 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 01:10:17.248302 (Thread-1): 18:10:17 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 01:10:17.248863 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:10:17.249022 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 01:10:17.249147 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 01:10:17.255324 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 01:10:17.255756 (Thread-1): finished collecting timing info
2020-04-29 01:10:17.263930 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:10:17.264061 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 01:10:17.450083 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:10:17.454217 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:10:17.454370 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:10:17.626665 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:10:17.629054 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 01:10:17.629618 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:10:17.629774 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 01:10:17.672694 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:10:17.673131 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:10:17.673412 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime

FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 01:10:17.744157 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-29 01:10:17.750324 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:10:17.750481 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 01:10:17.796199 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:10:17.799253 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:10:17.799395 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 01:10:17.842822 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:10:17.844438 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:10:17.844603 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:10:17.844723 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:10:18.031361 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-29 01:10:18.034457 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:10:18.034619 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:10:18.222176 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:10:18.226429 (Thread-1): finished collecting timing info
2020-04-29 01:10:18.227163 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a3a675a-9b1f-4f95-8cea-42e263397931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e2b290>]}
2020-04-29 01:10:18.227411 (Thread-1): 18:10:18 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 0.98s]
2020-04-29 01:10:18.227557 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 01:10:18.227706 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 01:10:18.227945 (Thread-1): 18:10:18 | 5 of 8 SKIP relation data_science.order_flash........................ [SKIP]
2020-04-29 01:10:18.228163 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 01:10:18.228446 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 01:10:18.228719 (Thread-1): 18:10:18 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 01:10:18.229117 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:18.229234 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 01:10:18.229343 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 01:10:18.236360 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 01:10:18.236755 (Thread-1): finished collecting timing info
2020-04-29 01:10:18.243247 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:18.243435 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 01:10:18.494637 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-29 01:10:18.498997 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:18.499147 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:10:18.705015 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 01:10:18.707887 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 01:10:18.708552 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:18.708710 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 01:10:18.751121 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:10:18.751536 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:18.751799 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 01:10:18.831073 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-29 01:10:18.834532 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:18.834699 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 01:10:18.881976 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:10:18.883115 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:10:18.883239 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:18.883355 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:10:19.055592 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:10:19.059072 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:10:19.059236 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:10:19.237710 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:10:19.240267 (Thread-1): finished collecting timing info
2020-04-29 01:10:19.240905 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a3a675a-9b1f-4f95-8cea-42e263397931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097cedd0>]}
2020-04-29 01:10:19.241135 (Thread-1): 18:10:19 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 1.01s]
2020-04-29 01:10:19.241272 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 01:10:19.241416 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 01:10:19.241658 (Thread-1): 18:10:19 | 7 of 8 SKIP relation data_science.order_flash_event.................. [SKIP]
2020-04-29 01:10:19.241940 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 01:10:19.242103 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:10:19.242286 (Thread-1): 18:10:19 | 8 of 8 SKIP relation data_science.customers.......................... [SKIP]
2020-04-29 01:10:19.242411 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:10:19.272353 (MainThread): Using postgres connection "master".
2020-04-29 01:10:19.272577 (MainThread): On master: BEGIN
2020-04-29 01:10:19.312491 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:10:19.312908 (MainThread): On master: COMMIT
2020-04-29 01:10:19.313088 (MainThread): Using postgres connection "master".
2020-04-29 01:10:19.313243 (MainThread): On master: COMMIT
2020-04-29 01:10:19.351316 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:10:19.351968 (MainThread): 18:10:19 | 
2020-04-29 01:10:19.352167 (MainThread): 18:10:19 | Finished running 8 view models in 6.16s.
2020-04-29 01:10:19.352320 (MainThread): Connection 'master' was left open.
2020-04-29 01:10:19.352442 (MainThread): On master: Close
2020-04-29 01:10:19.352770 (MainThread): Connection 'model.order_history.customer_broker' was left open.
2020-04-29 01:10:19.352899 (MainThread): On model.order_history.customer_broker: Close
2020-04-29 01:10:19.376311 (MainThread): 
2020-04-29 01:10:19.376538 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 01:10:19.376689 (MainThread): 
2020-04-29 01:10:19.376824 (MainThread): Database Error in model stg_order (models/staging/stg_order.sql)
2020-04-29 01:10:19.376947 (MainThread):   column "ticket_state" does not exist in order_tickets, price_codes, unnamed_join, zones, unnamed_join
2020-04-29 01:10:19.377061 (MainThread):   compiled SQL at target/run/order_history/staging/stg_order.sql
2020-04-29 01:10:19.377185 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 01:10:19.377380 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a7c650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e11f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098afed0>]}
2020-04-29 01:10:19.377596 (MainThread): Flushing usage events
2020-04-29 01:12:12.099707 (MainThread): Running with dbt=0.16.1
2020-04-29 01:12:12.169573 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:12:12.170298 (MainThread): Tracking: tracking
2020-04-29 01:12:12.175165 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be65ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0e6d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0e6e10>]}
2020-04-29 01:12:12.194820 (MainThread): Partial parsing not enabled
2020-04-29 01:12:12.196626 (MainThread): Parsing macros/core.sql
2020-04-29 01:12:12.201318 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:12:12.209275 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:12:12.211034 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:12:12.228915 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:12:12.262138 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:12:12.283909 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:12:12.286218 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:12:12.292687 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:12:12.305744 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:12:12.312559 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:12:12.318964 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:12:12.323940 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:12:12.324907 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:12:12.326004 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:12:12.327698 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:12:12.329790 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:12:12.338879 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:12:12.341093 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:12:12.342170 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:12:12.384789 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:12:12.386001 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:12:12.386963 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:12:12.388122 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:12:12.390461 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:12:12.392907 (MainThread): Parsing macros/relations.sql
2020-04-29 01:12:12.394314 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:12:12.412425 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:12:12.432982 (MainThread): Partial parsing not enabled
2020-04-29 01:12:12.465702 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:12:12.465831 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.481911 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:12.482005 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.485914 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:12.485999 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.490234 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:12:12.490320 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.494216 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:12:12.494302 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.498682 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:12.498771 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.503695 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:12:12.503781 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.509695 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:12:12.509784 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.648607 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:12:12.652792 (MainThread): 
2020-04-29 01:12:12.653091 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:12:12.653182 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:12:12.677226 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:12:12.677371 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:12:12.771575 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:12:12.771710 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:12:13.300052 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-29 01:12:13.339589 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:12:13.339800 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:12:13.341866 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:12:13.341986 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:12:13.381364 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:12:13.381563 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:12:13.381675 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:12:13.474922 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-29 01:12:13.482208 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:12:13.556976 (MainThread): Using postgres connection "master".
2020-04-29 01:12:13.557128 (MainThread): On master: BEGIN
2020-04-29 01:12:13.947811 (MainThread): SQL status: BEGIN in 0.39 seconds
2020-04-29 01:12:13.948229 (MainThread): Using postgres connection "master".
2020-04-29 01:12:13.948497 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:12:14.141270 (MainThread): SQL status: SELECT in 0.19 seconds
2020-04-29 01:12:14.214638 (MainThread): On master: ROLLBACK
2020-04-29 01:12:14.256506 (MainThread): Using postgres connection "master".
2020-04-29 01:12:14.256915 (MainThread): On master: BEGIN
2020-04-29 01:12:14.341043 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 01:12:14.341476 (MainThread): On master: COMMIT
2020-04-29 01:12:14.341671 (MainThread): Using postgres connection "master".
2020-04-29 01:12:14.341828 (MainThread): On master: COMMIT
2020-04-29 01:12:14.382452 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:12:14.382969 (MainThread): 18:12:14 | Concurrency: 1 threads (target='dev')
2020-04-29 01:12:14.383164 (MainThread): 18:12:14 | 
2020-04-29 01:12:14.385551 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 01:12:14.386029 (Thread-1): 18:12:14 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 01:12:14.386397 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.386522 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:12:14.386652 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 01:12:14.402849 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 01:12:14.403382 (Thread-1): finished collecting timing info
2020-04-29 01:12:14.449095 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.449251 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 01:12:14.531523 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:12:14.535925 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.536078 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:12:14.577403 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:12:14.580463 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 01:12:14.581070 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.581231 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 01:12:14.621835 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:12:14.622263 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.622536 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 01:12:14.679778 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:12:14.686097 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.686263 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 01:12:14.726666 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:12:14.729771 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.729909 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 01:12:14.772742 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:12:14.773830 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:12:14.773955 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.774052 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:12:14.948443 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:12:14.951846 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:12:14.952011 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:12:15.510390 (Thread-1): SQL status: DROP VIEW in 0.56 seconds
2020-04-29 01:12:15.514682 (Thread-1): finished collecting timing info
2020-04-29 01:12:15.515530 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c31d34db-f6b6-4844-9a7e-be1ac63f5777', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c36f350>]}
2020-04-29 01:12:15.515840 (Thread-1): 18:12:15 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 1.13s]
2020-04-29 01:12:15.516024 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 01:12:15.516258 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 01:12:15.516703 (Thread-1): 18:12:15 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 01:12:15.517187 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:12:15.517353 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 01:12:15.517588 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 01:12:15.523804 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 01:12:15.524299 (Thread-1): finished collecting timing info
2020-04-29 01:12:15.531825 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:12:15.531960 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 01:12:15.732536 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 01:12:15.736738 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:12:15.736891 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:12:15.910263 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:12:15.913408 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 01:12:15.914025 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:12:15.914187 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 01:12:15.954296 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:12:15.954586 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:12:15.954756 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 01:12:16.013912 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:12:16.021205 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:12:16.021340 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 01:12:16.061743 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:12:16.098241 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:12:16.098452 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 01:12:16.143101 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:12:16.144923 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:12:16.145117 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:12:16.145286 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:12:16.318961 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:12:16.322339 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:12:16.322491 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:12:16.506571 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:12:16.509117 (Thread-1): finished collecting timing info
2020-04-29 01:12:16.509748 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c31d34db-f6b6-4844-9a7e-be1ac63f5777', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c6e3e90>]}
2020-04-29 01:12:16.509979 (Thread-1): 18:12:16 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.99s]
2020-04-29 01:12:16.510113 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 01:12:16.510254 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 01:12:16.510501 (Thread-1): 18:12:16 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 01:12:16.511009 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:16.511113 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 01:12:16.511214 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 01:12:16.516517 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 01:12:16.516892 (Thread-1): finished collecting timing info
2020-04-29 01:12:16.523329 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:16.523451 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 01:12:16.692524 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:12:16.696121 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:16.696293 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:12:16.913369 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:12:16.916203 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 01:12:16.916872 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:16.917024 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 01:12:16.957362 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:12:16.957790 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:16.958058 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 01:12:17.010836 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:12:17.016912 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:17.017065 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 01:12:17.060559 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:12:17.064809 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:17.064962 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 01:12:17.105565 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:12:17.107541 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:12:17.107737 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:17.107895 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:12:17.288734 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:12:17.293300 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:12:17.293456 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:12:17.497327 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 01:12:17.501440 (Thread-1): finished collecting timing info
2020-04-29 01:12:17.502431 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c31d34db-f6b6-4844-9a7e-be1ac63f5777', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2a8350>]}
2020-04-29 01:12:17.502787 (Thread-1): 18:12:17 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.99s]
2020-04-29 01:12:17.503034 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 01:12:17.503273 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 01:12:17.503667 (Thread-1): 18:12:17 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 01:12:17.504292 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:12:17.504483 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 01:12:17.504660 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 01:12:17.512392 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 01:12:17.512873 (Thread-1): finished collecting timing info
2020-04-29 01:12:17.519637 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:12:17.519768 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 01:12:17.736784 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:12:17.740748 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:12:17.740901 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:12:17.933110 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:12:17.936131 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 01:12:17.936756 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:12:17.936915 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 01:12:17.978841 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:12:17.979268 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:12:17.979551 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 01:12:18.033195 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:12:18.039360 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:12:18.039520 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 01:12:18.082991 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:12:18.087229 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:12:18.087382 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 01:12:18.153317 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2020-04-29 01:12:18.154677 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:12:18.154821 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:12:18.154938 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:12:18.337415 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:12:18.340978 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:12:18.341131 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:12:18.536238 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:12:18.540351 (Thread-1): finished collecting timing info
2020-04-29 01:12:18.541399 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c31d34db-f6b6-4844-9a7e-be1ac63f5777', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2a8350>]}
2020-04-29 01:12:18.541788 (Thread-1): 18:12:18 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 1.04s]
2020-04-29 01:12:18.541995 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 01:12:18.542319 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 01:12:18.542672 (Thread-1): 18:12:18 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 01:12:18.543073 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:12:18.543239 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 01:12:18.543422 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 01:12:18.552998 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 01:12:18.553427 (Thread-1): finished collecting timing info
2020-04-29 01:12:18.559946 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:12:18.560076 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 01:12:18.734088 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:12:18.737558 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:12:18.737717 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:12:18.907361 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:12:18.909038 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 01:12:18.909457 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:12:18.909561 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 01:12:18.949422 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:12:18.949849 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:12:18.950134 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 01:12:18.991977 (Thread-1): Postgres error: column "event_unique_id" does not exist in orders, flash, unnamed_join

2020-04-29 01:12:18.992396 (Thread-1): On model.order_history.order_flash: ROLLBACK
2020-04-29 01:12:19.032511 (Thread-1): finished collecting timing info
2020-04-29 01:12:19.033318 (Thread-1): Database Error in model order_flash (models/intermediate/order_flash.sql)
  column "event_unique_id" does not exist in orders, flash, unnamed_join
  compiled SQL at target/run/order_history/intermediate/order_flash.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column "event_unique_id" does not exist in orders, flash, unnamed_join


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash (models/intermediate/order_flash.sql)
  column "event_unique_id" does not exist in orders, flash, unnamed_join
  compiled SQL at target/run/order_history/intermediate/order_flash.sql
2020-04-29 01:12:19.036062 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c31d34db-f6b6-4844-9a7e-be1ac63f5777', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c38af10>]}
2020-04-29 01:12:19.036354 (Thread-1): 18:12:19 | 5 of 8 ERROR creating view model data_science.order_flash............ [ERROR in 0.49s]
2020-04-29 01:12:19.036535 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 01:12:19.036723 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 01:12:19.036904 (Thread-1): 18:12:19 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 01:12:19.037245 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:19.037375 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 01:12:19.037506 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 01:12:19.045527 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 01:12:19.046093 (Thread-1): finished collecting timing info
2020-04-29 01:12:19.053570 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:19.053706 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 01:12:19.133734 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:12:19.137898 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:19.138051 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:12:19.178379 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:12:19.181331 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 01:12:19.181904 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:19.182059 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 01:12:19.221254 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:12:19.221581 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:19.221732 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 01:12:19.275699 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:12:19.279833 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:19.279988 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 01:12:19.320728 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:12:19.322666 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:12:19.322862 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:19.323022 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:12:19.494386 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:12:19.497772 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:12:19.497925 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:12:19.690936 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:12:19.695181 (Thread-1): finished collecting timing info
2020-04-29 01:12:19.696020 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c31d34db-f6b6-4844-9a7e-be1ac63f5777', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c13d390>]}
2020-04-29 01:12:19.696331 (Thread-1): 18:12:19 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.66s]
2020-04-29 01:12:19.696513 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 01:12:19.696758 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 01:12:19.697229 (Thread-1): 18:12:19 | 7 of 8 SKIP relation data_science.order_flash_event.................. [SKIP]
2020-04-29 01:12:19.697727 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 01:12:19.698023 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:12:19.698318 (Thread-1): 18:12:19 | 8 of 8 SKIP relation data_science.customers.......................... [SKIP]
2020-04-29 01:12:19.698485 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:12:19.723833 (MainThread): Using postgres connection "master".
2020-04-29 01:12:19.724091 (MainThread): On master: BEGIN
2020-04-29 01:12:19.764796 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:12:19.765286 (MainThread): On master: COMMIT
2020-04-29 01:12:19.765596 (MainThread): Using postgres connection "master".
2020-04-29 01:12:19.765752 (MainThread): On master: COMMIT
2020-04-29 01:12:19.811602 (MainThread): SQL status: COMMIT in 0.05 seconds
2020-04-29 01:12:19.812098 (MainThread): 18:12:19 | 
2020-04-29 01:12:19.812257 (MainThread): 18:12:19 | Finished running 8 view models in 7.16s.
2020-04-29 01:12:19.812389 (MainThread): Connection 'master' was left open.
2020-04-29 01:12:19.812493 (MainThread): On master: Close
2020-04-29 01:12:19.812773 (MainThread): Connection 'model.order_history.customer_broker' was left open.
2020-04-29 01:12:19.812883 (MainThread): On model.order_history.customer_broker: Close
2020-04-29 01:12:19.837553 (MainThread): 
2020-04-29 01:12:19.837795 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 01:12:19.837943 (MainThread): 
2020-04-29 01:12:19.838078 (MainThread): Database Error in model order_flash (models/intermediate/order_flash.sql)
2020-04-29 01:12:19.838200 (MainThread):   column "event_unique_id" does not exist in orders, flash, unnamed_join
2020-04-29 01:12:19.838311 (MainThread):   compiled SQL at target/run/order_history/intermediate/order_flash.sql
2020-04-29 01:12:19.838435 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 01:12:19.838640 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c379d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c3905d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c390b10>]}
2020-04-29 01:12:19.838861 (MainThread): Flushing usage events
2020-04-29 01:13:04.123705 (MainThread): Running with dbt=0.16.1
2020-04-29 01:13:04.188623 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:13:04.189316 (MainThread): Tracking: tracking
2020-04-29 01:13:04.194365 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061f5050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10644ef10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061f5850>]}
2020-04-29 01:13:04.215043 (MainThread): Partial parsing not enabled
2020-04-29 01:13:04.217038 (MainThread): Parsing macros/core.sql
2020-04-29 01:13:04.221572 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:13:04.229605 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:13:04.231516 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:13:04.249424 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:13:04.282615 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:13:04.303842 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:13:04.305739 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:13:04.312071 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:13:04.324646 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:13:04.331528 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:13:04.337816 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:13:04.343052 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:13:04.344007 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:13:04.345084 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:13:04.346752 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:13:04.349119 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:13:04.358382 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:13:04.360400 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:13:04.361593 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:13:04.411339 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:13:04.412594 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:13:04.413562 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:13:04.414712 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:13:04.417044 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:13:04.419465 (MainThread): Parsing macros/relations.sql
2020-04-29 01:13:04.420902 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:13:04.438057 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:13:04.456342 (MainThread): Partial parsing not enabled
2020-04-29 01:13:04.483964 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:13:04.484074 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.500031 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:04.500130 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.504161 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:04.504265 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.508600 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:13:04.508686 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.512439 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:13:04.512524 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.517618 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:04.517719 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.522471 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:13:04.522559 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.528106 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:13:04.528192 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.672981 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:13:04.677278 (MainThread): 
2020-04-29 01:13:04.677595 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:13:04.677691 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:04.707701 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:13:04.708004 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:13:04.791703 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:13:04.791847 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:13:05.242139 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.45 seconds
2020-04-29 01:13:05.281434 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:13:05.281626 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:13:05.283601 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:13:05.283718 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:13:05.322326 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:05.322832 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:13:05.323028 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:13:05.443480 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-29 01:13:05.450813 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:13:05.525223 (MainThread): Using postgres connection "master".
2020-04-29 01:13:05.525378 (MainThread): On master: BEGIN
2020-04-29 01:13:05.942960 (MainThread): SQL status: BEGIN in 0.42 seconds
2020-04-29 01:13:05.943384 (MainThread): Using postgres connection "master".
2020-04-29 01:13:05.943654 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:13:06.079186 (MainThread): SQL status: SELECT in 0.14 seconds
2020-04-29 01:13:06.150675 (MainThread): On master: ROLLBACK
2020-04-29 01:13:06.191813 (MainThread): Using postgres connection "master".
2020-04-29 01:13:06.192227 (MainThread): On master: BEGIN
2020-04-29 01:13:06.305263 (MainThread): SQL status: BEGIN in 0.11 seconds
2020-04-29 01:13:06.305727 (MainThread): On master: COMMIT
2020-04-29 01:13:06.306008 (MainThread): Using postgres connection "master".
2020-04-29 01:13:06.306211 (MainThread): On master: COMMIT
2020-04-29 01:13:06.346202 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:13:06.346658 (MainThread): 18:13:06 | Concurrency: 1 threads (target='dev')
2020-04-29 01:13:06.346832 (MainThread): 18:13:06 | 
2020-04-29 01:13:06.348656 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 01:13:06.349069 (Thread-1): 18:13:06 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 01:13:06.349481 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.349614 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:13:06.349745 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 01:13:06.365717 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 01:13:06.366356 (Thread-1): finished collecting timing info
2020-04-29 01:13:06.412390 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.412550 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 01:13:06.525948 (Thread-1): SQL status: DROP VIEW in 0.11 seconds
2020-04-29 01:13:06.530285 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.530438 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:13:06.569574 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:13:06.572709 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 01:13:06.573360 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.573521 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 01:13:06.611735 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:06.612199 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.612392 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 01:13:06.662053 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:13:06.668270 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.668432 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 01:13:06.707521 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:13:06.710739 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.710929 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 01:13:06.751130 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:13:06.752224 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:13:06.752348 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.752443 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:13:06.937601 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:13:06.941066 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:06.941228 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:13:07.115907 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:13:07.120092 (Thread-1): finished collecting timing info
2020-04-29 01:13:07.120961 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bac692c-b7c4-456c-bd3f-fb2efa5313d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064ee110>]}
2020-04-29 01:13:07.121285 (Thread-1): 18:13:07 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.77s]
2020-04-29 01:13:07.121476 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 01:13:07.121664 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 01:13:07.121849 (Thread-1): 18:13:07 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 01:13:07.122375 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:13:07.122517 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 01:13:07.122630 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 01:13:07.128587 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 01:13:07.129020 (Thread-1): finished collecting timing info
2020-04-29 01:13:07.136258 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:13:07.136384 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 01:13:07.348258 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 01:13:07.351310 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:13:07.351452 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:13:07.528238 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:13:07.531223 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 01:13:07.531775 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:13:07.531938 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 01:13:07.570861 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:07.571298 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:13:07.571568 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 01:13:07.613160 (Thread-1): Postgres error: column reference "event_unique_id" is ambiguous

2020-04-29 01:13:07.613572 (Thread-1): On model.order_history.stg_order: ROLLBACK
2020-04-29 01:13:07.652482 (Thread-1): finished collecting timing info
2020-04-29 01:13:07.653478 (Thread-1): Database Error in model stg_order (models/staging/stg_order.sql)
  column reference "event_unique_id" is ambiguous
  compiled SQL at target/run/order_history/staging/stg_order.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.AmbiguousColumn: column reference "event_unique_id" is ambiguous


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model stg_order (models/staging/stg_order.sql)
  column reference "event_unique_id" is ambiguous
  compiled SQL at target/run/order_history/staging/stg_order.sql
2020-04-29 01:13:07.656629 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bac692c-b7c4-456c-bd3f-fb2efa5313d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10680e3d0>]}
2020-04-29 01:13:07.656928 (Thread-1): 18:13:07 | 2 of 8 ERROR creating view model data_science.stg_order.............. [ERROR in 0.53s]
2020-04-29 01:13:07.657112 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 01:13:07.657349 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 01:13:07.657815 (Thread-1): 18:13:07 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 01:13:07.658394 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:07.658539 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 01:13:07.658677 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 01:13:07.666255 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 01:13:07.666682 (Thread-1): finished collecting timing info
2020-04-29 01:13:07.707842 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:07.708053 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 01:13:07.785150 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:13:07.788828 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:07.788985 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:13:07.827748 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:13:07.830798 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 01:13:07.831419 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:07.831568 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 01:13:07.869656 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:07.870087 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:07.870362 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 01:13:07.917466 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:13:07.923425 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:07.923580 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 01:13:07.962730 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:13:07.966935 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:07.967087 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 01:13:08.007476 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:13:08.009410 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:13:08.009621 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:08.009785 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:13:08.176187 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:13:08.179580 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:08.179731 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:13:08.380861 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 01:13:08.384957 (Thread-1): finished collecting timing info
2020-04-29 01:13:08.386118 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bac692c-b7c4-456c-bd3f-fb2efa5313d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a74c10>]}
2020-04-29 01:13:08.386507 (Thread-1): 18:13:08 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.73s]
2020-04-29 01:13:08.386693 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 01:13:08.386942 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 01:13:08.387337 (Thread-1): 18:13:08 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 01:13:08.387966 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:13:08.388196 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 01:13:08.388387 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 01:13:08.398041 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 01:13:08.398519 (Thread-1): finished collecting timing info
2020-04-29 01:13:08.406352 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:13:08.406494 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 01:13:08.582320 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:13:08.586485 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:13:08.586639 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:13:08.753142 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:13:08.756116 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 01:13:08.756662 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:13:08.756816 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 01:13:08.795730 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:08.796161 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:13:08.796448 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 01:13:08.843424 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:13:08.849595 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:13:08.849748 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 01:13:08.890554 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:13:08.894826 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:13:08.894983 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 01:13:08.935369 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:13:08.937301 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:13:08.937496 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:13:08.937658 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:13:09.109416 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:13:09.113023 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:13:09.113175 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:13:09.292677 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:13:09.296977 (Thread-1): finished collecting timing info
2020-04-29 01:13:09.297830 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bac692c-b7c4-456c-bd3f-fb2efa5313d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066f8350>]}
2020-04-29 01:13:09.298138 (Thread-1): 18:13:09 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 0.91s]
2020-04-29 01:13:09.298318 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 01:13:09.298502 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 01:13:09.298684 (Thread-1): 18:13:09 | 5 of 8 SKIP relation data_science.order_flash........................ [SKIP]
2020-04-29 01:13:09.298852 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 01:13:09.299147 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 01:13:09.299552 (Thread-1): 18:13:09 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 01:13:09.299908 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:09.300028 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 01:13:09.300147 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 01:13:09.307801 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 01:13:09.308222 (Thread-1): finished collecting timing info
2020-04-29 01:13:09.315781 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:09.315911 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 01:13:09.510940 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:13:09.515005 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:09.515158 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:13:09.684084 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:13:09.688344 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 01:13:09.689039 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:09.689211 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 01:13:09.727558 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:09.727839 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:09.728056 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 01:13:09.780261 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:13:09.784749 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:09.784904 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 01:13:09.826146 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:13:09.827547 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:13:09.827699 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:09.827829 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:13:09.998709 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:13:10.002054 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:10.002390 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:13:10.170418 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:13:10.174435 (Thread-1): finished collecting timing info
2020-04-29 01:13:10.175285 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bac692c-b7c4-456c-bd3f-fb2efa5313d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10661c690>]}
2020-04-29 01:13:10.175591 (Thread-1): 18:13:10 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.88s]
2020-04-29 01:13:10.175772 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 01:13:10.175956 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 01:13:10.176503 (Thread-1): 18:13:10 | 7 of 8 SKIP relation data_science.order_flash_event.................. [SKIP]
2020-04-29 01:13:10.176894 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 01:13:10.177168 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:13:10.177430 (Thread-1): 18:13:10 | 8 of 8 SKIP relation data_science.customers.......................... [SKIP]
2020-04-29 01:13:10.177596 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:13:10.268499 (MainThread): Using postgres connection "master".
2020-04-29 01:13:10.268710 (MainThread): On master: BEGIN
2020-04-29 01:13:10.309801 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:10.310272 (MainThread): On master: COMMIT
2020-04-29 01:13:10.310571 (MainThread): Using postgres connection "master".
2020-04-29 01:13:10.310734 (MainThread): On master: COMMIT
2020-04-29 01:13:10.351004 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:13:10.351533 (MainThread): 18:13:10 | 
2020-04-29 01:13:10.351706 (MainThread): 18:13:10 | Finished running 8 view models in 5.67s.
2020-04-29 01:13:10.351852 (MainThread): Connection 'master' was left open.
2020-04-29 01:13:10.351969 (MainThread): On master: Close
2020-04-29 01:13:10.352259 (MainThread): Connection 'model.order_history.customer_broker' was left open.
2020-04-29 01:13:10.352458 (MainThread): On model.order_history.customer_broker: Close
2020-04-29 01:13:10.379939 (MainThread): 
2020-04-29 01:13:10.380231 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 01:13:10.380452 (MainThread): 
2020-04-29 01:13:10.380672 (MainThread): Database Error in model stg_order (models/staging/stg_order.sql)
2020-04-29 01:13:10.380875 (MainThread):   column reference "event_unique_id" is ambiguous
2020-04-29 01:13:10.381066 (MainThread):   compiled SQL at target/run/order_history/staging/stg_order.sql
2020-04-29 01:13:10.381299 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 01:13:10.381614 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ad5c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066476d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064e3210>]}
2020-04-29 01:13:10.381896 (MainThread): Flushing usage events
2020-04-29 01:13:57.039852 (MainThread): Running with dbt=0.16.1
2020-04-29 01:13:57.111856 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:13:57.112680 (MainThread): Tracking: tracking
2020-04-29 01:13:57.117834 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e07b050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e057ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e07b850>]}
2020-04-29 01:13:57.140435 (MainThread): Partial parsing not enabled
2020-04-29 01:13:57.142490 (MainThread): Parsing macros/core.sql
2020-04-29 01:13:57.147318 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:13:57.155978 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:13:57.158085 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:13:57.177166 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:13:57.212411 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:13:57.234881 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:13:57.236924 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:13:57.243785 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:13:57.257243 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:13:57.264653 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:13:57.274214 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:13:57.280560 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:13:57.281716 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:13:57.283210 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:13:57.285050 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:13:57.287290 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:13:57.297022 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:13:57.299180 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:13:57.300326 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:13:57.344038 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:13:57.345313 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:13:57.346332 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:13:57.347517 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:13:57.349930 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:13:57.352496 (MainThread): Parsing macros/relations.sql
2020-04-29 01:13:57.353989 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:13:57.372042 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:13:57.391347 (MainThread): Partial parsing not enabled
2020-04-29 01:13:57.420228 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:13:57.420362 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.436898 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:13:57.437018 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.441193 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:57.441288 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.445739 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:13:57.445832 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.449983 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:13:57.450077 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.454778 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:13:57.454885 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.460621 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:13:57.460724 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.467331 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:13:57.467430 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.616526 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:13:57.619681 (MainThread): 
2020-04-29 01:13:57.619976 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:13:57.620068 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:13:57.644548 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:13:57.644689 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:13:57.730561 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:13:57.730697 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:13:58.256057 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-29 01:13:58.296944 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:13:58.297147 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:13:58.299144 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:13:58.299260 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:13:58.342536 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:58.342788 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:13:58.342933 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:13:58.465367 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-29 01:13:58.472681 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:13:58.554550 (MainThread): Using postgres connection "master".
2020-04-29 01:13:58.554718 (MainThread): On master: BEGIN
2020-04-29 01:13:58.929123 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-29 01:13:58.929332 (MainThread): Using postgres connection "master".
2020-04-29 01:13:58.929453 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:13:59.056755 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-29 01:13:59.136275 (MainThread): On master: ROLLBACK
2020-04-29 01:13:59.174627 (MainThread): Using postgres connection "master".
2020-04-29 01:13:59.175035 (MainThread): On master: BEGIN
2020-04-29 01:13:59.254786 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 01:13:59.255034 (MainThread): On master: COMMIT
2020-04-29 01:13:59.255177 (MainThread): Using postgres connection "master".
2020-04-29 01:13:59.255302 (MainThread): On master: COMMIT
2020-04-29 01:13:59.293218 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:13:59.293806 (MainThread): 18:13:59 | Concurrency: 1 threads (target='dev')
2020-04-29 01:13:59.294078 (MainThread): 18:13:59 | 
2020-04-29 01:13:59.296497 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 01:13:59.297083 (Thread-1): 18:13:59 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 01:13:59.297588 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.297787 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:13:59.298036 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 01:13:59.314497 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 01:13:59.314992 (Thread-1): finished collecting timing info
2020-04-29 01:13:59.352778 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.352942 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 01:13:59.438446 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-29 01:13:59.442809 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.442964 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:13:59.486066 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:13:59.489220 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 01:13:59.489829 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.489985 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 01:13:59.532229 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:13:59.532677 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.532867 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 01:13:59.596161 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:13:59.602601 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.602810 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 01:13:59.649097 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:13:59.651894 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.652023 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 01:13:59.694921 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:13:59.696850 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:13:59.697043 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.697206 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:13:59.870874 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:13:59.873880 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:13:59.874054 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:14:00.054549 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:14:00.058891 (Thread-1): finished collecting timing info
2020-04-29 01:14:00.059757 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd01c40b-bd5c-496e-865f-e78acebbda96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5d8f90>]}
2020-04-29 01:14:00.060078 (Thread-1): 18:14:00 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.76s]
2020-04-29 01:14:00.060264 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 01:14:00.060459 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 01:14:00.060651 (Thread-1): 18:14:00 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 01:14:00.061131 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.061279 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 01:14:00.061415 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 01:14:00.067783 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 01:14:00.068219 (Thread-1): finished collecting timing info
2020-04-29 01:14:00.075870 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.076005 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 01:14:00.250806 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:14:00.254104 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.254242 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:14:00.427396 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:14:00.430475 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 01:14:00.431097 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.431258 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 01:14:00.473981 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:14:00.474281 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.474460 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 01:14:00.530285 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:14:00.537727 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.537873 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 01:14:00.582709 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:14:00.616720 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.616919 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 01:14:00.659405 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:14:00.661191 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:14:00.661381 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.661555 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:14:00.837680 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:14:00.840667 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:14:00.840819 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:14:01.030124 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:14:01.032709 (Thread-1): finished collecting timing info
2020-04-29 01:14:01.033346 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd01c40b-bd5c-496e-865f-e78acebbda96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8d1fd0>]}
2020-04-29 01:14:01.033577 (Thread-1): 18:14:01 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.97s]
2020-04-29 01:14:01.033712 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 01:14:01.033847 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 01:14:01.034145 (Thread-1): 18:14:01 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 01:14:01.034762 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.034877 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 01:14:01.034978 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 01:14:01.040267 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 01:14:01.040655 (Thread-1): finished collecting timing info
2020-04-29 01:14:01.047156 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.047271 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 01:14:01.287447 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-29 01:14:01.291702 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.291862 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:14:01.505155 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 01:14:01.507912 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 01:14:01.508537 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.508697 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 01:14:01.581946 (Thread-1): SQL status: BEGIN in 0.07 seconds
2020-04-29 01:14:01.582362 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.582509 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 01:14:01.632851 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:14:01.639113 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.639273 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 01:14:01.683249 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:14:01.687413 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.687601 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 01:14:01.730527 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:14:01.732425 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:14:01.732629 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.732792 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:14:01.984072 (Thread-1): SQL status: COMMIT in 0.25 seconds
2020-04-29 01:14:01.988785 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:14:01.988966 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:14:02.170996 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:14:02.174753 (Thread-1): finished collecting timing info
2020-04-29 01:14:02.175692 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd01c40b-bd5c-496e-865f-e78acebbda96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5c4110>]}
2020-04-29 01:14:02.176023 (Thread-1): 18:14:02 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.14s]
2020-04-29 01:14:02.176215 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 01:14:02.176434 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 01:14:02.176639 (Thread-1): 18:14:02 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 01:14:02.177672 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:14:02.177878 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 01:14:02.178028 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 01:14:02.185747 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 01:14:02.186350 (Thread-1): finished collecting timing info
2020-04-29 01:14:02.195180 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:14:02.195372 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 01:14:02.452714 (Thread-1): SQL status: DROP VIEW in 0.26 seconds
2020-04-29 01:14:02.456862 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:14:02.457017 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:14:02.686093 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-29 01:14:02.688071 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 01:14:02.688575 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:14:02.688710 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 01:14:02.730993 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:14:02.731187 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:14:02.731296 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 01:14:02.783929 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:14:02.790080 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:14:02.790236 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 01:14:02.834748 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:14:02.838650 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:14:02.838808 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 01:14:02.883133 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:14:02.885057 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:14:02.885254 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:14:02.885415 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:14:03.109914 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-29 01:14:03.113478 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:14:03.113695 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:14:03.331357 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:14:03.335507 (Thread-1): finished collecting timing info
2020-04-29 01:14:03.336366 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd01c40b-bd5c-496e-865f-e78acebbda96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4bd4d0>]}
2020-04-29 01:14:03.336673 (Thread-1): 18:14:03 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 1.16s]
2020-04-29 01:14:03.336857 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 01:14:03.337044 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 01:14:03.337424 (Thread-1): 18:14:03 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 01:14:03.337993 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:14:03.338162 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 01:14:03.338289 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 01:14:03.348836 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 01:14:03.349285 (Thread-1): finished collecting timing info
2020-04-29 01:14:03.356815 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:14:03.356998 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 01:14:03.552994 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 01:14:03.557218 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:14:03.557373 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:14:03.773801 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:14:03.776068 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 01:14:03.776607 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:14:03.776769 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 01:14:03.819454 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:14:03.819878 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:14:03.820144 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 01:14:03.873005 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:14:03.877238 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:14:03.877394 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 01:14:03.920196 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:14:03.921919 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:14:03.922125 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:14:03.922290 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:14:04.142471 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-29 01:14:04.145790 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:14:04.145944 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:14:04.337811 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:14:04.341971 (Thread-1): finished collecting timing info
2020-04-29 01:14:04.342893 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd01c40b-bd5c-496e-865f-e78acebbda96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e327110>]}
2020-04-29 01:14:04.343206 (Thread-1): 18:14:04 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 1.00s]
2020-04-29 01:14:04.343387 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 01:14:04.343579 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 01:14:04.343968 (Thread-1): 18:14:04 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 01:14:04.344801 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:14:04.344964 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 01:14:04.345095 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 01:14:04.353109 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 01:14:04.353526 (Thread-1): finished collecting timing info
2020-04-29 01:14:04.361277 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:14:04.361409 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 01:14:04.589247 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-29 01:14:04.593514 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:14:04.593676 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:14:04.804326 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 01:14:04.807175 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 01:14:04.807849 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:14:04.808007 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 01:14:04.851049 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:14:04.851510 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:14:04.851800 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 01:14:04.905232 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:14:04.910887 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:14:04.911044 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 01:14:04.954002 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:14:04.955959 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:14:04.956159 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:14:04.956324 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:14:05.130699 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:14:05.132773 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:14:05.132891 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:14:05.315254 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:14:05.319437 (Thread-1): finished collecting timing info
2020-04-29 01:14:05.320306 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd01c40b-bd5c-496e-865f-e78acebbda96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5032d0>]}
2020-04-29 01:14:05.320617 (Thread-1): 18:14:05 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.98s]
2020-04-29 01:14:05.320803 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 01:14:05.320989 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 01:14:05.321292 (Thread-1): 18:14:05 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 01:14:05.321803 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:14:05.321915 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 01:14:05.322023 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 01:14:05.330192 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 01:14:05.330640 (Thread-1): finished collecting timing info
2020-04-29 01:14:05.337279 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:14:05.337400 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 01:14:05.553867 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:14:05.558058 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:14:05.558215 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 01:14:05.737078 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:14:05.740124 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 01:14:05.740788 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:14:05.740945 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 01:14:05.783044 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:14:05.783335 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:14:05.783529 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
final as (
    select 
    order_flash.*,
    SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,

    SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
    order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event

    FROM order_flash INNER JOIN events USING (event_unique_id)
)

select * from final
  );

2020-04-29 01:14:05.830589 (Thread-1): Postgres error: column "order_flash.order_ticket_unique_id" must appear in the GROUP BY clause or be used in an aggregate function

2020-04-29 01:14:05.831044 (Thread-1): On model.order_history.order_flash_event: ROLLBACK
2020-04-29 01:14:05.883020 (Thread-1): finished collecting timing info
2020-04-29 01:14:05.884081 (Thread-1): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  column "order_flash.order_ticket_unique_id" must appear in the GROUP BY clause or be used in an aggregate function
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.GroupingError: column "order_flash.order_ticket_unique_id" must appear in the GROUP BY clause or be used in an aggregate function


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  column "order_flash.order_ticket_unique_id" must appear in the GROUP BY clause or be used in an aggregate function
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 01:14:05.886989 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd01c40b-bd5c-496e-865f-e78acebbda96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4ad950>]}
2020-04-29 01:14:05.887286 (Thread-1): 18:14:05 | 7 of 8 ERROR creating view model data_science.order_flash_event...... [ERROR in 0.57s]
2020-04-29 01:14:05.887469 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 01:14:05.887653 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:14:05.888114 (Thread-1): 18:14:05 | 8 of 8 START view model data_science.customers....................... [RUN]
2020-04-29 01:14:05.888662 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:14:05.888856 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash_event).
2020-04-29 01:14:05.889061 (Thread-1): Compiling model.order_history.customers
2020-04-29 01:14:05.898619 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 01:14:05.899074 (Thread-1): finished collecting timing info
2020-04-29 01:14:05.906601 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:14:05.906742 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 01:14:05.991877 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:14:05.996034 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:14:05.996187 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 01:14:06.063989 (Thread-1): SQL status: DROP VIEW in 0.07 seconds
2020-04-29 01:14:06.068112 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 01:14:06.068676 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:14:06.068847 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 01:14:06.111980 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:14:06.112188 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:14:06.112309 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 01:14:06.171338 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:14:06.174917 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:14:06.175075 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 01:14:06.220500 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:14:06.222464 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 01:14:06.222666 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:14:06.222830 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 01:14:06.597108 (Thread-1): SQL status: COMMIT in 0.37 seconds
2020-04-29 01:14:06.600501 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:14:06.600656 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 01:14:06.775921 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:14:06.778942 (Thread-1): finished collecting timing info
2020-04-29 01:14:06.779707 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd01c40b-bd5c-496e-865f-e78acebbda96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e591610>]}
2020-04-29 01:14:06.779981 (Thread-1): 18:14:06 | 8 of 8 OK created view model data_science.customers.................. [CREATE VIEW in 0.89s]
2020-04-29 01:14:06.780147 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:14:06.800342 (MainThread): Using postgres connection "master".
2020-04-29 01:14:06.800606 (MainThread): On master: BEGIN
2020-04-29 01:14:06.840422 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:14:06.840848 (MainThread): On master: COMMIT
2020-04-29 01:14:06.841030 (MainThread): Using postgres connection "master".
2020-04-29 01:14:06.841189 (MainThread): On master: COMMIT
2020-04-29 01:14:06.879365 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:14:06.880294 (MainThread): 18:14:06 | 
2020-04-29 01:14:06.880538 (MainThread): 18:14:06 | Finished running 8 view models in 9.26s.
2020-04-29 01:14:06.880738 (MainThread): Connection 'master' was left open.
2020-04-29 01:14:06.880897 (MainThread): On master: Close
2020-04-29 01:14:06.881376 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 01:14:06.881614 (MainThread): On model.order_history.customers: Close
2020-04-29 01:14:06.907482 (MainThread): 
2020-04-29 01:14:06.907743 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 01:14:06.907915 (MainThread): 
2020-04-29 01:14:06.908058 (MainThread): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
2020-04-29 01:14:06.908184 (MainThread):   column "order_flash.order_ticket_unique_id" must appear in the GROUP BY clause or be used in an aggregate function
2020-04-29 01:14:06.908302 (MainThread):   compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 01:14:06.908439 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 01:14:06.908667 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e87d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10caac190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e48c5d0>]}
2020-04-29 01:14:06.908905 (MainThread): Flushing usage events
2020-04-29 01:21:34.866835 (MainThread): Running with dbt=0.16.1
2020-04-29 01:21:34.931277 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:21:34.932143 (MainThread): Tracking: tracking
2020-04-29 01:21:34.937328 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e35f510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e33a410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e331990>]}
2020-04-29 01:21:34.957945 (MainThread): Partial parsing not enabled
2020-04-29 01:21:34.959765 (MainThread): Parsing macros/core.sql
2020-04-29 01:21:34.964335 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:21:34.972544 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:21:34.974322 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:21:34.992453 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:21:35.025615 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:21:35.047044 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:21:35.049199 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:21:35.057334 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:21:35.074869 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:21:35.082528 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:21:35.089548 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:21:35.094703 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:21:35.095688 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:21:35.096807 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:21:35.098520 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:21:35.100625 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:21:35.109803 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:21:35.111864 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:21:35.112958 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:21:35.155329 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:21:35.156550 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:21:35.157519 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:21:35.158664 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:21:35.160997 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:21:35.163410 (MainThread): Parsing macros/relations.sql
2020-04-29 01:21:35.164809 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:21:35.182099 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:21:35.200202 (MainThread): Partial parsing not enabled
2020-04-29 01:21:35.227225 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:21:35.227327 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.243310 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:35.243411 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.247221 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:35.247306 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.251462 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:21:35.251547 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.255365 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:21:35.255450 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.260031 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:35.260122 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.264829 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:21:35.264917 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.270880 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:21:35.270968 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.418310 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:21:35.422074 (MainThread): 
2020-04-29 01:21:35.422579 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:21:35.422791 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:21:35.447534 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:21:35.447677 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:21:35.532147 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:21:35.532348 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:21:36.292439 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.76 seconds
2020-04-29 01:21:36.326508 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:21:36.326695 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:21:36.328378 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:21:36.328473 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:21:36.368968 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:21:36.369169 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:21:36.369284 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:21:36.498421 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-29 01:21:36.507404 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:21:36.796451 (MainThread): Using postgres connection "master".
2020-04-29 01:21:36.796618 (MainThread): On master: BEGIN
2020-04-29 01:21:37.230748 (MainThread): SQL status: BEGIN in 0.43 seconds
2020-04-29 01:21:37.230962 (MainThread): Using postgres connection "master".
2020-04-29 01:21:37.231079 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:21:37.433930 (MainThread): SQL status: SELECT in 0.20 seconds
2020-04-29 01:21:37.518265 (MainThread): On master: ROLLBACK
2020-04-29 01:21:37.768078 (MainThread): Using postgres connection "master".
2020-04-29 01:21:37.768316 (MainThread): On master: BEGIN
2020-04-29 01:21:38.190901 (MainThread): SQL status: BEGIN in 0.42 seconds
2020-04-29 01:21:38.191150 (MainThread): On master: COMMIT
2020-04-29 01:21:38.191293 (MainThread): Using postgres connection "master".
2020-04-29 01:21:38.191416 (MainThread): On master: COMMIT
2020-04-29 01:21:38.230206 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:21:38.230827 (MainThread): 18:21:38 | Concurrency: 1 threads (target='dev')
2020-04-29 01:21:38.231078 (MainThread): 18:21:38 | 
2020-04-29 01:21:38.233259 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 01:21:38.233535 (Thread-1): 18:21:38 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 01:21:38.233941 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:38.234079 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:21:38.234228 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 01:21:38.250860 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 01:21:38.251513 (Thread-1): finished collecting timing info
2020-04-29 01:21:38.295070 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:38.295248 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 01:21:38.386067 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-29 01:21:38.389256 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:38.389419 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:21:38.429415 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:21:38.431174 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 01:21:38.431615 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:38.431728 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 01:21:38.471310 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:21:38.471752 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:38.472017 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 01:21:38.535281 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:21:38.540507 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:38.540642 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 01:21:38.581012 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:21:38.584401 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:38.584646 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 01:21:38.817014 (Thread-1): SQL status: ALTER TABLE in 0.23 seconds
2020-04-29 01:21:38.818794 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:21:38.818983 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:38.819140 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:21:39.041573 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-29 01:21:39.043736 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:21:39.043867 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:21:39.292522 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-29 01:21:39.296694 (Thread-1): finished collecting timing info
2020-04-29 01:21:39.297571 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb733573-63f1-4971-9aea-67f414c1fda0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ea56b10>]}
2020-04-29 01:21:39.297895 (Thread-1): 18:21:39 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 1.06s]
2020-04-29 01:21:39.298074 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 01:21:39.298279 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 01:21:39.298612 (Thread-1): 18:21:39 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 01:21:39.299174 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:21:39.299341 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 01:21:39.299528 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 01:21:39.306230 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 01:21:39.306700 (Thread-1): finished collecting timing info
2020-04-29 01:21:39.314308 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:21:39.314458 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 01:21:39.558384 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-29 01:21:39.562113 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:21:39.562269 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:21:39.865812 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-29 01:21:39.869944 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 01:21:39.870505 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:21:39.870662 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 01:21:39.956224 (Thread-1): SQL status: BEGIN in 0.09 seconds
2020-04-29 01:21:39.956658 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:21:39.956933 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 01:21:40.021744 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:21:40.059083 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:21:40.059289 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 01:21:40.099824 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:21:40.104207 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:21:40.104363 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 01:21:40.185597 (Thread-1): SQL status: ALTER TABLE in 0.08 seconds
2020-04-29 01:21:40.187121 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:21:40.187302 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:21:40.187437 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:21:40.402089 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-29 01:21:40.405219 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:21:40.405421 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:21:40.628183 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:21:40.632613 (Thread-1): finished collecting timing info
2020-04-29 01:21:40.633453 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb733573-63f1-4971-9aea-67f414c1fda0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecca2d0>]}
2020-04-29 01:21:40.633755 (Thread-1): 18:21:40 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.33s]
2020-04-29 01:21:40.633934 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 01:21:40.634117 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 01:21:40.634299 (Thread-1): 18:21:40 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 01:21:40.635029 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:40.635306 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 01:21:40.635509 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 01:21:40.641974 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 01:21:40.642419 (Thread-1): finished collecting timing info
2020-04-29 01:21:40.650001 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:40.650134 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 01:21:40.889821 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-29 01:21:40.893927 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:40.894083 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:21:41.063943 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:21:41.065899 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 01:21:41.066386 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:41.066512 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 01:21:41.106198 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:21:41.106399 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:41.106503 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 01:21:41.161358 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:21:41.167584 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:41.167790 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 01:21:41.208203 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:21:41.213186 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:41.213344 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 01:21:41.256827 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:21:41.258817 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:21:41.259018 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:41.259188 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:21:41.432346 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:21:41.434804 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:21:41.434934 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:21:41.663593 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-29 01:21:41.667846 (Thread-1): finished collecting timing info
2020-04-29 01:21:41.668699 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb733573-63f1-4971-9aea-67f414c1fda0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e9b31d0>]}
2020-04-29 01:21:41.669007 (Thread-1): 18:21:41 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.03s]
2020-04-29 01:21:41.669189 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 01:21:41.669377 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 01:21:41.669634 (Thread-1): 18:21:41 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 01:21:41.670312 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:21:41.670550 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 01:21:41.670828 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 01:21:41.677223 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 01:21:41.677679 (Thread-1): finished collecting timing info
2020-04-29 01:21:41.685198 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:21:41.685334 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 01:21:41.864125 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:21:41.868281 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:21:41.868437 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:21:42.036819 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:21:42.039868 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 01:21:42.040483 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:21:42.040633 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 01:21:42.080736 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:21:42.080992 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:21:42.081123 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 01:21:42.133856 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:21:42.137899 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:21:42.138021 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 01:21:42.180988 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:21:42.185270 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:21:42.185426 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 01:21:42.226197 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:21:42.228145 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:21:42.228343 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:21:42.228499 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:21:42.399437 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:21:42.402739 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:21:42.402900 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:21:42.583455 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:21:42.589168 (Thread-1): finished collecting timing info
2020-04-29 01:21:42.590020 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb733573-63f1-4971-9aea-67f414c1fda0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecbfa90>]}
2020-04-29 01:21:42.590321 (Thread-1): 18:21:42 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 0.92s]
2020-04-29 01:21:42.590477 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 01:21:42.590637 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 01:21:42.590798 (Thread-1): 18:21:42 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 01:21:42.591328 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:21:42.591467 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 01:21:42.591588 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 01:21:42.600551 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 01:21:42.600988 (Thread-1): finished collecting timing info
2020-04-29 01:21:42.608312 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:21:42.608446 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 01:21:42.781957 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:21:42.784793 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:21:42.784932 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:21:42.956081 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:21:42.959204 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 01:21:42.959824 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:21:42.959982 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 01:21:43.000094 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:21:43.000521 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:21:43.000698 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 01:21:43.057429 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:21:43.061573 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:21:43.061727 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 01:21:43.102925 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:21:43.104104 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:21:43.104236 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:21:43.104341 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:21:43.273269 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:21:43.276636 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:21:43.276790 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:21:43.448791 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:21:43.451651 (Thread-1): finished collecting timing info
2020-04-29 01:21:43.452365 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb733573-63f1-4971-9aea-67f414c1fda0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e70f790>]}
2020-04-29 01:21:43.452619 (Thread-1): 18:21:43 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 0.86s]
2020-04-29 01:21:43.452768 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 01:21:43.452928 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 01:21:43.453266 (Thread-1): 18:21:43 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 01:21:43.453773 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:43.453903 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 01:21:43.454018 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 01:21:43.461773 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 01:21:43.462253 (Thread-1): finished collecting timing info
2020-04-29 01:21:43.472210 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:43.472404 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 01:21:43.667654 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 01:21:43.671841 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:43.672002 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:21:43.840344 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:21:43.843237 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 01:21:43.843676 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:43.843791 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 01:21:43.884315 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:21:43.884752 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:43.885024 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 01:21:43.940142 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:21:43.944434 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:43.944599 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 01:21:43.985094 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:21:43.987033 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:21:43.987232 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:43.987392 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:21:44.156903 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:21:44.159977 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:21:44.160169 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:21:44.375714 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:21:44.380195 (Thread-1): finished collecting timing info
2020-04-29 01:21:44.381047 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb733573-63f1-4971-9aea-67f414c1fda0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ea65d10>]}
2020-04-29 01:21:44.381353 (Thread-1): 18:21:44 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.93s]
2020-04-29 01:21:44.381534 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 01:21:44.381773 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 01:21:44.382113 (Thread-1): 18:21:44 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 01:21:44.382588 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:21:44.382756 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 01:21:44.382881 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 01:21:44.391831 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 01:21:44.392300 (Thread-1): finished collecting timing info
2020-04-29 01:21:44.399885 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:21:44.400023 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 01:21:44.577302 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:21:44.581442 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:21:44.581598 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 01:21:44.752334 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:21:44.755387 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 01:21:44.756001 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:21:44.756153 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 01:21:44.796373 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:21:44.796857 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:21:44.797140 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),

select * from FROM order_flash INNER JOIN events USING (event_unique_id)
  );

2020-04-29 01:21:44.837001 (Thread-1): Postgres error: syntax error at or near "select"
LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
         ^

2020-04-29 01:21:44.837266 (Thread-1): On model.order_history.order_flash_event: ROLLBACK
2020-04-29 01:21:44.877625 (Thread-1): finished collecting timing info
2020-04-29 01:21:44.878677 (Thread-1): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "select"
  LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
           ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "select"
LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
         ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "select"
  LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
           ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 01:21:44.881678 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb733573-63f1-4971-9aea-67f414c1fda0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ea65d10>]}
2020-04-29 01:21:44.881974 (Thread-1): 18:21:44 | 7 of 8 ERROR creating view model data_science.order_flash_event...... [ERROR in 0.50s]
2020-04-29 01:21:44.882157 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 01:21:44.882574 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:21:44.882768 (Thread-1): 18:21:44 | 8 of 8 SKIP relation data_science.customers.......................... [SKIP]
2020-04-29 01:21:44.882936 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:21:44.902036 (MainThread): Using postgres connection "master".
2020-04-29 01:21:44.902288 (MainThread): On master: BEGIN
2020-04-29 01:21:44.955573 (MainThread): SQL status: BEGIN in 0.05 seconds
2020-04-29 01:21:44.956048 (MainThread): On master: COMMIT
2020-04-29 01:21:44.956370 (MainThread): Using postgres connection "master".
2020-04-29 01:21:44.956550 (MainThread): On master: COMMIT
2020-04-29 01:21:44.994526 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:21:44.995469 (MainThread): 18:21:44 | 
2020-04-29 01:21:44.995711 (MainThread): 18:21:44 | Finished running 8 view models in 9.57s.
2020-04-29 01:21:44.995905 (MainThread): Connection 'master' was left open.
2020-04-29 01:21:44.996057 (MainThread): On master: Close
2020-04-29 01:21:44.996437 (MainThread): Connection 'model.order_history.order_flash_event' was left open.
2020-04-29 01:21:44.996601 (MainThread): On model.order_history.order_flash_event: Close
2020-04-29 01:21:45.021248 (MainThread): 
2020-04-29 01:21:45.021482 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 01:21:45.021630 (MainThread): 
2020-04-29 01:21:45.021761 (MainThread): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
2020-04-29 01:21:45.021879 (MainThread):   syntax error at or near "select"
2020-04-29 01:21:45.021987 (MainThread):   LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
2020-04-29 01:21:45.022094 (MainThread):            ^
2020-04-29 01:21:45.022197 (MainThread):   compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 01:21:45.022317 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 01:21:45.022521 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e73a6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ec6b9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ec67d50>]}
2020-04-29 01:21:45.022744 (MainThread): Flushing usage events
2020-04-29 01:22:21.664782 (MainThread): Running with dbt=0.16.1
2020-04-29 01:22:21.741051 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:22:21.741960 (MainThread): Tracking: tracking
2020-04-29 01:22:21.748334 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f12b9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f3b0dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f3b0e90>]}
2020-04-29 01:22:21.771173 (MainThread): Partial parsing not enabled
2020-04-29 01:22:21.773376 (MainThread): Parsing macros/core.sql
2020-04-29 01:22:21.778890 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:22:21.787248 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:22:21.789089 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:22:21.807520 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:22:21.841644 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:22:21.863548 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:22:21.865534 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:22:21.872126 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:22:21.885250 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:22:21.892386 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:22:21.898946 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:22:21.904123 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:22:21.905132 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:22:21.906270 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:22:21.908036 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:22:21.910128 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:22:21.919935 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:22:21.921960 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:22:21.923038 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:22:21.972934 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:22:21.974211 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:22:21.975191 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:22:21.976330 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:22:21.978789 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:22:21.981149 (MainThread): Parsing macros/relations.sql
2020-04-29 01:22:21.982630 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:22:21.999692 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:22:22.017801 (MainThread): Partial parsing not enabled
2020-04-29 01:22:22.045647 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:22:22.045754 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.062009 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:22.062103 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.066100 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:22.066191 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.070546 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:22:22.070637 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.074757 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:22:22.074847 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.079525 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:22.079635 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.085325 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:22:22.085427 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.091837 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:22.091927 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.233349 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:22:22.238192 (MainThread): 
2020-04-29 01:22:22.238510 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:22:22.238605 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:22.265111 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:22:22.265403 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:22:22.357605 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:22:22.357745 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:22:22.789364 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.43 seconds
2020-04-29 01:22:22.827797 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:22:22.827937 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:22:22.829857 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:22:22.829973 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:22:22.871067 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:22.871493 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:22:22.871766 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:22:22.989743 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-29 01:22:22.995310 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:22:23.072402 (MainThread): Using postgres connection "master".
2020-04-29 01:22:23.072558 (MainThread): On master: BEGIN
2020-04-29 01:22:23.443528 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-29 01:22:23.443992 (MainThread): Using postgres connection "master".
2020-04-29 01:22:23.444286 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:22:23.610194 (MainThread): SQL status: SELECT in 0.17 seconds
2020-04-29 01:22:23.682656 (MainThread): On master: ROLLBACK
2020-04-29 01:22:23.721367 (MainThread): Using postgres connection "master".
2020-04-29 01:22:23.721622 (MainThread): On master: BEGIN
2020-04-29 01:22:23.803403 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-29 01:22:23.803862 (MainThread): On master: COMMIT
2020-04-29 01:22:23.804188 (MainThread): Using postgres connection "master".
2020-04-29 01:22:23.804356 (MainThread): On master: COMMIT
2020-04-29 01:22:23.843398 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:22:23.844356 (MainThread): 18:22:23 | Concurrency: 1 threads (target='dev')
2020-04-29 01:22:23.844619 (MainThread): 18:22:23 | 
2020-04-29 01:22:23.846990 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 01:22:23.847265 (Thread-1): 18:22:23 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 01:22:23.847644 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:23.847781 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:22:23.847932 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 01:22:23.864049 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 01:22:23.864514 (Thread-1): finished collecting timing info
2020-04-29 01:22:23.905155 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:23.905315 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 01:22:23.986449 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:22:23.990238 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:23.990399 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:22:24.031300 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:22:24.034379 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 01:22:24.034958 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:24.035112 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 01:22:24.075411 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:24.075827 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:24.076086 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 01:22:24.138321 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:22:24.144274 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:24.144449 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 01:22:24.208111 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-29 01:22:24.212466 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:24.212626 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 01:22:24.253463 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:24.254681 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:22:24.254819 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:24.254928 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:22:24.430564 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:22:24.433945 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:24.434114 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:22:24.621751 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:22:24.626197 (Thread-1): finished collecting timing info
2020-04-29 01:22:24.627219 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1a3bb78-6ec0-48d7-92cd-13fa4ae83ab3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f994650>]}
2020-04-29 01:22:24.627583 (Thread-1): 18:22:24 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.78s]
2020-04-29 01:22:24.627810 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 01:22:24.628043 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 01:22:24.628349 (Thread-1): 18:22:24 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 01:22:24.628804 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:22:24.628977 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 01:22:24.629131 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 01:22:24.635359 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 01:22:24.635766 (Thread-1): finished collecting timing info
2020-04-29 01:22:24.642609 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:24.642766 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 01:22:24.824560 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:24.828607 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:24.828750 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:22:25.002040 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:25.005088 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 01:22:25.005685 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:25.005842 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 01:22:25.046799 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:25.047240 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:25.047418 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 01:22:25.131097 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-29 01:22:25.138743 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:25.138882 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 01:22:25.181566 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:25.215644 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:25.215852 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 01:22:25.258719 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:25.260661 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:22:25.260857 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:25.261016 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:22:25.433374 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:22:25.436425 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:25.436590 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:22:25.654659 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:22:25.657347 (Thread-1): finished collecting timing info
2020-04-29 01:22:25.658001 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1a3bb78-6ec0-48d7-92cd-13fa4ae83ab3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f9c8b90>]}
2020-04-29 01:22:25.658239 (Thread-1): 18:22:25 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.03s]
2020-04-29 01:22:25.658379 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 01:22:25.658538 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 01:22:25.658933 (Thread-1): 18:22:25 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 01:22:25.659562 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:25.659679 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 01:22:25.659790 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 01:22:25.665416 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 01:22:25.665874 (Thread-1): finished collecting timing info
2020-04-29 01:22:25.672570 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:25.672712 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 01:22:25.850808 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:25.854659 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:25.854826 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:22:26.027235 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:26.029930 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 01:22:26.030530 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:26.030685 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 01:22:26.071552 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:26.071987 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:26.072268 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 01:22:26.132414 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:22:26.138781 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:26.138944 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 01:22:26.183481 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:26.187879 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:26.188053 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 01:22:26.229190 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:26.231115 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:22:26.231324 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:26.231488 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:22:26.405111 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:22:26.409581 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:26.409745 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:22:26.590196 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:26.594434 (Thread-1): finished collecting timing info
2020-04-29 01:22:26.595270 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1a3bb78-6ec0-48d7-92cd-13fa4ae83ab3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f5ac5d0>]}
2020-04-29 01:22:26.595575 (Thread-1): 18:22:26 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.94s]
2020-04-29 01:22:26.595757 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 01:22:26.595947 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 01:22:26.596476 (Thread-1): 18:22:26 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 01:22:26.597136 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:22:26.597311 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 01:22:26.597444 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 01:22:26.603731 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 01:22:26.604179 (Thread-1): finished collecting timing info
2020-04-29 01:22:26.611673 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:26.611857 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 01:22:26.778056 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:26.782212 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:26.782374 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:22:26.950861 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:26.954016 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 01:22:26.954646 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:26.954802 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 01:22:26.995220 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:26.995502 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:26.995690 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 01:22:27.071825 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-29 01:22:27.077830 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:27.077989 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 01:22:27.119497 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:27.123395 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:27.123623 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 01:22:27.166172 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:27.167541 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:22:27.167699 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:27.167826 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:22:27.347211 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:22:27.350250 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:27.350411 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:22:27.545107 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:22:27.548206 (Thread-1): finished collecting timing info
2020-04-29 01:22:27.548967 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1a3bb78-6ec0-48d7-92cd-13fa4ae83ab3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f9b2b10>]}
2020-04-29 01:22:27.549237 (Thread-1): 18:22:27 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 0.95s]
2020-04-29 01:22:27.549397 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 01:22:27.549571 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 01:22:27.549794 (Thread-1): 18:22:27 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 01:22:27.550392 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:22:27.550620 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 01:22:27.550767 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 01:22:27.561388 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 01:22:27.561941 (Thread-1): finished collecting timing info
2020-04-29 01:22:27.569951 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:27.570161 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 01:22:27.743261 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:27.746733 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:27.746876 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:22:27.947567 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 01:22:27.949896 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 01:22:27.950587 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:27.950815 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 01:22:27.992223 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:27.992430 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:27.992552 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 01:22:28.068724 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-29 01:22:28.072788 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:28.072942 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 01:22:28.114792 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:28.116337 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:22:28.116494 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:28.116641 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:22:28.299406 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:22:28.301673 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:28.301825 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:22:28.529040 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-29 01:22:28.533299 (Thread-1): finished collecting timing info
2020-04-29 01:22:28.534145 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1a3bb78-6ec0-48d7-92cd-13fa4ae83ab3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f667610>]}
2020-04-29 01:22:28.534447 (Thread-1): 18:22:28 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 0.98s]
2020-04-29 01:22:28.534628 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 01:22:28.534926 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 01:22:28.535507 (Thread-1): 18:22:28 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 01:22:28.536073 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:28.536205 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 01:22:28.536387 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 01:22:28.544352 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 01:22:28.544796 (Thread-1): finished collecting timing info
2020-04-29 01:22:28.552663 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:28.552813 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 01:22:28.737834 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:28.742131 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:28.742283 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:22:28.916745 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:28.919491 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 01:22:28.920132 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:28.920303 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 01:22:28.961990 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:28.962462 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:28.962643 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 01:22:29.022405 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:22:29.026130 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:29.026254 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 01:22:29.068865 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:29.070810 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:22:29.071003 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:29.071162 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:22:29.246761 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:22:29.249629 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:29.249789 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:22:29.422932 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:29.426635 (Thread-1): finished collecting timing info
2020-04-29 01:22:29.427489 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1a3bb78-6ec0-48d7-92cd-13fa4ae83ab3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f959450>]}
2020-04-29 01:22:29.427804 (Thread-1): 18:22:29 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.89s]
2020-04-29 01:22:29.427957 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 01:22:29.428113 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 01:22:29.428273 (Thread-1): 18:22:29 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 01:22:29.428780 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:29.428966 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 01:22:29.429093 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 01:22:29.437793 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 01:22:29.438204 (Thread-1): finished collecting timing info
2020-04-29 01:22:29.445525 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:29.445651 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 01:22:29.617574 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:29.621749 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:29.621903 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 01:22:29.799406 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:29.802480 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 01:22:29.803088 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:29.803246 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 01:22:29.845171 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:29.845599 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:29.845884 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
)

select * from FROM order_flash INNER JOIN events USING (event_unique_id)
  );

2020-04-29 01:22:29.887561 (Thread-1): Postgres error: syntax error at or near "FROM"
LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
                       ^

2020-04-29 01:22:29.888019 (Thread-1): On model.order_history.order_flash_event: ROLLBACK
2020-04-29 01:22:29.929728 (Thread-1): finished collecting timing info
2020-04-29 01:22:29.930640 (Thread-1): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "FROM"
  LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
                         ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "FROM"
LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
                       ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
  syntax error at or near "FROM"
  LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
                         ^
  compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 01:22:29.933529 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1a3bb78-6ec0-48d7-92cd-13fa4ae83ab3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f62a590>]}
2020-04-29 01:22:29.933822 (Thread-1): 18:22:29 | 7 of 8 ERROR creating view model data_science.order_flash_event...... [ERROR in 0.50s]
2020-04-29 01:22:29.934004 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 01:22:29.934539 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:22:29.934764 (Thread-1): 18:22:29 | 8 of 8 SKIP relation data_science.customers.......................... [SKIP]
2020-04-29 01:22:29.934944 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:22:29.998678 (MainThread): Using postgres connection "master".
2020-04-29 01:22:29.998938 (MainThread): On master: BEGIN
2020-04-29 01:22:30.039177 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:30.039444 (MainThread): On master: COMMIT
2020-04-29 01:22:30.039650 (MainThread): Using postgres connection "master".
2020-04-29 01:22:30.039846 (MainThread): On master: COMMIT
2020-04-29 01:22:30.079758 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:22:30.080710 (MainThread): 18:22:30 | 
2020-04-29 01:22:30.080951 (MainThread): 18:22:30 | Finished running 8 view models in 7.84s.
2020-04-29 01:22:30.081150 (MainThread): Connection 'master' was left open.
2020-04-29 01:22:30.081309 (MainThread): On master: Close
2020-04-29 01:22:30.081694 (MainThread): Connection 'model.order_history.order_flash_event' was left open.
2020-04-29 01:22:30.081861 (MainThread): On model.order_history.order_flash_event: Close
2020-04-29 01:22:30.105338 (MainThread): 
2020-04-29 01:22:30.105573 (MainThread): Completed with 1 error and 0 warnings:
2020-04-29 01:22:30.105712 (MainThread): 
2020-04-29 01:22:30.105831 (MainThread): Database Error in model order_flash_event (models/intermediate/order_flash_event.sql)
2020-04-29 01:22:30.105940 (MainThread):   syntax error at or near "FROM"
2020-04-29 01:22:30.106042 (MainThread):   LINE 11: select * from FROM order_flash INNER JOIN events USING (even...
2020-04-29 01:22:30.106144 (MainThread):                          ^
2020-04-29 01:22:30.106242 (MainThread):   compiled SQL at target/run/order_history/intermediate/order_flash_event.sql
2020-04-29 01:22:30.106352 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-04-29 01:22:30.106530 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f650cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f746550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f697f90>]}
2020-04-29 01:22:30.106724 (MainThread): Flushing usage events
2020-04-29 01:22:43.851035 (MainThread): Running with dbt=0.16.1
2020-04-29 01:22:43.924329 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:22:43.925138 (MainThread): Tracking: tracking
2020-04-29 01:22:43.930092 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111cf45d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f4cd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f4ce10>]}
2020-04-29 01:22:43.948903 (MainThread): Partial parsing not enabled
2020-04-29 01:22:43.950734 (MainThread): Parsing macros/core.sql
2020-04-29 01:22:43.955438 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:22:43.963590 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:22:43.965373 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:22:43.983726 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:22:44.017291 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:22:44.038923 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:22:44.040895 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:22:44.047384 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:22:44.060277 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:22:44.067252 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:22:44.073731 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:22:44.078864 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:22:44.079854 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:22:44.080972 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:22:44.082693 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:22:44.084848 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:22:44.094180 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:22:44.096239 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:22:44.097335 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:22:44.140850 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:22:44.142050 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:22:44.143003 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:22:44.144227 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:22:44.146447 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:22:44.148990 (MainThread): Parsing macros/relations.sql
2020-04-29 01:22:44.150502 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:22:44.170265 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:22:44.194200 (MainThread): Partial parsing not enabled
2020-04-29 01:22:44.222306 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:22:44.222431 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.238781 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:44.238878 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.242983 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:44.243071 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.247562 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:22:44.247651 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.251618 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:22:44.251706 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.256232 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:44.256324 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.261237 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:22:44.261325 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.267284 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:44.267376 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.406807 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:22:44.411065 (MainThread): 
2020-04-29 01:22:44.411546 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:22:44.411635 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:22:44.435685 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:22:44.435828 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:22:44.526143 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:22:44.526314 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:22:45.025305 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.50 seconds
2020-04-29 01:22:45.064562 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:22:45.064689 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:22:45.066612 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:22:45.066729 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:22:45.105654 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:45.106079 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:22:45.106342 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:22:45.197072 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-29 01:22:45.207139 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:22:45.282742 (MainThread): Using postgres connection "master".
2020-04-29 01:22:45.282890 (MainThread): On master: BEGIN
2020-04-29 01:22:45.680809 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-29 01:22:45.681239 (MainThread): Using postgres connection "master".
2020-04-29 01:22:45.681543 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:22:45.823794 (MainThread): SQL status: SELECT in 0.14 seconds
2020-04-29 01:22:45.911682 (MainThread): On master: ROLLBACK
2020-04-29 01:22:45.951531 (MainThread): Using postgres connection "master".
2020-04-29 01:22:45.951922 (MainThread): On master: BEGIN
2020-04-29 01:22:46.043553 (MainThread): SQL status: BEGIN in 0.09 seconds
2020-04-29 01:22:46.044043 (MainThread): On master: COMMIT
2020-04-29 01:22:46.044242 (MainThread): Using postgres connection "master".
2020-04-29 01:22:46.044401 (MainThread): On master: COMMIT
2020-04-29 01:22:46.083898 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:22:46.084569 (MainThread): 18:22:46 | Concurrency: 1 threads (target='dev')
2020-04-29 01:22:46.084800 (MainThread): 18:22:46 | 
2020-04-29 01:22:46.087286 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 01:22:46.087547 (Thread-1): 18:22:46 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 01:22:46.087925 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.088058 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:22:46.088194 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 01:22:46.104784 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 01:22:46.105271 (Thread-1): finished collecting timing info
2020-04-29 01:22:46.146322 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.146482 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 01:22:46.223941 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:22:46.226771 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.226907 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:22:46.266404 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:22:46.269532 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 01:22:46.270178 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.270331 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 01:22:46.309105 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:46.309537 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.309815 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 01:22:46.364517 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:22:46.370146 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.370284 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 01:22:46.409832 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:46.413787 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.413942 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 01:22:46.456325 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:46.458269 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:22:46.458467 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.458627 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 01:22:46.636169 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:22:46.639663 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 01:22:46.639821 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 01:22:46.822665 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:46.827023 (Thread-1): finished collecting timing info
2020-04-29 01:22:46.827916 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06450d7a-f289-4569-a91d-3c179f511904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11211f390>]}
2020-04-29 01:22:46.828245 (Thread-1): 18:22:46 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.74s]
2020-04-29 01:22:46.828437 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 01:22:46.828629 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 01:22:46.828815 (Thread-1): 18:22:46 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 01:22:46.829158 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:22:46.829292 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 01:22:46.829443 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 01:22:46.836005 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 01:22:46.836416 (Thread-1): finished collecting timing info
2020-04-29 01:22:46.844163 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:46.844302 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 01:22:47.012498 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:47.016618 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:47.016781 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:22:47.196142 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:47.199245 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 01:22:47.199877 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:47.200029 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 01:22:47.238875 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:47.239084 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:47.239213 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 01:22:47.294778 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:22:47.302062 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:47.302193 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 01:22:47.344318 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:47.378974 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:47.379171 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 01:22:47.418954 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:47.420666 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:22:47.420871 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:47.421107 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 01:22:47.609423 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-29 01:22:47.611374 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 01:22:47.611490 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 01:22:47.835687 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:22:47.839885 (Thread-1): finished collecting timing info
2020-04-29 01:22:47.840753 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06450d7a-f289-4569-a91d-3c179f511904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111cc8fd0>]}
2020-04-29 01:22:47.841063 (Thread-1): 18:22:47 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.01s]
2020-04-29 01:22:47.841244 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 01:22:47.841478 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 01:22:47.841921 (Thread-1): 18:22:47 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 01:22:47.842691 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:47.842889 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 01:22:47.843034 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 01:22:47.849679 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 01:22:47.850103 (Thread-1): finished collecting timing info
2020-04-29 01:22:47.857703 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:47.857844 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 01:22:48.042567 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:48.046909 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:48.047072 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:22:48.215176 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:48.216981 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 01:22:48.217481 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:48.217612 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 01:22:48.256443 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:48.256634 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:48.256736 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 01:22:48.307423 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:22:48.313801 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:48.313969 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 01:22:48.356174 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:48.360436 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:48.360599 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 01:22:48.407180 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:22:48.409097 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:22:48.409301 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:48.409472 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 01:22:48.580802 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:22:48.585530 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 01:22:48.585758 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 01:22:48.763034 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:48.767278 (Thread-1): finished collecting timing info
2020-04-29 01:22:48.768123 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06450d7a-f289-4569-a91d-3c179f511904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112258950>]}
2020-04-29 01:22:48.768429 (Thread-1): 18:22:48 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.93s]
2020-04-29 01:22:48.768612 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 01:22:48.768882 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 01:22:48.769350 (Thread-1): 18:22:48 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 01:22:48.769785 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:22:48.769925 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 01:22:48.770037 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 01:22:48.776162 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 01:22:48.776597 (Thread-1): finished collecting timing info
2020-04-29 01:22:48.784102 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:48.784231 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 01:22:48.973162 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 01:22:48.977004 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:48.977155 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:22:49.143800 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:49.146752 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 01:22:49.147320 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:49.147476 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 01:22:49.187264 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:49.187565 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:49.187744 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 01:22:49.241773 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 01:22:49.246308 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:49.246449 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-29 01:22:49.285899 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:49.288449 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:49.288566 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 01:22:49.330632 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:49.332483 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:22:49.332680 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:49.332841 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 01:22:49.502418 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:22:49.506015 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 01:22:49.506188 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 01:22:49.682103 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:49.686517 (Thread-1): finished collecting timing info
2020-04-29 01:22:49.687451 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06450d7a-f289-4569-a91d-3c179f511904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11211cf10>]}
2020-04-29 01:22:49.687769 (Thread-1): 18:22:49 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 0.92s]
2020-04-29 01:22:49.687952 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 01:22:49.688137 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 01:22:49.688580 (Thread-1): 18:22:49 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 01:22:49.689050 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:22:49.689214 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 01:22:49.689393 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 01:22:49.699408 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 01:22:49.699869 (Thread-1): finished collecting timing info
2020-04-29 01:22:49.707405 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:49.707616 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 01:22:49.876152 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:49.880332 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:49.880489 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:22:50.055301 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:50.058268 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 01:22:50.058857 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:50.059019 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 01:22:50.098464 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:50.098898 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:50.099180 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 01:22:50.156280 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:22:50.160577 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:50.160737 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 01:22:50.209287 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 01:22:50.211225 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:22:50.211428 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:50.211588 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 01:22:50.677557 (Thread-1): SQL status: COMMIT in 0.47 seconds
2020-04-29 01:22:50.680943 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 01:22:50.681098 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 01:22:50.947685 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2020-04-29 01:22:50.950685 (Thread-1): finished collecting timing info
2020-04-29 01:22:50.951581 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06450d7a-f289-4569-a91d-3c179f511904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111fa15d0>]}
2020-04-29 01:22:50.951871 (Thread-1): 18:22:50 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 1.26s]
2020-04-29 01:22:50.952033 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 01:22:50.952199 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 01:22:50.952601 (Thread-1): 18:22:50 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 01:22:50.953012 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:50.953182 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 01:22:50.953317 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 01:22:50.961308 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 01:22:50.961792 (Thread-1): finished collecting timing info
2020-04-29 01:22:50.972598 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:50.972854 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 01:22:51.152141 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:51.156429 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:51.156593 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:22:51.339394 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:51.341036 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 01:22:51.341471 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:51.341587 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 01:22:51.509084 (Thread-1): SQL status: BEGIN in 0.17 seconds
2020-04-29 01:22:51.509374 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:51.509545 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 01:22:51.689011 (Thread-1): SQL status: CREATE VIEW in 0.18 seconds
2020-04-29 01:22:51.692810 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:51.692933 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 01:22:51.733369 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:51.735344 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:22:51.735548 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:51.735710 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 01:22:52.031158 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-29 01:22:52.033203 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 01:22:52.033320 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 01:22:52.321673 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-04-29 01:22:52.325011 (Thread-1): finished collecting timing info
2020-04-29 01:22:52.325767 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06450d7a-f289-4569-a91d-3c179f511904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121840d0>]}
2020-04-29 01:22:52.326041 (Thread-1): 18:22:52 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 1.37s]
2020-04-29 01:22:52.326200 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 01:22:52.326368 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 01:22:52.326654 (Thread-1): 18:22:52 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 01:22:52.327110 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:52.327248 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 01:22:52.327399 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 01:22:52.336458 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 01:22:52.336939 (Thread-1): finished collecting timing info
2020-04-29 01:22:52.344644 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:52.344842 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 01:22:52.517547 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:52.521673 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:52.521846 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 01:22:52.698734 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 01:22:52.701270 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 01:22:52.701843 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:52.701993 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 01:22:52.865100 (Thread-1): SQL status: BEGIN in 0.16 seconds
2020-04-29 01:22:52.865442 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:52.865583 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
)

select * FROM order_flash INNER JOIN events USING (event_unique_id)
  );

2020-04-29 01:22:53.027896 (Thread-1): SQL status: CREATE VIEW in 0.16 seconds
2020-04-29 01:22:53.032112 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:53.032317 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
alter table "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" rename to "order_flash_event"
2020-04-29 01:22:53.072311 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:22:53.074265 (Thread-1): On model.order_history.order_flash_event: COMMIT
2020-04-29 01:22:53.074462 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:53.074624 (Thread-1): On model.order_history.order_flash_event: COMMIT
2020-04-29 01:22:53.245856 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:22:53.249245 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 01:22:53.249401 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 01:22:53.782157 (Thread-1): SQL status: DROP VIEW in 0.53 seconds
2020-04-29 01:22:53.785764 (Thread-1): finished collecting timing info
2020-04-29 01:22:53.786648 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06450d7a-f289-4569-a91d-3c179f511904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11212c350>]}
2020-04-29 01:22:53.786964 (Thread-1): 18:22:53 | 7 of 8 OK created view model data_science.order_flash_event.......... [CREATE VIEW in 1.46s]
2020-04-29 01:22:53.787153 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 01:22:53.787579 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:22:53.787921 (Thread-1): 18:22:53 | 8 of 8 START view model data_science.customers....................... [RUN]
2020-04-29 01:22:53.788459 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:22:53.788637 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash_event).
2020-04-29 01:22:53.788807 (Thread-1): Compiling model.order_history.customers
2020-04-29 01:22:53.798157 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 01:22:53.798591 (Thread-1): finished collecting timing info
2020-04-29 01:22:53.807166 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:22:53.807301 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 01:22:53.982236 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:53.986513 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:22:53.986668 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 01:22:54.157235 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-29 01:22:54.160053 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 01:22:54.160711 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:22:54.160874 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 01:22:54.200396 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:22:54.200847 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:22:54.201028 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash_event"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers,

        SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 01:22:54.262065 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 01:22:54.266453 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:22:54.266610 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 01:22:54.329631 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-29 01:22:54.331312 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 01:22:54.331503 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:22:54.331663 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 01:22:54.509048 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-29 01:22:54.512630 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:22:54.512793 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 01:22:55.993022 (Thread-1): SQL status: DROP VIEW in 1.48 seconds
2020-04-29 01:22:55.997271 (Thread-1): finished collecting timing info
2020-04-29 01:22:55.998129 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06450d7a-f289-4569-a91d-3c179f511904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112204d10>]}
2020-04-29 01:22:55.998437 (Thread-1): 18:22:55 | 8 of 8 OK created view model data_science.customers.................. [CREATE VIEW in 2.21s]
2020-04-29 01:22:55.998620 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:22:56.033943 (MainThread): Using postgres connection "master".
2020-04-29 01:22:56.034167 (MainThread): On master: BEGIN
2020-04-29 01:22:56.082074 (MainThread): SQL status: BEGIN in 0.05 seconds
2020-04-29 01:22:56.082336 (MainThread): On master: COMMIT
2020-04-29 01:22:56.082459 (MainThread): Using postgres connection "master".
2020-04-29 01:22:56.082566 (MainThread): On master: COMMIT
2020-04-29 01:22:56.121695 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:22:56.122138 (MainThread): 18:22:56 | 
2020-04-29 01:22:56.122286 (MainThread): 18:22:56 | Finished running 8 view models in 11.71s.
2020-04-29 01:22:56.122402 (MainThread): Connection 'master' was left open.
2020-04-29 01:22:56.122501 (MainThread): On master: Close
2020-04-29 01:22:56.122746 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 01:22:56.122844 (MainThread): On model.order_history.customers: Close
2020-04-29 01:22:56.144109 (MainThread): 
2020-04-29 01:22:56.144296 (MainThread): Completed successfully
2020-04-29 01:22:56.144426 (MainThread): 
Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
2020-04-29 01:22:56.144608 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121ec350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111cc86d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120d8ad0>]}
2020-04-29 01:22:56.144798 (MainThread): Flushing usage events
2020-04-29 01:34:50.619677 (MainThread): Running with dbt=0.16.1
2020-04-29 01:34:50.694009 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 01:34:50.695625 (MainThread): Tracking: tracking
2020-04-29 01:34:50.701829 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10749edd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076f8350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10772d890>]}
2020-04-29 01:34:50.725141 (MainThread): Partial parsing not enabled
2020-04-29 01:34:50.728591 (MainThread): Parsing macros/core.sql
2020-04-29 01:34:50.733788 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 01:34:50.741872 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 01:34:50.743662 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 01:34:50.761874 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 01:34:50.796797 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 01:34:50.824219 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 01:34:50.826270 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 01:34:50.832961 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 01:34:50.846145 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 01:34:50.853618 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 01:34:50.860226 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 01:34:50.865418 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 01:34:50.866432 (MainThread): Parsing macros/etc/query.sql
2020-04-29 01:34:50.868375 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 01:34:50.870152 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 01:34:50.872445 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 01:34:50.882146 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 01:34:50.884244 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 01:34:50.885366 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 01:34:50.928611 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 01:34:50.929833 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 01:34:50.930787 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 01:34:50.931928 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 01:34:50.934312 (MainThread): Parsing macros/catalog.sql
2020-04-29 01:34:50.936813 (MainThread): Parsing macros/relations.sql
2020-04-29 01:34:50.938210 (MainThread): Parsing macros/adapters.sql
2020-04-29 01:34:50.955445 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 01:34:50.973909 (MainThread): Partial parsing not enabled
2020-04-29 01:34:51.002148 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:34:51.002267 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.018626 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 01:34:51.018729 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.022761 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 01:34:51.022894 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.027267 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 01:34:51.027355 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.031222 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 01:34:51.031309 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.035942 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 01:34:51.036031 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.041029 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 01:34:51.041118 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.046882 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 01:34:51.046972 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.196856 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 01:34:51.200271 (MainThread): 
2020-04-29 01:34:51.200687 (MainThread): Acquiring new postgres connection "master".
2020-04-29 01:34:51.200778 (MainThread): Opening a new connection, currently in state init
2020-04-29 01:34:51.205140 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 01:34:51.205252 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 01:34:51.290516 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 01:34:51.290658 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 01:34:52.181877 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.89 seconds
2020-04-29 01:34:52.216525 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:34:52.216723 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 01:34:52.218164 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:34:52.218262 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 01:34:52.256964 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:34:52.257273 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 01:34:52.257450 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 01:34:52.355070 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-29 01:34:52.364448 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 01:34:52.655979 (MainThread): Using postgres connection "master".
2020-04-29 01:34:52.656140 (MainThread): On master: BEGIN
2020-04-29 01:34:53.236190 (MainThread): SQL status: BEGIN in 0.58 seconds
2020-04-29 01:34:53.236699 (MainThread): Using postgres connection "master".
2020-04-29 01:34:53.236967 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 01:34:53.367081 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-29 01:34:53.454335 (MainThread): On master: ROLLBACK
2020-04-29 01:34:53.531690 (MainThread): Using postgres connection "master".
2020-04-29 01:34:53.531961 (MainThread): On master: BEGIN
2020-04-29 01:34:53.634065 (MainThread): SQL status: BEGIN in 0.10 seconds
2020-04-29 01:34:53.634541 (MainThread): On master: COMMIT
2020-04-29 01:34:53.634741 (MainThread): Using postgres connection "master".
2020-04-29 01:34:53.634899 (MainThread): On master: COMMIT
2020-04-29 01:34:53.672721 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 01:34:53.673516 (MainThread): 18:34:53 | Concurrency: 1 threads (target='dev')
2020-04-29 01:34:53.673784 (MainThread): 18:34:53 | 
2020-04-29 01:34:53.676093 (Thread-1): Began running node model.order_history.customers
2020-04-29 01:34:53.676356 (Thread-1): 18:34:53 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-29 01:34:53.676731 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 01:34:53.676873 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 01:34:53.677023 (Thread-1): Compiling model.order_history.customers
2020-04-29 01:34:53.696501 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 01:34:53.697161 (Thread-1): finished collecting timing info
2020-04-29 01:34:53.738134 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:34:53.738292 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 01:34:53.820590 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-29 01:34:53.823657 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:34:53.823811 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 01:34:53.862575 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 01:34:53.865538 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 01:34:53.866165 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:34:53.866317 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 01:34:53.904527 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:34:53.904962 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:34:53.905239 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash_event"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers,

        SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        average_days_sold_after_onsale,
        average_days_sold_before_event,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 01:34:54.153498 (Thread-1): SQL status: CREATE VIEW in 0.25 seconds
2020-04-29 01:34:54.158116 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:34:54.158253 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-29 01:34:54.224414 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2020-04-29 01:34:54.228660 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:34:54.228817 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 01:34:54.268715 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 01:34:54.270741 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 01:34:54.270941 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:34:54.271100 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 01:34:54.442603 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-29 01:34:54.445107 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 01:34:54.445254 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 01:34:54.661600 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-29 01:34:54.665952 (Thread-1): finished collecting timing info
2020-04-29 01:34:54.666829 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0ac4461-0816-4a8b-8bcb-24cf0a97fc5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078f9e10>]}
2020-04-29 01:34:54.667146 (Thread-1): 18:34:54 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 0.99s]
2020-04-29 01:34:54.667341 (Thread-1): Finished running node model.order_history.customers
2020-04-29 01:34:54.694556 (MainThread): Using postgres connection "master".
2020-04-29 01:34:54.694749 (MainThread): On master: BEGIN
2020-04-29 01:34:54.733597 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-29 01:34:54.733922 (MainThread): On master: COMMIT
2020-04-29 01:34:54.734109 (MainThread): Using postgres connection "master".
2020-04-29 01:34:54.734266 (MainThread): On master: COMMIT
2020-04-29 01:34:54.794073 (MainThread): SQL status: COMMIT in 0.06 seconds
2020-04-29 01:34:54.794542 (MainThread): 18:34:54 | 
2020-04-29 01:34:54.794715 (MainThread): 18:34:54 | Finished running 1 view model in 3.59s.
2020-04-29 01:34:54.794885 (MainThread): Connection 'master' was left open.
2020-04-29 01:34:54.795074 (MainThread): On master: Close
2020-04-29 01:34:54.795383 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 01:34:54.795512 (MainThread): On model.order_history.customers: Close
2020-04-29 01:34:54.799722 (MainThread): 
2020-04-29 01:34:54.799944 (MainThread): Completed successfully
2020-04-29 01:34:54.800095 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-29 01:34:54.800310 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ccf9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10790a8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ab5a50>]}
2020-04-29 01:34:54.800528 (MainThread): Flushing usage events
2020-04-29 17:44:08.116367 (MainThread): Running with dbt=0.16.1
2020-04-29 17:44:08.251069 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-29 17:44:08.252186 (MainThread): Tracking: tracking
2020-04-29 17:44:08.260741 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b71290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b7fc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108926a50>]}
2020-04-29 17:44:08.291320 (MainThread): Partial parsing not enabled
2020-04-29 17:44:08.296762 (MainThread): Parsing macros/core.sql
2020-04-29 17:44:08.311413 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-29 17:44:08.323753 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-29 17:44:08.327491 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-29 17:44:08.352388 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-29 17:44:08.391689 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-29 17:44:08.417794 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-29 17:44:08.421852 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-29 17:44:08.430680 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-29 17:44:08.446507 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-29 17:44:08.454422 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-29 17:44:08.462597 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-29 17:44:08.469233 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-29 17:44:08.471071 (MainThread): Parsing macros/etc/query.sql
2020-04-29 17:44:08.472922 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-29 17:44:08.475408 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-29 17:44:08.478861 (MainThread): Parsing macros/etc/datetime.sql
2020-04-29 17:44:08.488986 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-29 17:44:08.491719 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-29 17:44:08.494534 (MainThread): Parsing macros/adapters/common.sql
2020-04-29 17:44:08.540014 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-29 17:44:08.542388 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-29 17:44:08.544036 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-29 17:44:08.545858 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-29 17:44:08.548917 (MainThread): Parsing macros/catalog.sql
2020-04-29 17:44:08.552175 (MainThread): Parsing macros/relations.sql
2020-04-29 17:44:08.554402 (MainThread): Parsing macros/adapters.sql
2020-04-29 17:44:08.572678 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-29 17:44:08.591351 (MainThread): Partial parsing not enabled
2020-04-29 17:44:08.619911 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 17:44:08.620044 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.641138 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:08.641318 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.648368 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:08.648492 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.653091 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 17:44:08.653184 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.657459 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 17:44:08.657550 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.662527 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:08.662629 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.667795 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 17:44:08.667890 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.673741 (MainThread): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:08.673830 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.820106 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-29 17:44:08.824010 (MainThread): 
2020-04-29 17:44:08.824435 (MainThread): Acquiring new postgres connection "master".
2020-04-29 17:44:08.824527 (MainThread): Opening a new connection, currently in state init
2020-04-29 17:44:08.850927 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-29 17:44:08.851067 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-29 17:44:08.944788 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-29 17:44:08.944924 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-29 17:44:09.417596 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.47 seconds
2020-04-29 17:44:09.463450 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-29 17:44:09.463701 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-29 17:44:09.465891 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 17:44:09.466020 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-29 17:44:09.502190 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-29 17:44:09.502600 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-29 17:44:09.502857 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-29 17:44:09.599501 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-29 17:44:09.605233 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-29 17:44:09.981511 (MainThread): Using postgres connection "master".
2020-04-29 17:44:09.981639 (MainThread): On master: BEGIN
2020-04-29 17:44:10.352581 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-29 17:44:10.352869 (MainThread): Using postgres connection "master".
2020-04-29 17:44:10.353038 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-29 17:44:10.497133 (MainThread): SQL status: SELECT in 0.14 seconds
2020-04-29 17:44:10.567090 (MainThread): On master: ROLLBACK
2020-04-29 17:44:10.607853 (MainThread): Using postgres connection "master".
2020-04-29 17:44:10.608021 (MainThread): On master: BEGIN
2020-04-29 17:44:10.717976 (MainThread): SQL status: BEGIN in 0.11 seconds
2020-04-29 17:44:10.718411 (MainThread): On master: COMMIT
2020-04-29 17:44:10.718656 (MainThread): Using postgres connection "master".
2020-04-29 17:44:10.718814 (MainThread): On master: COMMIT
2020-04-29 17:44:10.756809 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-29 17:44:10.757676 (MainThread): 10:44:10 | Concurrency: 1 threads (target='dev')
2020-04-29 17:44:10.757919 (MainThread): 10:44:10 | 
2020-04-29 17:44:10.761869 (Thread-1): Began running node model.order_history.stg_flash
2020-04-29 17:44:10.762248 (Thread-1): 10:44:10 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-04-29 17:44:10.762570 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:10.762681 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-29 17:44:10.762798 (Thread-1): Compiling model.order_history.stg_flash
2020-04-29 17:44:10.777243 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-29 17:44:10.778218 (Thread-1): finished collecting timing info
2020-04-29 17:44:10.816632 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:10.816790 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-29 17:44:10.891111 (Thread-1): SQL status: DROP VIEW in 0.07 seconds
2020-04-29 17:44:10.895622 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:10.895798 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 17:44:10.935944 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-29 17:44:10.938310 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-29 17:44:10.938900 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:10.939017 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-29 17:44:10.975054 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 17:44:10.975336 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:10.975505 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-29 17:44:11.023153 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 17:44:11.029172 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:11.029308 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-29 17:44:11.068050 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:11.071962 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:11.072122 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-29 17:44:11.109212 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:11.111121 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 17:44:11.111317 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:11.111478 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-29 17:44:12.490642 (Thread-1): SQL status: COMMIT in 1.38 seconds
2020-04-29 17:44:12.493645 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-29 17:44:12.493885 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-29 17:44:13.391695 (Thread-1): SQL status: DROP VIEW in 0.90 seconds
2020-04-29 17:44:13.394277 (Thread-1): finished collecting timing info
2020-04-29 17:44:13.394927 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb22783a-6277-43f1-af70-acca7c1a7c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f49810>]}
2020-04-29 17:44:13.395168 (Thread-1): 10:44:13 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 2.63s]
2020-04-29 17:44:13.395309 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-29 17:44:13.395470 (Thread-1): Began running node model.order_history.stg_order
2020-04-29 17:44:13.395840 (Thread-1): 10:44:13 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-04-29 17:44:13.396295 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-29 17:44:13.396407 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-29 17:44:13.396507 (Thread-1): Compiling model.order_history.stg_order
2020-04-29 17:44:13.401924 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-29 17:44:13.402375 (Thread-1): finished collecting timing info
2020-04-29 17:44:13.409198 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 17:44:13.409352 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-29 17:44:13.786235 (Thread-1): SQL status: DROP VIEW in 0.38 seconds
2020-04-29 17:44:13.790406 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 17:44:13.790560 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 17:44:14.049737 (Thread-1): SQL status: DROP VIEW in 0.26 seconds
2020-04-29 17:44:14.052768 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-29 17:44:14.053357 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 17:44:14.053513 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-29 17:44:14.090110 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 17:44:14.090561 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 17:44:14.090765 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-29 17:44:14.162502 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-29 17:44:14.169767 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 17:44:14.169911 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-29 17:44:14.211694 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:14.245849 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 17:44:14.246047 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-29 17:44:14.283604 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:14.285257 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 17:44:14.285419 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 17:44:14.285554 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-29 17:44:14.922844 (Thread-1): SQL status: COMMIT in 0.64 seconds
2020-04-29 17:44:14.926157 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-29 17:44:14.926316 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-29 17:44:16.439313 (Thread-1): SQL status: DROP VIEW in 1.51 seconds
2020-04-29 17:44:16.443605 (Thread-1): finished collecting timing info
2020-04-29 17:44:16.444459 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb22783a-6277-43f1-af70-acca7c1a7c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062ecb90>]}
2020-04-29 17:44:16.444767 (Thread-1): 10:44:16 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 3.05s]
2020-04-29 17:44:16.444949 (Thread-1): Finished running node model.order_history.stg_order
2020-04-29 17:44:16.445136 (Thread-1): Began running node model.order_history.stg_customers
2020-04-29 17:44:16.445571 (Thread-1): 10:44:16 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-04-29 17:44:16.446394 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:16.446561 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-29 17:44:16.446680 (Thread-1): Compiling model.order_history.stg_customers
2020-04-29 17:44:16.452636 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-29 17:44:16.453096 (Thread-1): finished collecting timing info
2020-04-29 17:44:16.460440 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:16.460590 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-29 17:44:16.698773 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-29 17:44:16.701365 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:16.701483 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 17:44:16.880742 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-29 17:44:16.883797 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-29 17:44:16.884507 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:16.884655 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-29 17:44:16.921766 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 17:44:16.922255 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:16.922474 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-29 17:44:16.981201 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 17:44:16.986787 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:16.986953 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-29 17:44:17.027668 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:17.030723 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:17.030857 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-29 17:44:17.070709 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:17.072517 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 17:44:17.072700 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:17.072863 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-29 17:44:17.271963 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-29 17:44:17.276874 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-29 17:44:17.277112 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-29 17:44:17.482167 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 17:44:17.486103 (Thread-1): finished collecting timing info
2020-04-29 17:44:17.486972 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb22783a-6277-43f1-af70-acca7c1a7c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108eb63d0>]}
2020-04-29 17:44:17.487263 (Thread-1): 10:44:17 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.04s]
2020-04-29 17:44:17.487425 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-29 17:44:17.487589 (Thread-1): Began running node model.order_history.stg_events
2020-04-29 17:44:17.487843 (Thread-1): 10:44:17 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-04-29 17:44:17.488428 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-29 17:44:17.488676 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-29 17:44:17.488814 (Thread-1): Compiling model.order_history.stg_events
2020-04-29 17:44:17.495087 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-29 17:44:17.495556 (Thread-1): finished collecting timing info
2020-04-29 17:44:17.503517 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 17:44:17.503704 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-29 17:44:17.704600 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-29 17:44:17.707389 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 17:44:17.707522 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 17:44:17.935991 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-29 17:44:17.938542 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-29 17:44:17.939220 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 17:44:17.939403 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-29 17:44:18.004735 (Thread-1): SQL status: BEGIN in 0.07 seconds
2020-04-29 17:44:18.005039 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 17:44:18.005191 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-04-29 17:44:18.061424 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 17:44:18.065667 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 17:44:18.065832 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-29 17:44:18.107508 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:18.108699 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 17:44:18.108830 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 17:44:18.108939 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-29 17:44:18.319831 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-29 17:44:18.323057 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-29 17:44:18.323287 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-29 17:44:18.536238 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 17:44:18.539013 (Thread-1): finished collecting timing info
2020-04-29 17:44:18.539660 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb22783a-6277-43f1-af70-acca7c1a7c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e6ae50>]}
2020-04-29 17:44:18.539981 (Thread-1): 10:44:18 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 1.05s]
2020-04-29 17:44:18.540119 (Thread-1): Finished running node model.order_history.stg_events
2020-04-29 17:44:18.540255 (Thread-1): Began running node model.order_history.order_flash
2020-04-29 17:44:18.540458 (Thread-1): 10:44:18 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-04-29 17:44:18.540765 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-29 17:44:18.540864 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-29 17:44:18.540957 (Thread-1): Compiling model.order_history.order_flash
2020-04-29 17:44:18.550032 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-29 17:44:18.550445 (Thread-1): finished collecting timing info
2020-04-29 17:44:18.557857 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 17:44:18.558398 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-29 17:44:18.840074 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2020-04-29 17:44:18.843451 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 17:44:18.843636 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 17:44:19.554773 (Thread-1): SQL status: DROP VIEW in 0.71 seconds
2020-04-29 17:44:19.557492 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-29 17:44:19.558081 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 17:44:19.558244 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-29 17:44:19.596556 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 17:44:19.596851 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 17:44:19.597023 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-29 17:44:19.656184 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 17:44:19.659538 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 17:44:19.659754 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-29 17:44:19.698149 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:19.700118 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 17:44:19.700312 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 17:44:19.700474 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-29 17:44:19.943635 (Thread-1): SQL status: COMMIT in 0.24 seconds
2020-04-29 17:44:19.947026 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-29 17:44:19.947177 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-29 17:44:20.200900 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-29 17:44:20.204552 (Thread-1): finished collecting timing info
2020-04-29 17:44:20.205414 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb22783a-6277-43f1-af70-acca7c1a7c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109166350>]}
2020-04-29 17:44:20.205673 (Thread-1): 10:44:20 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 1.66s]
2020-04-29 17:44:20.205826 (Thread-1): Finished running node model.order_history.order_flash
2020-04-29 17:44:20.205981 (Thread-1): Began running node model.order_history.customer_broker
2020-04-29 17:44:20.206349 (Thread-1): 10:44:20 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-04-29 17:44:20.206913 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:20.207091 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-29 17:44:20.207280 (Thread-1): Compiling model.order_history.customer_broker
2020-04-29 17:44:20.214956 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-29 17:44:20.215405 (Thread-1): finished collecting timing info
2020-04-29 17:44:20.222807 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:20.222944 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-29 17:44:21.158862 (Thread-1): SQL status: DROP VIEW in 0.94 seconds
2020-04-29 17:44:21.163001 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:21.163211 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 17:44:22.425451 (Thread-1): SQL status: DROP VIEW in 1.26 seconds
2020-04-29 17:44:22.427763 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-29 17:44:22.428370 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:22.428509 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-29 17:44:22.467442 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 17:44:22.467936 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:22.468118 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-29 17:44:22.519141 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 17:44:22.522055 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:22.522196 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-29 17:44:22.559905 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:22.560934 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 17:44:22.561055 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:22.561150 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-29 17:44:22.760930 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-29 17:44:22.764479 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-29 17:44:22.764653 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-29 17:44:23.061958 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-29 17:44:23.064306 (Thread-1): finished collecting timing info
2020-04-29 17:44:23.064857 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb22783a-6277-43f1-af70-acca7c1a7c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e03410>]}
2020-04-29 17:44:23.065060 (Thread-1): 10:44:23 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 2.86s]
2020-04-29 17:44:23.065178 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-29 17:44:23.065297 (Thread-1): Began running node model.order_history.order_flash_event
2020-04-29 17:44:23.065476 (Thread-1): 10:44:23 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-04-29 17:44:23.065959 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:23.066117 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-29 17:44:23.066288 (Thread-1): Compiling model.order_history.order_flash_event
2020-04-29 17:44:23.073004 (Thread-1): Writing injected SQL for node "model.order_history.order_flash_event"
2020-04-29 17:44:23.073386 (Thread-1): finished collecting timing info
2020-04-29 17:44:23.079281 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:23.079420 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-04-29 17:44:23.286472 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-29 17:44:23.290514 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:23.290678 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 17:44:23.481317 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-29 17:44:23.484050 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash_event"
2020-04-29 17:44:23.484648 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:23.484808 (Thread-1): On model.order_history.order_flash_event: BEGIN
2020-04-29 17:44:23.521068 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 17:44:23.521275 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:23.521390 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
)

select * FROM order_flash INNER JOIN events USING (event_unique_id)
  );

2020-04-29 17:44:23.572565 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-29 17:44:23.576392 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:23.576593 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
alter table "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" rename to "order_flash_event"
2020-04-29 17:44:23.628888 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-29 17:44:23.630189 (Thread-1): On model.order_history.order_flash_event: COMMIT
2020-04-29 17:44:23.630374 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:23.630488 (Thread-1): On model.order_history.order_flash_event: COMMIT
2020-04-29 17:44:24.057320 (Thread-1): SQL status: COMMIT in 0.43 seconds
2020-04-29 17:44:24.060766 (Thread-1): Using postgres connection "model.order_history.order_flash_event".
2020-04-29 17:44:24.060941 (Thread-1): On model.order_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-04-29 17:44:24.295077 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-29 17:44:24.298711 (Thread-1): finished collecting timing info
2020-04-29 17:44:24.299558 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb22783a-6277-43f1-af70-acca7c1a7c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e03410>]}
2020-04-29 17:44:24.299812 (Thread-1): 10:44:24 | 7 of 8 OK created view model data_science.order_flash_event.......... [CREATE VIEW in 1.23s]
2020-04-29 17:44:24.299964 (Thread-1): Finished running node model.order_history.order_flash_event
2020-04-29 17:44:24.300466 (Thread-1): Began running node model.order_history.customers
2020-04-29 17:44:24.300653 (Thread-1): 10:44:24 | 8 of 8 START view model data_science.customers....................... [RUN]
2020-04-29 17:44:24.300980 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-29 17:44:24.301104 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash_event).
2020-04-29 17:44:24.301217 (Thread-1): Compiling model.order_history.customers
2020-04-29 17:44:24.309710 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-29 17:44:24.310113 (Thread-1): finished collecting timing info
2020-04-29 17:44:24.316745 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 17:44:24.316886 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-29 17:44:24.625576 (Thread-1): SQL status: DROP VIEW in 0.31 seconds
2020-04-29 17:44:24.629215 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 17:44:24.629339 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 17:44:24.897560 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2020-04-29 17:44:24.899604 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-29 17:44:24.900150 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 17:44:24.900289 (Thread-1): On model.order_history.customers: BEGIN
2020-04-29 17:44:24.936557 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-29 17:44:24.936806 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 17:44:24.936975 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash_event"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers,

        SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        average_days_sold_after_onsale,
        average_days_sold_before_event,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-29 17:44:24.998595 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-29 17:44:25.002867 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 17:44:25.003023 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-29 17:44:25.041250 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-29 17:44:25.042467 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 17:44:25.042609 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 17:44:25.042722 (Thread-1): On model.order_history.customers: COMMIT
2020-04-29 17:44:25.285799 (Thread-1): SQL status: COMMIT in 0.24 seconds
2020-04-29 17:44:25.289999 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-29 17:44:25.290263 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-29 17:44:25.531887 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-29 17:44:25.534752 (Thread-1): finished collecting timing info
2020-04-29 17:44:25.535446 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb22783a-6277-43f1-af70-acca7c1a7c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e37a90>]}
2020-04-29 17:44:25.535700 (Thread-1): 10:44:25 | 8 of 8 OK created view model data_science.customers.................. [CREATE VIEW in 1.23s]
2020-04-29 17:44:25.535849 (Thread-1): Finished running node model.order_history.customers
2020-04-29 17:44:25.559546 (MainThread): Using postgres connection "master".
2020-04-29 17:44:25.559739 (MainThread): On master: BEGIN
2020-04-29 17:44:25.818881 (MainThread): SQL status: BEGIN in 0.26 seconds
2020-04-29 17:44:25.819110 (MainThread): On master: COMMIT
2020-04-29 17:44:25.819227 (MainThread): Using postgres connection "master".
2020-04-29 17:44:25.819331 (MainThread): On master: COMMIT
2020-04-29 17:44:26.289108 (MainThread): SQL status: COMMIT in 0.47 seconds
2020-04-29 17:44:26.290125 (MainThread): 10:44:26 | 
2020-04-29 17:44:26.290590 (MainThread): 10:44:26 | Finished running 8 view models in 17.47s.
2020-04-29 17:44:26.291099 (MainThread): Connection 'master' was left open.
2020-04-29 17:44:26.291433 (MainThread): On master: Close
2020-04-29 17:44:26.291850 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-29 17:44:26.291988 (MainThread): On model.order_history.customers: Close
2020-04-29 17:44:26.322363 (MainThread): 
2020-04-29 17:44:26.322597 (MainThread): Completed successfully
2020-04-29 17:44:26.322758 (MainThread): 
Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
2020-04-29 17:44:26.322972 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f4a810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bca3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068bda50>]}
2020-04-29 17:44:26.323202 (MainThread): Flushing usage events
2020-05-04 19:15:40.080396 (MainThread): Running with dbt=0.16.1
2020-05-04 19:15:40.228681 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-04 19:15:40.229526 (MainThread): Tracking: tracking
2020-05-04 19:15:40.238137 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f7f4310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa3bc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f7c98d0>]}
2020-05-04 19:15:40.261068 (MainThread): Partial parsing not enabled
2020-05-04 19:15:40.264564 (MainThread): Parsing macros/core.sql
2020-05-04 19:15:40.271487 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-04 19:15:40.281075 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-04 19:15:40.284163 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-04 19:15:40.304880 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-04 19:15:40.343572 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-04 19:15:40.366470 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-04 19:15:40.371259 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-04 19:15:40.381269 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-04 19:15:40.397891 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-04 19:15:40.406068 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-04 19:15:40.413850 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-04 19:15:40.419845 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-04 19:15:40.421584 (MainThread): Parsing macros/etc/query.sql
2020-05-04 19:15:40.423434 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-04 19:15:40.425936 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-04 19:15:40.428889 (MainThread): Parsing macros/etc/datetime.sql
2020-05-04 19:15:40.439050 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-04 19:15:40.441863 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-04 19:15:40.444837 (MainThread): Parsing macros/adapters/common.sql
2020-05-04 19:15:40.490295 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-04 19:15:40.492615 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-04 19:15:40.494300 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-04 19:15:40.496229 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-04 19:15:40.499748 (MainThread): Parsing macros/catalog.sql
2020-05-04 19:15:40.503410 (MainThread): Parsing macros/relations.sql
2020-05-04 19:15:40.505993 (MainThread): Parsing macros/adapters.sql
2020-05-04 19:15:40.524910 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-04 19:15:40.543169 (MainThread): Partial parsing not enabled
2020-05-04 19:15:40.570773 (MainThread): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 19:15:40.570888 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:15:40.586805 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:15:40.586909 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:15:40.590827 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:15:40.590915 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:15:40.595165 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:15:40.595252 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:15:40.599131 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:15:40.599219 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:15:40.603795 (MainThread): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 19:15:40.603883 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:15:40.608690 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:15:40.608778 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:15:40.614822 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:15:40.614919 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:15:40.621289 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc38f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc3c550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc3c350>]}
2020-05-04 19:15:40.621471 (MainThread): Flushing usage events
2020-05-04 19:15:40.988425 (MainThread): Connection 'model.customer_history.order_flash_event' was properly closed.
2020-05-04 19:15:40.988671 (MainThread): Encountered an error:
2020-05-04 19:15:40.988884 (MainThread): Compilation Error in models/description.md
  Reached EOF without finding a close tag for docs (searched from line 1)
2020-05-04 19:15:41.006295 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 336, in load_all
    loader.load(internal_manifest=internal_manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 208, in load
    self.parse_project(project, macro_manifest, old_results)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 182, in parse_project
    self.parse_with_cache(path, parser, old_results)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 138, in parse_with_cache
    parser.parse_file(block)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/docs.py", line 90, in parse_file
    for block in searcher:
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/search.py", line 124, in __iter__
    for block in self.extract_blocks(entry):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/search.py", line 110, in extract_blocks
    collect_raw_data=False
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 443, in extract_toplevel_blocks
    collect_raw_data=collect_raw_data
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/_jinja_blocks.py", line 373, in lex_for_blocks
    collect_raw_data=collect_raw_data))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/_jinja_blocks.py", line 364, in find_blocks
    ).format(self.current.block_type_name, linecount))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in models/description.md
  Reached EOF without finding a close tag for docs (searched from line 1)

2020-05-04 19:16:07.964810 (MainThread): Running with dbt=0.16.1
2020-05-04 19:16:08.028748 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-04 19:16:08.029541 (MainThread): Tracking: tracking
2020-05-04 19:16:08.034317 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120149d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11229ad50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11229ae10>]}
2020-05-04 19:16:08.052596 (MainThread): Partial parsing not enabled
2020-05-04 19:16:08.054420 (MainThread): Parsing macros/core.sql
2020-05-04 19:16:08.058949 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-04 19:16:08.066987 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-04 19:16:08.068761 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-04 19:16:08.086932 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-04 19:16:08.120362 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-04 19:16:08.141675 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-04 19:16:08.143556 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-04 19:16:08.149837 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-04 19:16:08.162378 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-04 19:16:08.169289 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-04 19:16:08.175606 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-04 19:16:08.180752 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-04 19:16:08.181722 (MainThread): Parsing macros/etc/query.sql
2020-05-04 19:16:08.182814 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-04 19:16:08.184636 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-04 19:16:08.186736 (MainThread): Parsing macros/etc/datetime.sql
2020-05-04 19:16:08.195852 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-04 19:16:08.197868 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-04 19:16:08.198948 (MainThread): Parsing macros/adapters/common.sql
2020-05-04 19:16:08.248336 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-04 19:16:08.250287 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-04 19:16:08.251634 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-04 19:16:08.253175 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-04 19:16:08.256139 (MainThread): Parsing macros/catalog.sql
2020-05-04 19:16:08.258654 (MainThread): Parsing macros/relations.sql
2020-05-04 19:16:08.260100 (MainThread): Parsing macros/adapters.sql
2020-05-04 19:16:08.277374 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-04 19:16:08.294870 (MainThread): Partial parsing not enabled
2020-05-04 19:16:08.321721 (MainThread): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 19:16:08.321824 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:08.337988 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:16:08.338080 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:08.342025 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:16:08.342112 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:08.346426 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:16:08.346513 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:08.350452 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:16:08.350538 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:08.355130 (MainThread): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 19:16:08.355221 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:08.360148 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:16:08.360236 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:08.366030 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:16:08.366119 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:08.375237 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122eba50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112486c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112486f10>]}
2020-05-04 19:16:08.375408 (MainThread): Flushing usage events
2020-05-04 19:16:08.694434 (MainThread): Connection 'model.customer_history.order_flash_event' was properly closed.
2020-05-04 19:16:08.694683 (MainThread): Encountered an error:
2020-05-04 19:16:08.694870 (MainThread): Compilation Error
  The schema file at schema.yml is invalid because version 1 is not supported. Please consult the documentation for more information on schema.yml syntax:
  
  https://docs.getdbt.com/docs/schemayml-files
2020-05-04 19:16:08.699375 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 336, in load_all
    loader.load(internal_manifest=internal_manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 208, in load
    self.parse_project(project, macro_manifest, old_results)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 182, in parse_project
    self.parse_with_cache(path, parser, old_results)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 138, in parse_with_cache
    parser.parse_file(block)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/schemas.py", line 289, in parse_file
    self._parse_format_version(yaml_block)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/schemas.py", line 153, in _parse_format_version
    path, 'version {} is not supported'.format(version)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 764, in raise_invalid_schema_yml_version
    .format(path, issue)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error
  The schema file at schema.yml is invalid because version 1 is not supported. Please consult the documentation for more information on schema.yml syntax:
  
  https://docs.getdbt.com/docs/schemayml-files

2020-05-04 19:16:20.897969 (MainThread): Running with dbt=0.16.1
2020-05-04 19:16:20.961188 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-04 19:16:20.961910 (MainThread): Tracking: tracking
2020-05-04 19:16:20.967079 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056a61d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105901c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056a6510>]}
2020-05-04 19:16:20.988313 (MainThread): Partial parsing not enabled
2020-05-04 19:16:20.990543 (MainThread): Parsing macros/core.sql
2020-05-04 19:16:20.998443 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-04 19:16:21.010329 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-04 19:16:21.012214 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-04 19:16:21.030550 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-04 19:16:21.064410 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-04 19:16:21.086294 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-04 19:16:21.088263 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-04 19:16:21.094722 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-04 19:16:21.107915 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-04 19:16:21.114875 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-04 19:16:21.121161 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-04 19:16:21.126102 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-04 19:16:21.127048 (MainThread): Parsing macros/etc/query.sql
2020-05-04 19:16:21.128116 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-04 19:16:21.129770 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-04 19:16:21.131871 (MainThread): Parsing macros/etc/datetime.sql
2020-05-04 19:16:21.141053 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-04 19:16:21.143024 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-04 19:16:21.144073 (MainThread): Parsing macros/adapters/common.sql
2020-05-04 19:16:21.184927 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-04 19:16:21.186054 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-04 19:16:21.186958 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-04 19:16:21.188028 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-04 19:16:21.190256 (MainThread): Parsing macros/catalog.sql
2020-05-04 19:16:21.192743 (MainThread): Parsing macros/relations.sql
2020-05-04 19:16:21.194068 (MainThread): Parsing macros/adapters.sql
2020-05-04 19:16:21.210504 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-04 19:16:21.227772 (MainThread): Partial parsing not enabled
2020-05-04 19:16:21.255352 (MainThread): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 19:16:21.255456 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.270722 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:16:21.270806 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.274713 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:16:21.274799 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.278885 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:16:21.278968 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.282701 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:16:21.282783 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.287617 (MainThread): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 19:16:21.287718 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.292600 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:16:21.292689 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.298638 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:16:21.298896 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.449194 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-04 19:16:21.453689 (MainThread): 
2020-05-04 19:16:21.453988 (MainThread): Acquiring new postgres connection "master".
2020-05-04 19:16:21.454079 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:16:21.480414 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-04 19:16:21.480634 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-04 19:16:21.565076 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-04 19:16:21.565214 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-04 19:16:31.604891 (ThreadPoolExecutor-0_0): Got an error when attempting to open a postgres connection: 'timeout expired
'
2020-05-04 19:16:31.605347 (ThreadPoolExecutor-0_0): Error running SQL: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-04 19:16:31.605624 (ThreadPoolExecutor-0_0): Rolling back transaction.
2020-05-04 19:16:31.605807 (ThreadPoolExecutor-0_0): On list_data_platform_prod: No close available on handle
2020-05-04 19:16:31.606040 (ThreadPoolExecutor-0_0): Error running SQL: macro list_schemas
2020-05-04 19:16:31.606187 (ThreadPoolExecutor-0_0): Rolling back transaction.
2020-05-04 19:16:31.607070 (MainThread): Connection 'master' was properly closed.
2020-05-04 19:16:31.607260 (MainThread): Connection 'list_data_platform_prod' was properly closed.
2020-05-04 19:16:31.607457 (MainThread): ERROR: Database Error
  timeout expired
  
2020-05-04 19:16:31.607743 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059a1bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b7c7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a932d0>]}
2020-05-04 19:16:31.608054 (MainThread): Flushing usage events
2020-05-04 19:20:23.072423 (MainThread): Running with dbt=0.16.1
2020-05-04 19:20:23.137653 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-04 19:20:23.138373 (MainThread): Tracking: tracking
2020-05-04 19:20:23.143187 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe6ed50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc16210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc16510>]}
2020-05-04 19:20:23.161544 (MainThread): Partial parsing not enabled
2020-05-04 19:20:23.163380 (MainThread): Parsing macros/core.sql
2020-05-04 19:20:23.167928 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-04 19:20:23.176020 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-04 19:20:23.177788 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-04 19:20:23.196026 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-04 19:20:23.229659 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-04 19:20:23.250807 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-04 19:20:23.252696 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-04 19:20:23.258984 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-04 19:20:23.271683 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-04 19:20:23.278633 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-04 19:20:23.284935 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-04 19:20:23.289872 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-04 19:20:23.290825 (MainThread): Parsing macros/etc/query.sql
2020-05-04 19:20:23.291949 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-04 19:20:23.293623 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-04 19:20:23.295703 (MainThread): Parsing macros/etc/datetime.sql
2020-05-04 19:20:23.304847 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-04 19:20:23.306839 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-04 19:20:23.307906 (MainThread): Parsing macros/adapters/common.sql
2020-05-04 19:20:23.360568 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-04 19:20:23.361862 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-04 19:20:23.362850 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-04 19:20:23.364024 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-04 19:20:23.366421 (MainThread): Parsing macros/catalog.sql
2020-05-04 19:20:23.368941 (MainThread): Parsing macros/relations.sql
2020-05-04 19:20:23.370384 (MainThread): Parsing macros/adapters.sql
2020-05-04 19:20:23.387483 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-04 19:20:23.404858 (MainThread): Partial parsing not enabled
2020-05-04 19:20:23.431957 (MainThread): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 19:20:23.432058 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.448080 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:23.448172 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.452134 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:23.452228 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.456535 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:23.456624 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.460557 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:23.460646 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.465381 (MainThread): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 19:20:23.465492 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.471098 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:20:23.471197 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.477543 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:20:23.477638 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.617017 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-04 19:20:23.621627 (MainThread): 
2020-05-04 19:20:23.621919 (MainThread): Acquiring new postgres connection "master".
2020-05-04 19:20:23.622010 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:20:23.646119 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-04 19:20:23.646253 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-04 19:20:23.737015 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-04 19:20:23.737151 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-04 19:20:24.314665 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.58 seconds
2020-05-04 19:20:24.358216 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:20:24.358469 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-04 19:20:24.360761 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:20:24.360900 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-04 19:20:24.401960 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:20:24.402139 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:20:24.402241 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-04 19:20:25.565079 (ThreadPoolExecutor-1_0): SQL status: SELECT in 1.16 seconds
2020-05-04 19:20:25.571625 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-04 19:20:25.648041 (MainThread): Using postgres connection "master".
2020-05-04 19:20:25.648202 (MainThread): On master: BEGIN
2020-05-04 19:20:25.997508 (MainThread): SQL status: BEGIN in 0.35 seconds
2020-05-04 19:20:25.997941 (MainThread): Using postgres connection "master".
2020-05-04 19:20:25.998206 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-04 19:20:26.189155 (MainThread): SQL status: SELECT in 0.19 seconds
2020-05-04 19:20:26.257658 (MainThread): On master: ROLLBACK
2020-05-04 19:20:26.296170 (MainThread): Using postgres connection "master".
2020-05-04 19:20:26.296584 (MainThread): On master: BEGIN
2020-05-04 19:20:26.374749 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-05-04 19:20:26.375215 (MainThread): On master: COMMIT
2020-05-04 19:20:26.375533 (MainThread): Using postgres connection "master".
2020-05-04 19:20:26.375695 (MainThread): On master: COMMIT
2020-05-04 19:20:26.414243 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-04 19:20:26.415122 (MainThread): 12:20:26 | Concurrency: 1 threads (target='dev')
2020-05-04 19:20:26.415362 (MainThread): 12:20:26 | 
2020-05-04 19:20:26.419485 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-04 19:20:26.419949 (Thread-1): 12:20:26 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-05-04 19:20:26.420334 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:26.420472 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-04 19:20:26.420628 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-04 19:20:26.437100 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-04 19:20:26.437813 (Thread-1): finished collecting timing info
2020-05-04 19:20:26.479920 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:26.480087 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-04 19:20:26.554398 (Thread-1): SQL status: DROP VIEW in 0.07 seconds
2020-05-04 19:20:26.557752 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:26.557884 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 19:20:26.595515 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-04 19:20:26.598553 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-04 19:20:26.599352 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:26.599512 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-04 19:20:26.635995 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:20:26.636428 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:26.636711 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-04 19:20:27.922892 (Thread-1): SQL status: CREATE VIEW in 1.29 seconds
2020-05-04 19:20:27.929059 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:27.929220 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-04 19:20:28.049547 (Thread-1): SQL status: ALTER TABLE in 0.12 seconds
2020-05-04 19:20:28.052822 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:28.052951 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-04 19:20:28.094439 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:20:28.096412 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 19:20:28.096616 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:28.096779 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 19:20:29.785633 (Thread-1): SQL status: COMMIT in 1.69 seconds
2020-05-04 19:20:29.788341 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:20:29.788508 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 19:20:33.893837 (Thread-1): SQL status: DROP VIEW in 4.11 seconds
2020-05-04 19:20:33.897178 (Thread-1): finished collecting timing info
2020-05-04 19:20:33.898014 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '714d071c-be7b-4085-8541-4ad58fa9aeb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100241d0>]}
2020-05-04 19:20:33.898376 (Thread-1): 12:20:33 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 7.48s]
2020-05-04 19:20:33.898629 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-04 19:20:33.898973 (Thread-1): Began running node model.customer_history.stg_order
2020-05-04 19:20:33.899705 (Thread-1): 12:20:33 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-05-04 19:20:33.900714 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:33.900988 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-04 19:20:33.901359 (Thread-1): Compiling model.customer_history.stg_order
2020-05-04 19:20:33.913219 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-04 19:20:33.913677 (Thread-1): finished collecting timing info
2020-05-04 19:20:33.922561 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:33.922764 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-04 19:20:34.358258 (Thread-1): SQL status: DROP VIEW in 0.44 seconds
2020-05-04 19:20:34.362025 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:34.362209 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 19:20:34.534745 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-04 19:20:34.537819 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-04 19:20:34.538438 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:34.538599 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-04 19:20:34.575995 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:20:34.576247 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:34.576388 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-04 19:20:35.007242 (Thread-1): SQL status: CREATE VIEW in 0.43 seconds
2020-05-04 19:20:35.014588 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:35.014739 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-04 19:20:35.358443 (Thread-1): SQL status: ALTER TABLE in 0.34 seconds
2020-05-04 19:20:35.394083 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:35.394299 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-04 19:20:38.992465 (Thread-1): SQL status: ALTER TABLE in 3.60 seconds
2020-05-04 19:20:38.993944 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 19:20:38.994100 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:38.994230 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 19:20:39.346415 (Thread-1): SQL status: COMMIT in 0.35 seconds
2020-05-04 19:20:39.348958 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:20:39.349118 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 19:20:39.815047 (Thread-1): SQL status: DROP VIEW in 0.47 seconds
2020-05-04 19:20:39.819289 (Thread-1): finished collecting timing info
2020-05-04 19:20:39.820131 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '714d071c-be7b-4085-8541-4ad58fa9aeb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110485390>]}
2020-05-04 19:20:39.820438 (Thread-1): 12:20:39 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 5.92s]
2020-05-04 19:20:39.820619 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-04 19:20:39.820806 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-04 19:20:39.821039 (Thread-1): 12:20:39 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-05-04 19:20:39.821837 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:39.822103 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-04 19:20:39.822336 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-04 19:20:39.828854 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-04 19:20:39.829331 (Thread-1): finished collecting timing info
2020-05-04 19:20:39.836737 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:39.836870 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-04 19:20:40.028478 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-05-04 19:20:40.032793 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:40.032997 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 19:20:40.247262 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-05-04 19:20:40.250001 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-04 19:20:40.250648 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:40.250805 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-04 19:20:40.288524 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:20:40.288957 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:40.289231 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-05-04 19:20:42.833248 (Thread-1): SQL status: CREATE VIEW in 2.54 seconds
2020-05-04 19:20:42.838724 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:42.838896 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-04 19:20:44.093838 (Thread-1): SQL status: ALTER TABLE in 1.25 seconds
2020-05-04 19:20:44.098073 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:44.098230 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-04 19:20:44.832154 (Thread-1): SQL status: ALTER TABLE in 0.73 seconds
2020-05-04 19:20:44.834087 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 19:20:44.834288 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:44.834451 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 19:20:48.778012 (Thread-1): SQL status: COMMIT in 3.94 seconds
2020-05-04 19:20:48.782640 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:20:48.782793 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 19:20:49.261042 (Thread-1): SQL status: DROP VIEW in 0.48 seconds
2020-05-04 19:20:49.265234 (Thread-1): finished collecting timing info
2020-05-04 19:20:49.266132 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '714d071c-be7b-4085-8541-4ad58fa9aeb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110093150>]}
2020-05-04 19:20:49.266459 (Thread-1): 12:20:49 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 9.44s]
2020-05-04 19:20:49.266640 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-04 19:20:49.266823 (Thread-1): Began running node model.customer_history.stg_events
2020-05-04 19:20:49.267065 (Thread-1): 12:20:49 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-05-04 19:20:49.267795 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:49.267947 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-04 19:20:49.268082 (Thread-1): Compiling model.customer_history.stg_events
2020-05-04 19:20:49.275165 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-04 19:20:49.275650 (Thread-1): finished collecting timing info
2020-05-04 19:20:49.283314 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:49.283470 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-04 19:20:49.850261 (Thread-1): SQL status: DROP VIEW in 0.57 seconds
2020-05-04 19:20:49.854367 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:49.854521 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 19:20:50.261440 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-05-04 19:20:50.264370 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-04 19:20:50.264936 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:50.265093 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-04 19:20:50.303344 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:20:50.303785 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:50.304075 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-04 19:20:50.674080 (Thread-1): SQL status: CREATE VIEW in 0.37 seconds
2020-05-04 19:20:50.677611 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:50.677770 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-04 19:20:50.824657 (Thread-1): SQL status: ALTER TABLE in 0.15 seconds
2020-05-04 19:20:50.826046 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 19:20:50.826207 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:50.826334 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 19:20:51.416823 (Thread-1): SQL status: COMMIT in 0.59 seconds
2020-05-04 19:20:51.419877 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:20:51.420066 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 19:20:51.593668 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-04 19:20:51.596503 (Thread-1): finished collecting timing info
2020-05-04 19:20:51.597166 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '714d071c-be7b-4085-8541-4ad58fa9aeb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff0ce50>]}
2020-05-04 19:20:51.597407 (Thread-1): 12:20:51 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 2.33s]
2020-05-04 19:20:51.597546 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-04 19:20:51.597703 (Thread-1): Began running node model.customer_history.order_flash
2020-05-04 19:20:51.598093 (Thread-1): 12:20:51 | 5 of 8 START table model data_science.order_flash.................... [RUN]
2020-05-04 19:20:51.598617 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:20:51.598731 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-04 19:20:51.598836 (Thread-1): Compiling model.customer_history.order_flash
2020-05-04 19:20:51.607061 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash"
2020-05-04 19:20:51.607563 (Thread-1): finished collecting timing info
2020-05-04 19:20:51.630757 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:20:51.630937 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop table if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-05-04 19:20:51.674528 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2020-05-04 19:20:51.677418 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:20:51.677598 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop table if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-05-04 19:20:52.354668 (Thread-1): SQL status: DROP TABLE in 0.68 seconds
2020-05-04 19:20:52.357617 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash"
2020-05-04 19:20:52.358319 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:20:52.358471 (Thread-1): On model.customer_history.order_flash: BEGIN
2020-05-04 19:20:52.396323 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:20:52.396627 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:20:52.396808 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */


  create  table "data_platform_prod"."data_science"."order_flash__dbt_tmp"
  as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );
2020-05-04 19:22:01.039638 (MainThread): Cancelling query 'model.customer_history.order_flash' (9296)
2020-05-04 19:22:01.039847 (MainThread): Using postgres connection "master".
2020-05-04 19:22:01.039964 (MainThread): On master: BEGIN
2020-05-04 19:22:01.080572 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:22:01.080872 (MainThread): Using postgres connection "master".
2020-05-04 19:22:01.081051 (MainThread): On master: select pg_terminate_backend(9296)
2020-05-04 19:22:01.121959 (MainThread): SQL status: SELECT in 0.04 seconds
2020-05-04 19:22:01.122401 (MainThread): Cancel query 'model.customer_history.order_flash': (1,)
2020-05-04 19:22:01.122755 (MainThread): 12:22:01 | CANCEL query model.customer_history.order_flash...................... [CANCEL]
2020-05-04 19:22:01.731173 (Thread-1): Postgres error: Query (14320962) cancelled on user's request

2020-05-04 19:22:01.731558 (Thread-1): On model.customer_history.order_flash: ROLLBACK
2020-05-04 19:22:01.731993 (Thread-1): Failed to rollback model.customer_history.order_flash
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.QueryCanceled: Query (14320962) cancelled on user's request


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/connections.py", line 221, in _rollback_handle
    connection.handle.rollback()
psycopg2.OperationalError: SSL connection has been closed unexpectedly
2020-05-04 19:22:01.739271 (Thread-1): finished collecting timing info
2020-05-04 19:22:01.739897 (Thread-1): Database Error in model order_flash (models/intermediate/order_flash.sql)
  Query (14320962) cancelled on user's request
  compiled SQL at target/run/customer_history/intermediate/order_flash.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.QueryCanceled: Query (14320962) cancelled on user's request


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 62, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash (models/intermediate/order_flash.sql)
  Query (14320962) cancelled on user's request
  compiled SQL at target/run/customer_history/intermediate/order_flash.sql
2020-05-04 19:22:01.759472 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '714d071c-be7b-4085-8541-4ad58fa9aeb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff17dd0>]}
2020-05-04 19:22:01.759760 (Thread-1): 12:22:01 | 5 of 8 ERROR creating table model data_science.order_flash........... [ERROR in 70.16s]
2020-05-04 19:22:01.759923 (Thread-1): Finished running node model.customer_history.order_flash
2020-05-04 19:22:01.760204 (MainThread): 
2020-05-04 19:22:01.760384 (MainThread): Exited because of keyboard interrupt.
2020-05-04 19:22:01.760532 (MainThread): 
Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
2020-05-04 19:22:01.760677 (MainThread): Connection 'master' was left open.
2020-05-04 19:22:01.800668 (MainThread): On master: Close
2020-05-04 19:22:01.801193 (MainThread): Connection 'model.customer_history.order_flash' was left open.
2020-05-04 19:22:01.801433 (MainThread): On model.customer_history.order_flash: Close
2020-05-04 19:22:01.801631 (MainThread): Flushing usage events
2020-05-04 19:22:02.720290 (MainThread): ctrl-c
2020-05-04 19:25:37.941956 (MainThread): Running with dbt=0.16.1
2020-05-04 19:25:38.005069 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-04 19:25:38.006320 (MainThread): Tracking: tracking
2020-05-04 19:25:38.012711 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eaf1e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed7fc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed7fcd0>]}
2020-05-04 19:25:38.033615 (MainThread): Partial parsing not enabled
2020-05-04 19:25:38.036873 (MainThread): Parsing macros/core.sql
2020-05-04 19:25:38.045354 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-04 19:25:38.055038 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-04 19:25:38.056901 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-04 19:25:38.075308 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-04 19:25:38.108562 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-04 19:25:38.130641 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-04 19:25:38.132627 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-04 19:25:38.139177 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-04 19:25:38.152446 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-04 19:25:38.159543 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-04 19:25:38.166071 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-04 19:25:38.171263 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-04 19:25:38.172266 (MainThread): Parsing macros/etc/query.sql
2020-05-04 19:25:38.173391 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-04 19:25:38.175144 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-04 19:25:38.177313 (MainThread): Parsing macros/etc/datetime.sql
2020-05-04 19:25:38.186756 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-04 19:25:38.188844 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-04 19:25:38.189956 (MainThread): Parsing macros/adapters/common.sql
2020-05-04 19:25:38.231627 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-04 19:25:38.232779 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-04 19:25:38.233687 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-04 19:25:38.234760 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-04 19:25:38.236960 (MainThread): Parsing macros/catalog.sql
2020-05-04 19:25:38.239246 (MainThread): Parsing macros/relations.sql
2020-05-04 19:25:38.240579 (MainThread): Parsing macros/adapters.sql
2020-05-04 19:25:38.257350 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-04 19:25:38.274462 (MainThread): Partial parsing not enabled
2020-05-04 19:25:38.300757 (MainThread): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 19:25:38.300850 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.316180 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:38.316268 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.320009 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:38.320093 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.324201 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:38.324287 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.328119 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:38.328203 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.332804 (MainThread): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 19:25:38.332889 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.337872 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:25:38.337960 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.343993 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:25:38.344095 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.494129 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-04 19:25:38.498001 (MainThread): 
2020-05-04 19:25:38.498280 (MainThread): Acquiring new postgres connection "master".
2020-05-04 19:25:38.498368 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:25:38.523053 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-04 19:25:38.523321 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-04 19:25:38.606229 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-04 19:25:38.606370 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-04 19:25:39.105963 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.50 seconds
2020-05-04 19:25:39.138300 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:25:39.138428 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-04 19:25:39.140144 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:25:39.140243 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-04 19:25:39.179636 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:25:39.180064 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:25:39.180336 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-04 19:25:39.415476 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.23 seconds
2020-05-04 19:25:39.421366 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-04 19:25:39.495689 (MainThread): Using postgres connection "master".
2020-05-04 19:25:39.495814 (MainThread): On master: BEGIN
2020-05-04 19:25:39.889359 (MainThread): SQL status: BEGIN in 0.39 seconds
2020-05-04 19:25:39.889592 (MainThread): Using postgres connection "master".
2020-05-04 19:25:39.889722 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-04 19:25:40.113936 (MainThread): SQL status: SELECT in 0.22 seconds
2020-05-04 19:25:40.199143 (MainThread): On master: ROLLBACK
2020-05-04 19:25:40.239743 (MainThread): Using postgres connection "master".
2020-05-04 19:25:40.240169 (MainThread): On master: BEGIN
2020-05-04 19:25:40.329771 (MainThread): SQL status: BEGIN in 0.09 seconds
2020-05-04 19:25:40.330227 (MainThread): On master: COMMIT
2020-05-04 19:25:40.330516 (MainThread): Using postgres connection "master".
2020-05-04 19:25:40.330689 (MainThread): On master: COMMIT
2020-05-04 19:25:40.370364 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-04 19:25:40.371261 (MainThread): 12:25:40 | Concurrency: 1 threads (target='dev')
2020-05-04 19:25:40.371508 (MainThread): 12:25:40 | 
2020-05-04 19:25:40.374127 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-04 19:25:40.374407 (Thread-1): 12:25:40 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-05-04 19:25:40.374803 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:40.374947 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-04 19:25:40.375095 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-04 19:25:40.392051 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-04 19:25:40.392589 (Thread-1): finished collecting timing info
2020-05-04 19:25:40.433978 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:40.434142 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-04 19:25:40.514707 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-04 19:25:40.519396 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:40.519582 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 19:25:40.558900 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-04 19:25:40.560628 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-04 19:25:40.561040 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:40.561156 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-04 19:25:40.601788 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:25:40.602218 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:40.602484 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-04 19:25:41.075512 (Thread-1): SQL status: CREATE VIEW in 0.47 seconds
2020-05-04 19:25:41.083787 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:41.084035 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-04 19:25:41.132594 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-04 19:25:41.136424 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:41.136580 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-04 19:25:41.178317 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:25:41.179985 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 19:25:41.180160 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:41.180285 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 19:25:42.928930 (Thread-1): SQL status: COMMIT in 1.75 seconds
2020-05-04 19:25:42.931718 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:25:42.931898 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 19:25:43.240491 (Thread-1): SQL status: DROP VIEW in 0.31 seconds
2020-05-04 19:25:43.244580 (Thread-1): finished collecting timing info
2020-05-04 19:25:43.245506 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e78b1512-5fcc-4d6a-81a2-6c94a870c34c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee20a10>]}
2020-05-04 19:25:43.245842 (Thread-1): 12:25:43 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 2.87s]
2020-05-04 19:25:43.246035 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-04 19:25:43.246226 (Thread-1): Began running node model.customer_history.stg_order
2020-05-04 19:25:43.246594 (Thread-1): 12:25:43 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-05-04 19:25:43.247028 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:43.247160 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-04 19:25:43.247286 (Thread-1): Compiling model.customer_history.stg_order
2020-05-04 19:25:43.253667 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-04 19:25:43.254123 (Thread-1): finished collecting timing info
2020-05-04 19:25:43.262074 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:43.262255 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-04 19:25:45.200594 (Thread-1): SQL status: DROP VIEW in 1.94 seconds
2020-05-04 19:25:45.204806 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:45.204963 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 19:25:47.206718 (Thread-1): SQL status: DROP VIEW in 2.00 seconds
2020-05-04 19:25:47.209718 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-04 19:25:47.210400 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:47.210539 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-04 19:25:47.250933 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:25:47.251364 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:47.251645 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-04 19:25:47.352402 (Thread-1): SQL status: CREATE VIEW in 0.10 seconds
2020-05-04 19:25:47.359743 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:47.359877 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-04 19:25:47.405142 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-04 19:25:47.440325 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:47.440536 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-04 19:25:47.482121 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:25:47.484173 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 19:25:47.484370 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:47.484530 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 19:25:48.028052 (Thread-1): SQL status: COMMIT in 0.54 seconds
2020-05-04 19:25:48.031478 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:25:48.031631 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 19:25:50.020049 (Thread-1): SQL status: DROP VIEW in 1.99 seconds
2020-05-04 19:25:50.024335 (Thread-1): finished collecting timing info
2020-05-04 19:25:50.025179 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e78b1512-5fcc-4d6a-81a2-6c94a870c34c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f38ec90>]}
2020-05-04 19:25:50.025487 (Thread-1): 12:25:50 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 6.78s]
2020-05-04 19:25:50.025669 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-04 19:25:50.025857 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-04 19:25:50.026056 (Thread-1): 12:25:50 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-05-04 19:25:50.026888 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:50.027223 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-04 19:25:50.027391 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-04 19:25:50.033641 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-04 19:25:50.034095 (Thread-1): finished collecting timing info
2020-05-04 19:25:50.041539 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:50.041671 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-04 19:25:50.776486 (Thread-1): SQL status: DROP VIEW in 0.73 seconds
2020-05-04 19:25:50.780786 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:50.780943 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 19:25:54.099412 (Thread-1): SQL status: DROP VIEW in 3.32 seconds
2020-05-04 19:25:54.101773 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-04 19:25:54.102372 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:54.102544 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-04 19:25:54.142368 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:25:54.142541 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:54.142639 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-05-04 19:25:54.489971 (Thread-1): SQL status: CREATE VIEW in 0.35 seconds
2020-05-04 19:25:54.496136 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:54.496293 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-04 19:25:54.539905 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:25:54.543526 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:54.543675 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-04 19:25:54.584894 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:25:54.586861 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 19:25:54.587059 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:54.587221 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 19:25:56.075673 (Thread-1): SQL status: COMMIT in 1.49 seconds
2020-05-04 19:25:56.079687 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:25:56.079843 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 19:25:57.138377 (Thread-1): SQL status: DROP VIEW in 1.06 seconds
2020-05-04 19:25:57.142508 (Thread-1): finished collecting timing info
2020-05-04 19:25:57.143336 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e78b1512-5fcc-4d6a-81a2-6c94a870c34c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f00cb10>]}
2020-05-04 19:25:57.143637 (Thread-1): 12:25:57 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 7.12s]
2020-05-04 19:25:57.143818 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-04 19:25:57.144003 (Thread-1): Began running node model.customer_history.stg_events
2020-05-04 19:25:57.144459 (Thread-1): 12:25:57 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-05-04 19:25:57.145054 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:57.145200 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-04 19:25:57.145318 (Thread-1): Compiling model.customer_history.stg_events
2020-05-04 19:25:57.151515 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-04 19:25:57.151986 (Thread-1): finished collecting timing info
2020-05-04 19:25:57.159387 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:57.159518 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-04 19:25:57.651142 (Thread-1): SQL status: DROP VIEW in 0.49 seconds
2020-05-04 19:25:57.653618 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:57.653733 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 19:25:57.831894 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-04 19:25:57.834916 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-04 19:25:57.835556 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:57.835718 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-04 19:25:57.876267 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:25:57.876700 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:57.876983 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-04 19:25:58.222606 (Thread-1): SQL status: CREATE VIEW in 0.35 seconds
2020-05-04 19:25:58.228382 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:58.228557 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-04 19:25:58.272809 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:25:58.277101 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:58.277270 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-04 19:25:58.317943 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:25:58.319325 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 19:25:58.319485 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:58.319612 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 19:25:58.756683 (Thread-1): SQL status: COMMIT in 0.44 seconds
2020-05-04 19:25:58.760283 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:25:58.760438 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 19:25:58.938110 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-04 19:25:58.942568 (Thread-1): finished collecting timing info
2020-05-04 19:25:58.943435 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e78b1512-5fcc-4d6a-81a2-6c94a870c34c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f00cb10>]}
2020-05-04 19:25:58.943750 (Thread-1): 12:25:58 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 1.80s]
2020-05-04 19:25:58.943937 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-04 19:25:58.944178 (Thread-1): Began running node model.customer_history.order_flash
2020-05-04 19:25:58.944582 (Thread-1): 12:25:58 | 5 of 8 START table model data_science.order_flash.................... [RUN]
2020-05-04 19:25:58.945090 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:25:58.945255 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-04 19:25:58.945562 (Thread-1): Compiling model.customer_history.order_flash
2020-05-04 19:25:58.956202 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash"
2020-05-04 19:25:58.956762 (Thread-1): finished collecting timing info
2020-05-04 19:25:58.981233 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:25:58.981425 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop table if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-05-04 19:25:59.344972 (Thread-1): SQL status: DROP TABLE in 0.36 seconds
2020-05-04 19:25:59.349138 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:25:59.349296 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop table if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-05-04 19:25:59.413379 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2020-05-04 19:25:59.415746 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash"
2020-05-04 19:25:59.416326 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:25:59.416454 (Thread-1): On model.customer_history.order_flash: BEGIN
2020-05-04 19:25:59.456236 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:25:59.456446 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:25:59.456556 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */


  create  table "data_platform_prod"."data_science"."order_flash__dbt_tmp"
  as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );
2020-05-04 19:29:28.386458 (MainThread): Cancelling query 'model.customer_history.order_flash' (10066)
2020-05-04 19:29:28.386696 (MainThread): Using postgres connection "master".
2020-05-04 19:29:28.386834 (MainThread): On master: BEGIN
2020-05-04 19:29:28.428147 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:29:28.428429 (MainThread): Using postgres connection "master".
2020-05-04 19:29:28.428599 (MainThread): On master: select pg_terminate_backend(10066)
2020-05-04 19:29:28.470796 (MainThread): SQL status: SELECT in 0.04 seconds
2020-05-04 19:29:28.471228 (MainThread): Cancel query 'model.customer_history.order_flash': (1,)
2020-05-04 19:29:28.471572 (MainThread): 12:29:28 | CANCEL query model.customer_history.order_flash...................... [CANCEL]
2020-05-04 19:29:28.556374 (Thread-1): Postgres error: Query (14322285) cancelled on user's request

2020-05-04 19:29:28.556645 (Thread-1): On model.customer_history.order_flash: ROLLBACK
2020-05-04 19:29:28.567275 (Thread-1): Failed to rollback model.customer_history.order_flash
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.QueryCanceled: Query (14322285) cancelled on user's request


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/connections.py", line 221, in _rollback_handle
    connection.handle.rollback()
psycopg2.OperationalError: SSL connection has been closed unexpectedly
2020-05-04 19:29:28.568662 (Thread-1): finished collecting timing info
2020-05-04 19:29:28.569394 (Thread-1): Database Error in model order_flash (models/intermediate/order_flash.sql)
  Query (14322285) cancelled on user's request
  compiled SQL at target/run/customer_history/intermediate/order_flash.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.QueryCanceled: Query (14322285) cancelled on user's request


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 62, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash (models/intermediate/order_flash.sql)
  Query (14322285) cancelled on user's request
  compiled SQL at target/run/customer_history/intermediate/order_flash.sql
2020-05-04 19:29:28.571683 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e78b1512-5fcc-4d6a-81a2-6c94a870c34c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee1e5d0>]}
2020-05-04 19:29:28.571979 (Thread-1): 12:29:28 | 5 of 8 ERROR creating table model data_science.order_flash........... [ERROR in 209.63s]
2020-05-04 19:29:28.572163 (Thread-1): Finished running node model.customer_history.order_flash
2020-05-04 19:29:28.572464 (MainThread): 
2020-05-04 19:29:28.572647 (MainThread): Exited because of keyboard interrupt.
2020-05-04 19:29:28.572810 (MainThread): 
Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
2020-05-04 19:29:28.572972 (MainThread): Connection 'master' was left open.
2020-05-04 19:29:28.615673 (MainThread): On master: Close
2020-05-04 19:29:28.616169 (MainThread): Connection 'model.customer_history.order_flash' was left open.
2020-05-04 19:29:28.616352 (MainThread): On model.customer_history.order_flash: Close
2020-05-04 19:29:28.616547 (MainThread): Flushing usage events
2020-05-04 19:29:29.067517 (MainThread): ctrl-c
2020-05-04 19:33:14.032650 (MainThread): Running with dbt=0.16.1
2020-05-04 19:33:14.116911 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-04 19:33:14.117766 (MainThread): Tracking: tracking
2020-05-04 19:33:14.123296 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070e3fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107372b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070ec9d0>]}
2020-05-04 19:33:14.141604 (MainThread): Partial parsing not enabled
2020-05-04 19:33:14.144148 (MainThread): Parsing macros/core.sql
2020-05-04 19:33:14.149826 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-04 19:33:14.158472 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-04 19:33:14.161462 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-04 19:33:14.180904 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-04 19:33:14.215746 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-04 19:33:14.241721 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-04 19:33:14.244752 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-04 19:33:14.252031 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-04 19:33:14.265700 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-04 19:33:14.273559 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-04 19:33:14.281500 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-04 19:33:14.290828 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-04 19:33:14.292855 (MainThread): Parsing macros/etc/query.sql
2020-05-04 19:33:14.295059 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-04 19:33:14.297973 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-04 19:33:14.301339 (MainThread): Parsing macros/etc/datetime.sql
2020-05-04 19:33:14.313133 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-04 19:33:14.316434 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-04 19:33:14.319097 (MainThread): Parsing macros/adapters/common.sql
2020-05-04 19:33:14.371294 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-04 19:33:14.374670 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-04 19:33:14.377503 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-04 19:33:14.380570 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-04 19:33:14.385308 (MainThread): Parsing macros/catalog.sql
2020-05-04 19:33:14.390293 (MainThread): Parsing macros/relations.sql
2020-05-04 19:33:14.393385 (MainThread): Parsing macros/adapters.sql
2020-05-04 19:33:14.413931 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-04 19:33:14.432047 (MainThread): Partial parsing not enabled
2020-05-04 19:33:14.459975 (MainThread): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 19:33:14.460079 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.477293 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:14.477388 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.482276 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:14.482366 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.487640 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:14.487731 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.492420 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:14.492512 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.497264 (MainThread): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:14.497351 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.502925 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:14.503016 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.509633 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:14.509725 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.663355 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-04 19:33:14.668141 (MainThread): 
2020-05-04 19:33:14.668505 (MainThread): Acquiring new postgres connection "master".
2020-05-04 19:33:14.668613 (MainThread): Opening a new connection, currently in state init
2020-05-04 19:33:14.694968 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-04 19:33:14.695108 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-04 19:33:14.777924 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-04 19:33:14.778062 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-04 19:33:15.241800 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.46 seconds
2020-05-04 19:33:15.281824 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:33:15.282030 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-04 19:33:15.284025 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:33:15.284139 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-04 19:33:15.326355 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:33:15.326770 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 19:33:15.326947 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-04 19:33:15.490658 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.16 seconds
2020-05-04 19:33:15.497010 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-04 19:33:15.574918 (MainThread): Using postgres connection "master".
2020-05-04 19:33:15.575087 (MainThread): On master: BEGIN
2020-05-04 19:33:15.936012 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-05-04 19:33:15.936438 (MainThread): Using postgres connection "master".
2020-05-04 19:33:15.936704 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-04 19:33:16.159966 (MainThread): SQL status: SELECT in 0.22 seconds
2020-05-04 19:33:16.233638 (MainThread): On master: ROLLBACK
2020-05-04 19:33:16.273283 (MainThread): Using postgres connection "master".
2020-05-04 19:33:16.273696 (MainThread): On master: BEGIN
2020-05-04 19:33:16.353224 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-05-04 19:33:16.353686 (MainThread): On master: COMMIT
2020-05-04 19:33:16.354023 (MainThread): Using postgres connection "master".
2020-05-04 19:33:16.354185 (MainThread): On master: COMMIT
2020-05-04 19:33:16.393818 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-04 19:33:16.394725 (MainThread): 12:33:16 | Concurrency: 1 threads (target='dev')
2020-05-04 19:33:16.394997 (MainThread): 12:33:16 | 
2020-05-04 19:33:16.398198 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-04 19:33:16.398455 (Thread-1): 12:33:16 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-05-04 19:33:16.398835 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:16.398971 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-04 19:33:16.399115 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-04 19:33:16.416005 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-04 19:33:16.416574 (Thread-1): finished collecting timing info
2020-05-04 19:33:16.461849 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:16.462016 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-04 19:33:16.540903 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-04 19:33:16.545237 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:16.545391 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 19:33:16.586260 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-04 19:33:16.589359 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-04 19:33:16.589997 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:16.590148 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-04 19:33:16.630702 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:33:16.631005 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:16.631181 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-04 19:33:16.687871 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-05-04 19:33:16.694163 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:16.694324 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-04 19:33:16.734159 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:16.738499 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:16.738651 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-04 19:33:16.778843 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:16.780241 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 19:33:16.780395 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:16.780520 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 19:33:18.196871 (Thread-1): SQL status: COMMIT in 1.42 seconds
2020-05-04 19:33:18.200358 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 19:33:18.200526 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 19:33:19.863553 (Thread-1): SQL status: DROP VIEW in 1.66 seconds
2020-05-04 19:33:19.867912 (Thread-1): finished collecting timing info
2020-05-04 19:33:19.868793 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd894006-a09a-4dda-8a74-0bbdbe2841ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10753df90>]}
2020-05-04 19:33:19.869118 (Thread-1): 12:33:19 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 3.47s]
2020-05-04 19:33:19.869306 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-04 19:33:19.869501 (Thread-1): Began running node model.customer_history.stg_order
2020-05-04 19:33:19.869800 (Thread-1): 12:33:19 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-05-04 19:33:19.870470 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:19.870661 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-04 19:33:19.870814 (Thread-1): Compiling model.customer_history.stg_order
2020-05-04 19:33:19.877722 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-04 19:33:19.878240 (Thread-1): finished collecting timing info
2020-05-04 19:33:19.885927 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:19.886062 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-04 19:33:21.253358 (Thread-1): SQL status: DROP VIEW in 1.37 seconds
2020-05-04 19:33:21.257640 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:21.257844 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 19:33:23.211774 (Thread-1): SQL status: DROP VIEW in 1.95 seconds
2020-05-04 19:33:23.214860 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-04 19:33:23.215499 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:23.215661 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-04 19:33:23.255577 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:33:23.256006 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:23.256272 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-04 19:33:23.653340 (Thread-1): SQL status: CREATE VIEW in 0.40 seconds
2020-05-04 19:33:23.661688 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:23.661902 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-04 19:33:23.702620 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:23.732891 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:23.733079 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-04 19:33:23.773463 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:23.775507 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 19:33:23.775702 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:23.775861 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 19:33:24.604076 (Thread-1): SQL status: COMMIT in 0.83 seconds
2020-05-04 19:33:24.607478 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 19:33:24.607636 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 19:33:25.283106 (Thread-1): SQL status: DROP VIEW in 0.68 seconds
2020-05-04 19:33:25.287397 (Thread-1): finished collecting timing info
2020-05-04 19:33:25.288274 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd894006-a09a-4dda-8a74-0bbdbe2841ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10737b610>]}
2020-05-04 19:33:25.288598 (Thread-1): 12:33:25 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 5.42s]
2020-05-04 19:33:25.288775 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-04 19:33:25.288958 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-04 19:33:25.289573 (Thread-1): 12:33:25 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-05-04 19:33:25.290333 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:25.290482 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-04 19:33:25.290633 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-04 19:33:25.297102 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-04 19:33:25.297547 (Thread-1): finished collecting timing info
2020-05-04 19:33:25.304955 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:25.305085 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-04 19:33:25.532169 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-05-04 19:33:25.536433 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:25.536587 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 19:33:25.735980 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-05-04 19:33:25.738818 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-04 19:33:25.739539 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:25.739738 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-04 19:33:25.779206 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:33:25.779638 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:25.779905 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-05-04 19:33:26.225979 (Thread-1): SQL status: CREATE VIEW in 0.45 seconds
2020-05-04 19:33:26.232310 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:26.232461 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-04 19:33:26.277807 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-04 19:33:26.282074 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:26.282223 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-04 19:33:26.322621 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:26.324590 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 19:33:26.324785 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:26.324942 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 19:33:26.810773 (Thread-1): SQL status: COMMIT in 0.49 seconds
2020-05-04 19:33:26.814921 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 19:33:26.815087 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 19:33:27.329731 (Thread-1): SQL status: DROP VIEW in 0.51 seconds
2020-05-04 19:33:27.333967 (Thread-1): finished collecting timing info
2020-05-04 19:33:27.334793 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd894006-a09a-4dda-8a74-0bbdbe2841ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107635350>]}
2020-05-04 19:33:27.335094 (Thread-1): 12:33:27 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 2.04s]
2020-05-04 19:33:27.335272 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-04 19:33:27.335455 (Thread-1): Began running node model.customer_history.stg_events
2020-05-04 19:33:27.335823 (Thread-1): 12:33:27 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-05-04 19:33:27.336375 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:27.336725 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-04 19:33:27.336891 (Thread-1): Compiling model.customer_history.stg_events
2020-05-04 19:33:27.343283 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-04 19:33:27.343730 (Thread-1): finished collecting timing info
2020-05-04 19:33:27.351334 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:27.351468 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-04 19:33:27.903875 (Thread-1): SQL status: DROP VIEW in 0.55 seconds
2020-05-04 19:33:27.907375 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:27.907529 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 19:33:28.081147 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-04 19:33:28.084160 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-04 19:33:28.084801 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:28.084966 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-04 19:33:28.192465 (Thread-1): SQL status: BEGIN in 0.11 seconds
2020-05-04 19:33:28.192871 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:28.193126 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-04 19:33:28.719780 (Thread-1): SQL status: CREATE VIEW in 0.53 seconds
2020-05-04 19:33:28.725982 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:28.726138 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-04 19:33:28.769510 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:28.773754 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:28.773907 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-04 19:33:28.814279 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:28.816206 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 19:33:28.816483 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:28.816677 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 19:33:29.259368 (Thread-1): SQL status: COMMIT in 0.44 seconds
2020-05-04 19:33:29.262932 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 19:33:29.263093 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 19:33:29.766739 (Thread-1): SQL status: DROP VIEW in 0.50 seconds
2020-05-04 19:33:29.771036 (Thread-1): finished collecting timing info
2020-05-04 19:33:29.771886 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd894006-a09a-4dda-8a74-0bbdbe2841ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107595350>]}
2020-05-04 19:33:29.772191 (Thread-1): 12:33:29 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 2.44s]
2020-05-04 19:33:29.772371 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-04 19:33:29.772580 (Thread-1): Began running node model.customer_history.order_flash
2020-05-04 19:33:29.773006 (Thread-1): 12:33:29 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-05-04 19:33:29.773638 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:29.773806 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-04 19:33:29.773924 (Thread-1): Compiling model.customer_history.order_flash
2020-05-04 19:33:29.783780 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash"
2020-05-04 19:33:29.784223 (Thread-1): finished collecting timing info
2020-05-04 19:33:29.791335 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:29.791466 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-05-04 19:33:29.970336 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-04 19:33:29.974490 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:29.974642 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-05-04 19:33:30.494153 (Thread-1): SQL status: DROP VIEW in 0.52 seconds
2020-05-04 19:33:30.497186 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash"
2020-05-04 19:33:30.497812 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:30.497964 (Thread-1): On model.customer_history.order_flash: BEGIN
2020-05-04 19:33:30.537768 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:33:30.538204 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:30.538481 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-05-04 19:33:30.812674 (Thread-1): SQL status: CREATE VIEW in 0.27 seconds
2020-05-04 19:33:30.815236 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:30.815349 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-05-04 19:33:30.855797 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:30.857746 (Thread-1): On model.customer_history.order_flash: COMMIT
2020-05-04 19:33:30.857969 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:30.858131 (Thread-1): On model.customer_history.order_flash: COMMIT
2020-05-04 19:33:31.336387 (Thread-1): SQL status: COMMIT in 0.48 seconds
2020-05-04 19:33:31.339794 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 19:33:31.339942 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-05-04 19:33:31.861351 (Thread-1): SQL status: DROP VIEW in 0.52 seconds
2020-05-04 19:33:31.865619 (Thread-1): finished collecting timing info
2020-05-04 19:33:31.866533 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd894006-a09a-4dda-8a74-0bbdbe2841ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075a2cd0>]}
2020-05-04 19:33:31.866895 (Thread-1): 12:33:31 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 2.09s]
2020-05-04 19:33:31.867088 (Thread-1): Finished running node model.customer_history.order_flash
2020-05-04 19:33:31.867303 (Thread-1): Began running node model.customer_history.customer_broker
2020-05-04 19:33:31.867685 (Thread-1): 12:33:31 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-05-04 19:33:31.868306 (Thread-1): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:31.868524 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash).
2020-05-04 19:33:31.868740 (Thread-1): Compiling model.customer_history.customer_broker
2020-05-04 19:33:31.879209 (Thread-1): Writing injected SQL for node "model.customer_history.customer_broker"
2020-05-04 19:33:31.879649 (Thread-1): finished collecting timing info
2020-05-04 19:33:31.886360 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:31.886534 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-05-04 19:33:32.910596 (Thread-1): SQL status: DROP VIEW in 1.02 seconds
2020-05-04 19:33:32.914865 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:32.915019 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-05-04 19:33:33.434455 (Thread-1): SQL status: DROP VIEW in 0.52 seconds
2020-05-04 19:33:33.437278 (Thread-1): Writing runtime SQL for node "model.customer_history.customer_broker"
2020-05-04 19:33:33.437939 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:33.438094 (Thread-1): On model.customer_history.customer_broker: BEGIN
2020-05-04 19:33:33.477504 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:33:33.477809 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:33.477987 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-04 19:33:33.959303 (Thread-1): SQL status: CREATE VIEW in 0.48 seconds
2020-05-04 19:33:33.965086 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:33.965242 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-05-04 19:33:34.361925 (Thread-1): SQL status: ALTER TABLE in 0.40 seconds
2020-05-04 19:33:34.363467 (Thread-1): On model.customer_history.customer_broker: COMMIT
2020-05-04 19:33:34.363622 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:34.363752 (Thread-1): On model.customer_history.customer_broker: COMMIT
2020-05-04 19:33:34.539227 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-05-04 19:33:34.542794 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 19:33:34.542945 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-05-04 19:33:35.076039 (Thread-1): SQL status: DROP VIEW in 0.53 seconds
2020-05-04 19:33:35.080312 (Thread-1): finished collecting timing info
2020-05-04 19:33:35.081190 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd894006-a09a-4dda-8a74-0bbdbe2841ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10761cbd0>]}
2020-05-04 19:33:35.081509 (Thread-1): 12:33:35 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 3.21s]
2020-05-04 19:33:35.081704 (Thread-1): Finished running node model.customer_history.customer_broker
2020-05-04 19:33:35.081889 (Thread-1): Began running node model.customer_history.order_flash_event
2020-05-04 19:33:35.082072 (Thread-1): 12:33:35 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-05-04 19:33:35.082586 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:35.082850 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.customer_broker).
2020-05-04 19:33:35.083039 (Thread-1): Compiling model.customer_history.order_flash_event
2020-05-04 19:33:35.092650 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_event"
2020-05-04 19:33:35.093081 (Thread-1): finished collecting timing info
2020-05-04 19:33:35.100740 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:35.100904 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-05-04 19:33:37.227818 (Thread-1): SQL status: DROP VIEW in 2.13 seconds
2020-05-04 19:33:37.232065 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:37.232224 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-05-04 19:33:37.777862 (Thread-1): SQL status: DROP VIEW in 0.55 seconds
2020-05-04 19:33:37.780917 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_event"
2020-05-04 19:33:37.781541 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:37.781697 (Thread-1): On model.customer_history.order_flash_event: BEGIN
2020-05-04 19:33:37.821401 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:33:37.821837 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:37.822112 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
)

select * FROM order_flash INNER JOIN events USING (event_unique_id)
  );

2020-05-04 19:33:38.364210 (Thread-1): SQL status: CREATE VIEW in 0.54 seconds
2020-05-04 19:33:38.368504 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:38.368666 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
alter table "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" rename to "order_flash_event"
2020-05-04 19:33:38.409204 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 19:33:38.410881 (Thread-1): On model.customer_history.order_flash_event: COMMIT
2020-05-04 19:33:38.411071 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:38.411227 (Thread-1): On model.customer_history.order_flash_event: COMMIT
2020-05-04 19:33:38.673444 (Thread-1): SQL status: COMMIT in 0.26 seconds
2020-05-04 19:33:38.676841 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 19:33:38.677000 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-05-04 19:33:38.850135 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-04 19:33:38.852720 (Thread-1): finished collecting timing info
2020-05-04 19:33:38.853356 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd894006-a09a-4dda-8a74-0bbdbe2841ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075e0c90>]}
2020-05-04 19:33:38.853587 (Thread-1): 12:33:38 | 7 of 8 OK created view model data_science.order_flash_event.......... [CREATE VIEW in 3.77s]
2020-05-04 19:33:38.853725 (Thread-1): Finished running node model.customer_history.order_flash_event
2020-05-04 19:33:38.854086 (Thread-1): Began running node model.customer_history.customers
2020-05-04 19:33:38.854257 (Thread-1): 12:33:38 | 8 of 8 START table model data_science.customers...................... [RUN]
2020-05-04 19:33:38.854562 (Thread-1): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 19:33:38.854673 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_event).
2020-05-04 19:33:38.854773 (Thread-1): Compiling model.customer_history.customers
2020-05-04 19:33:38.862633 (Thread-1): Writing injected SQL for node "model.customer_history.customers"
2020-05-04 19:33:38.862981 (Thread-1): finished collecting timing info
2020-05-04 19:33:38.881913 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 19:33:38.882049 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
drop table if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-05-04 19:33:39.378948 (Thread-1): SQL status: DROP TABLE in 0.50 seconds
2020-05-04 19:33:39.383101 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 19:33:39.383255 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
drop table if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-05-04 19:33:39.695890 (Thread-1): SQL status: DROP TABLE in 0.31 seconds
2020-05-04 19:33:39.699030 (Thread-1): Writing runtime SQL for node "model.customer_history.customers"
2020-05-04 19:33:39.699620 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 19:33:39.699777 (Thread-1): On model.customer_history.customers: BEGIN
2020-05-04 19:33:39.741486 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:33:39.741949 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 19:33:39.742239 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */


  create  table "data_platform_prod"."data_science"."customers__dbt_tmp"
  as (
    

with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash_event"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        COUNT(DISTINCT event_unique_id) AS number_of_events,
        SUM(amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers,

        SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.total_revenue,
        average_days_sold_after_onsale,
        average_days_sold_before_event,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );
2020-05-04 19:44:53.552446 (Thread-1): SQL status: SELECT in 673.81 seconds
2020-05-04 19:44:53.556005 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 19:44:53.556176 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-05-04 19:44:53.773978 (Thread-1): SQL status: ALTER TABLE in 0.22 seconds
2020-05-04 19:44:53.775911 (Thread-1): On model.customer_history.customers: COMMIT
2020-05-04 19:44:53.776106 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 19:44:53.776265 (Thread-1): On model.customer_history.customers: COMMIT
2020-05-04 19:44:59.869439 (Thread-1): SQL status: COMMIT in 6.09 seconds
2020-05-04 19:44:59.872838 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 19:44:59.872999 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
drop table if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-05-04 19:45:00.507213 (Thread-1): SQL status: DROP TABLE in 0.63 seconds
2020-05-04 19:45:00.511446 (Thread-1): finished collecting timing info
2020-05-04 19:45:00.512294 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd894006-a09a-4dda-8a74-0bbdbe2841ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076d0590>]}
2020-05-04 19:45:00.512599 (Thread-1): 12:45:00 | 8 of 8 OK created table model data_science.customers................. [SELECT in 681.66s]
2020-05-04 19:45:00.512779 (Thread-1): Finished running node model.customer_history.customers
2020-05-04 19:45:00.565204 (MainThread): Using postgres connection "master".
2020-05-04 19:45:00.565515 (MainThread): On master: BEGIN
2020-05-04 19:45:00.607125 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-04 19:45:00.607583 (MainThread): On master: COMMIT
2020-05-04 19:45:00.607870 (MainThread): Using postgres connection "master".
2020-05-04 19:45:00.608026 (MainThread): On master: COMMIT
2020-05-04 19:45:00.648664 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-04 19:45:00.649595 (MainThread): 12:45:00 | 
2020-05-04 19:45:00.649833 (MainThread): 12:45:00 | Finished running 7 view models, 1 table model in 705.98s.
2020-05-04 19:45:00.650028 (MainThread): Connection 'master' was left open.
2020-05-04 19:45:00.650184 (MainThread): On master: Close
2020-05-04 19:45:00.650594 (MainThread): Connection 'model.customer_history.customers' was left open.
2020-05-04 19:45:00.650771 (MainThread): On model.customer_history.customers: Close
2020-05-04 19:45:00.675856 (MainThread): 
2020-05-04 19:45:00.676060 (MainThread): Completed successfully
2020-05-04 19:45:00.676204 (MainThread): 
Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
2020-05-04 19:45:00.676403 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10763c8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10751ac50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074ed190>]}
2020-05-04 19:45:00.676618 (MainThread): Flushing usage events
2020-05-04 21:02:04.520810 (MainThread): Running with dbt=0.16.1
2020-05-04 21:02:04.591213 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-04 21:02:04.591915 (MainThread): Tracking: tracking
2020-05-04 21:02:04.596822 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059de450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d9410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059dec10>]}
2020-05-04 21:02:04.614937 (MainThread): Partial parsing not enabled
2020-05-04 21:02:04.616768 (MainThread): Parsing macros/core.sql
2020-05-04 21:02:04.621326 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-04 21:02:04.629371 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-04 21:02:04.631129 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-04 21:02:04.649166 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-04 21:02:04.682708 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-04 21:02:04.703586 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-04 21:02:04.706671 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-04 21:02:04.713029 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-04 21:02:04.727013 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-04 21:02:04.734490 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-04 21:02:04.742804 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-04 21:02:04.749855 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-04 21:02:04.751358 (MainThread): Parsing macros/etc/query.sql
2020-05-04 21:02:04.753764 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-04 21:02:04.756687 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-04 21:02:04.759766 (MainThread): Parsing macros/etc/datetime.sql
2020-05-04 21:02:04.769560 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-04 21:02:04.771663 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-04 21:02:04.772792 (MainThread): Parsing macros/adapters/common.sql
2020-05-04 21:02:04.816513 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-04 21:02:04.819395 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-04 21:02:04.821486 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-04 21:02:04.823780 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-04 21:02:04.826841 (MainThread): Parsing macros/catalog.sql
2020-05-04 21:02:04.829742 (MainThread): Parsing macros/relations.sql
2020-05-04 21:02:04.831780 (MainThread): Parsing macros/adapters.sql
2020-05-04 21:02:04.849814 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-04 21:02:04.867803 (MainThread): Partial parsing not enabled
2020-05-04 21:02:04.894464 (MainThread): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 21:02:04.894561 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:04.910575 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:04.910678 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:04.914708 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:04.914795 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:04.918941 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:04.919025 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:04.922842 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:04.922926 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:04.927396 (MainThread): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:04.927487 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:04.932280 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:04.932365 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:04.937863 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:04.937950 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:05.086041 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-04 21:02:05.089682 (MainThread): 
2020-05-04 21:02:05.089988 (MainThread): Acquiring new postgres connection "master".
2020-05-04 21:02:05.090080 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:05.113878 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-04 21:02:05.114013 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-04 21:02:05.198153 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-04 21:02:05.198277 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-04 21:02:05.939443 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.74 seconds
2020-05-04 21:02:05.977620 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-04 21:02:05.977839 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-04 21:02:05.979840 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 21:02:05.979953 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-04 21:02:06.020960 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:06.021181 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 21:02:06.021315 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-04 21:02:06.195530 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.17 seconds
2020-05-04 21:02:06.205724 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-04 21:02:06.285471 (MainThread): Using postgres connection "master".
2020-05-04 21:02:06.285626 (MainThread): On master: BEGIN
2020-05-04 21:02:06.629028 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-05-04 21:02:06.629451 (MainThread): Using postgres connection "master".
2020-05-04 21:02:06.629723 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-04 21:02:06.833811 (MainThread): SQL status: SELECT in 0.20 seconds
2020-05-04 21:02:06.906721 (MainThread): On master: ROLLBACK
2020-05-04 21:02:06.943824 (MainThread): Using postgres connection "master".
2020-05-04 21:02:06.944041 (MainThread): On master: BEGIN
2020-05-04 21:02:07.019638 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-05-04 21:02:07.020100 (MainThread): On master: COMMIT
2020-05-04 21:02:07.020399 (MainThread): Using postgres connection "master".
2020-05-04 21:02:07.020554 (MainThread): On master: COMMIT
2020-05-04 21:02:07.057767 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-04 21:02:07.058645 (MainThread): 14:02:07 | Concurrency: 1 threads (target='dev')
2020-05-04 21:02:07.058893 (MainThread): 14:02:07 | 
2020-05-04 21:02:07.061198 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-04 21:02:07.061466 (Thread-1): 14:02:07 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-05-04 21:02:07.061843 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:07.061979 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-04 21:02:07.062120 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-04 21:02:07.078675 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-04 21:02:07.079212 (Thread-1): finished collecting timing info
2020-05-04 21:02:07.119909 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:07.120058 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-04 21:02:07.201139 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-04 21:02:07.204283 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:07.204412 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 21:02:07.245074 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-04 21:02:07.248129 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-04 21:02:07.248814 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:07.248968 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-04 21:02:07.290291 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:07.290751 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:07.291023 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-04 21:02:07.524273 (Thread-1): SQL status: CREATE VIEW in 0.23 seconds
2020-05-04 21:02:07.529903 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:07.530061 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-04 21:02:07.575520 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-04 21:02:07.578444 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:07.578573 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-04 21:02:07.619899 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:02:07.621884 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 21:02:07.622079 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:07.622239 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 21:02:08.121214 (Thread-1): SQL status: COMMIT in 0.50 seconds
2020-05-04 21:02:08.124728 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:08.124885 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 21:02:08.760550 (Thread-1): SQL status: DROP VIEW in 0.64 seconds
2020-05-04 21:02:08.764668 (Thread-1): finished collecting timing info
2020-05-04 21:02:08.765512 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44d1473e-2d66-4ad4-a717-00e5f86b9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d24b50>]}
2020-05-04 21:02:08.765825 (Thread-1): 14:02:08 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 1.70s]
2020-05-04 21:02:08.766013 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-04 21:02:08.766199 (Thread-1): Began running node model.customer_history.stg_order
2020-05-04 21:02:08.766384 (Thread-1): 14:02:08 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-05-04 21:02:08.767068 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:08.767282 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-04 21:02:08.767445 (Thread-1): Compiling model.customer_history.stg_order
2020-05-04 21:02:08.773684 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-04 21:02:08.774170 (Thread-1): finished collecting timing info
2020-05-04 21:02:08.781634 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:08.781768 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-04 21:02:09.912761 (Thread-1): SQL status: DROP VIEW in 1.13 seconds
2020-05-04 21:02:09.917056 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:09.917226 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 21:02:10.279349 (Thread-1): SQL status: DROP VIEW in 0.36 seconds
2020-05-04 21:02:10.283676 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-04 21:02:10.284320 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:10.284472 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-04 21:02:10.325806 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:10.326234 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:10.326497 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-04 21:02:10.499509 (Thread-1): SQL status: CREATE VIEW in 0.17 seconds
2020-05-04 21:02:10.530215 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:10.530395 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-04 21:02:10.578115 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-04 21:02:10.582487 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:10.582642 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-04 21:02:10.630466 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-04 21:02:10.632375 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 21:02:10.632571 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:10.632731 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 21:02:11.241384 (Thread-1): SQL status: COMMIT in 0.61 seconds
2020-05-04 21:02:11.243310 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:11.243421 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 21:02:11.790285 (Thread-1): SQL status: DROP VIEW in 0.55 seconds
2020-05-04 21:02:11.794722 (Thread-1): finished collecting timing info
2020-05-04 21:02:11.795569 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44d1473e-2d66-4ad4-a717-00e5f86b9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fe6d50>]}
2020-05-04 21:02:11.795870 (Thread-1): 14:02:11 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 3.03s]
2020-05-04 21:02:11.796049 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-04 21:02:11.796236 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-04 21:02:11.796419 (Thread-1): 14:02:11 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-05-04 21:02:11.797078 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:11.797347 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-04 21:02:11.797536 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-04 21:02:11.804137 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-04 21:02:11.804634 (Thread-1): finished collecting timing info
2020-05-04 21:02:11.812140 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:11.812270 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-04 21:02:12.396602 (Thread-1): SQL status: DROP VIEW in 0.58 seconds
2020-05-04 21:02:12.400659 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:12.400807 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 21:02:12.579125 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-04 21:02:12.581762 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-04 21:02:12.582424 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:12.582587 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-04 21:02:12.622368 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:12.622722 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:12.623010 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-05-04 21:02:12.678031 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-05-04 21:02:12.684162 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:12.684341 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-04 21:02:12.730203 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-04 21:02:12.735682 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:12.735834 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-04 21:02:13.607353 (Thread-1): SQL status: ALTER TABLE in 0.87 seconds
2020-05-04 21:02:13.609209 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 21:02:13.609425 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:13.609626 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 21:02:14.205645 (Thread-1): SQL status: COMMIT in 0.60 seconds
2020-05-04 21:02:14.209077 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:14.209296 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 21:02:14.671671 (Thread-1): SQL status: DROP VIEW in 0.46 seconds
2020-05-04 21:02:14.675338 (Thread-1): finished collecting timing info
2020-05-04 21:02:14.676184 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44d1473e-2d66-4ad4-a717-00e5f86b9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cefed0>]}
2020-05-04 21:02:14.676485 (Thread-1): 14:02:14 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 2.88s]
2020-05-04 21:02:14.676666 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-04 21:02:14.676906 (Thread-1): Began running node model.customer_history.stg_events
2020-05-04 21:02:14.677329 (Thread-1): 14:02:14 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-05-04 21:02:14.677752 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:14.677936 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-04 21:02:14.678069 (Thread-1): Compiling model.customer_history.stg_events
2020-05-04 21:02:14.684344 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-04 21:02:14.684828 (Thread-1): finished collecting timing info
2020-05-04 21:02:14.694937 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:14.695183 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-04 21:02:15.373527 (Thread-1): SQL status: DROP VIEW in 0.68 seconds
2020-05-04 21:02:15.377472 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:15.377630 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 21:02:15.944701 (Thread-1): SQL status: DROP VIEW in 0.57 seconds
2020-05-04 21:02:15.947787 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-04 21:02:15.948435 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:15.948602 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-04 21:02:15.988975 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:15.989404 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:15.989672 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-04 21:02:16.045408 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-05-04 21:02:16.051589 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:16.051742 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-04 21:02:16.092034 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:02:16.095703 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:16.095858 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-04 21:02:16.353969 (Thread-1): SQL status: ALTER TABLE in 0.26 seconds
2020-05-04 21:02:16.355833 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 21:02:16.356028 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:16.356187 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 21:02:16.821539 (Thread-1): SQL status: COMMIT in 0.47 seconds
2020-05-04 21:02:16.824774 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:16.824938 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 21:02:17.781189 (Thread-1): SQL status: DROP VIEW in 0.96 seconds
2020-05-04 21:02:17.787015 (Thread-1): finished collecting timing info
2020-05-04 21:02:17.787869 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44d1473e-2d66-4ad4-a717-00e5f86b9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fca650>]}
2020-05-04 21:02:17.788172 (Thread-1): 14:02:17 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 3.11s]
2020-05-04 21:02:17.788350 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-04 21:02:17.788531 (Thread-1): Began running node model.customer_history.order_flash
2020-05-04 21:02:17.788718 (Thread-1): 14:02:17 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-05-04 21:02:17.789271 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:17.789490 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-04 21:02:17.789642 (Thread-1): Compiling model.customer_history.order_flash
2020-05-04 21:02:17.799226 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash"
2020-05-04 21:02:17.799707 (Thread-1): finished collecting timing info
2020-05-04 21:02:17.807329 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:17.807474 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-05-04 21:02:17.998336 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-05-04 21:02:18.000901 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:18.001026 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-05-04 21:02:18.219828 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-05-04 21:02:18.222915 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash"
2020-05-04 21:02:18.223620 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:18.223783 (Thread-1): On model.customer_history.order_flash: BEGIN
2020-05-04 21:02:18.264743 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:18.265173 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:18.265443 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-05-04 21:02:18.611555 (Thread-1): SQL status: CREATE VIEW in 0.35 seconds
2020-05-04 21:02:18.615824 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:18.615993 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-05-04 21:02:18.660509 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:02:18.662173 (Thread-1): On model.customer_history.order_flash: COMMIT
2020-05-04 21:02:18.662374 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:18.662501 (Thread-1): On model.customer_history.order_flash: COMMIT
2020-05-04 21:02:19.249899 (Thread-1): SQL status: COMMIT in 0.59 seconds
2020-05-04 21:02:19.253275 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:19.253429 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-05-04 21:02:19.587323 (Thread-1): SQL status: DROP VIEW in 0.33 seconds
2020-05-04 21:02:19.591535 (Thread-1): finished collecting timing info
2020-05-04 21:02:19.592377 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44d1473e-2d66-4ad4-a717-00e5f86b9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fabc50>]}
2020-05-04 21:02:19.592681 (Thread-1): 14:02:19 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 1.80s]
2020-05-04 21:02:19.592858 (Thread-1): Finished running node model.customer_history.order_flash
2020-05-04 21:02:19.593051 (Thread-1): Began running node model.customer_history.customer_broker
2020-05-04 21:02:19.593527 (Thread-1): 14:02:19 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-05-04 21:02:19.594239 (Thread-1): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:19.594390 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash).
2020-05-04 21:02:19.594517 (Thread-1): Compiling model.customer_history.customer_broker
2020-05-04 21:02:19.602514 (Thread-1): Writing injected SQL for node "model.customer_history.customer_broker"
2020-05-04 21:02:19.603011 (Thread-1): finished collecting timing info
2020-05-04 21:02:19.610607 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:19.610755 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-05-04 21:02:19.886781 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2020-05-04 21:02:19.890391 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:19.890544 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-05-04 21:02:20.357577 (Thread-1): SQL status: DROP VIEW in 0.47 seconds
2020-05-04 21:02:20.361944 (Thread-1): Writing runtime SQL for node "model.customer_history.customer_broker"
2020-05-04 21:02:20.362575 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:20.362726 (Thread-1): On model.customer_history.customer_broker: BEGIN
2020-05-04 21:02:20.538170 (Thread-1): SQL status: BEGIN in 0.18 seconds
2020-05-04 21:02:20.538356 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:20.538459 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-04 21:02:20.896070 (Thread-1): SQL status: CREATE VIEW in 0.36 seconds
2020-05-04 21:02:20.898703 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:20.898837 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-05-04 21:02:21.430832 (Thread-1): SQL status: ALTER TABLE in 0.53 seconds
2020-05-04 21:02:21.432361 (Thread-1): On model.customer_history.customer_broker: COMMIT
2020-05-04 21:02:21.432538 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:21.432667 (Thread-1): On model.customer_history.customer_broker: COMMIT
2020-05-04 21:02:21.927594 (Thread-1): SQL status: COMMIT in 0.49 seconds
2020-05-04 21:02:21.931029 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:21.931183 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-05-04 21:02:22.108584 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-04 21:02:22.112630 (Thread-1): finished collecting timing info
2020-05-04 21:02:22.113480 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44d1473e-2d66-4ad4-a717-00e5f86b9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f9c450>]}
2020-05-04 21:02:22.113784 (Thread-1): 14:02:22 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 2.52s]
2020-05-04 21:02:22.113964 (Thread-1): Finished running node model.customer_history.customer_broker
2020-05-04 21:02:22.114196 (Thread-1): Began running node model.customer_history.order_flash_event
2020-05-04 21:02:22.114604 (Thread-1): 14:02:22 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-05-04 21:02:22.115089 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:22.115244 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.customer_broker).
2020-05-04 21:02:22.115393 (Thread-1): Compiling model.customer_history.order_flash_event
2020-05-04 21:02:22.125011 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_event"
2020-05-04 21:02:22.125518 (Thread-1): finished collecting timing info
2020-05-04 21:02:22.133171 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:22.133335 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-05-04 21:02:22.832176 (Thread-1): SQL status: DROP VIEW in 0.70 seconds
2020-05-04 21:02:22.835963 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:22.836123 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-05-04 21:02:23.241943 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-05-04 21:02:23.244596 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_event"
2020-05-04 21:02:23.245245 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:23.245406 (Thread-1): On model.customer_history.order_flash_event: BEGIN
2020-05-04 21:02:23.285155 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:23.285354 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:23.285465 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
)

select * FROM order_flash INNER JOIN events USING (event_unique_id)
  );

2020-05-04 21:02:23.340144 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-05-04 21:02:23.343055 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:23.343239 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
alter table "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" rename to "order_flash_event"
2020-05-04 21:02:23.383803 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:02:23.385166 (Thread-1): On model.customer_history.order_flash_event: COMMIT
2020-05-04 21:02:23.385323 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:23.385450 (Thread-1): On model.customer_history.order_flash_event: COMMIT
2020-05-04 21:02:23.826980 (Thread-1): SQL status: COMMIT in 0.44 seconds
2020-05-04 21:02:23.830316 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:23.830464 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-05-04 21:02:24.011138 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-04 21:02:24.016059 (Thread-1): finished collecting timing info
2020-05-04 21:02:24.017262 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44d1473e-2d66-4ad4-a717-00e5f86b9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f9c450>]}
2020-05-04 21:02:24.017650 (Thread-1): 14:02:24 | 7 of 8 OK created view model data_science.order_flash_event.......... [CREATE VIEW in 1.90s]
2020-05-04 21:02:24.017880 (Thread-1): Finished running node model.customer_history.order_flash_event
2020-05-04 21:02:24.018400 (Thread-1): Began running node model.customer_history.customers
2020-05-04 21:02:24.018674 (Thread-1): 14:02:24 | 8 of 8 START table model data_science.customers...................... [RUN]
2020-05-04 21:02:24.019168 (Thread-1): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 21:02:24.019345 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_event).
2020-05-04 21:02:24.019521 (Thread-1): Compiling model.customer_history.customers
2020-05-04 21:02:24.031671 (Thread-1): Writing injected SQL for node "model.customer_history.customers"
2020-05-04 21:02:24.032207 (Thread-1): finished collecting timing info
2020-05-04 21:02:24.055543 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:02:24.055700 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
drop table if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-05-04 21:02:24.474445 (Thread-1): SQL status: DROP TABLE in 0.42 seconds
2020-05-04 21:02:24.478684 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:02:24.478852 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
drop table if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-05-04 21:02:24.525474 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2020-05-04 21:02:24.528081 (Thread-1): Writing runtime SQL for node "model.customer_history.customers"
2020-05-04 21:02:24.528695 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:02:24.528849 (Thread-1): On model.customer_history.customers: BEGIN
2020-05-04 21:02:24.591857 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-05-04 21:02:24.592097 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:02:24.592237 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */


  create  table "data_platform_prod"."data_science"."customers__dbt_tmp"
  as (
    

with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash_event"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        COUNT(DISTINCT event_unique_id) AS number_of_events,
        SUM(amount_gross) AS total_revenue,

        SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.number_of_events,
        customer_orders.total_revenue,
        average_days_sold_after_onsale,
        average_days_sold_before_event,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers,

    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );
2020-05-04 21:02:24.665366 (Thread-1): Postgres error: syntax error at or near "from"
LINE 57:     from customers
             ^

2020-05-04 21:02:24.665644 (Thread-1): On model.customer_history.customers: ROLLBACK
2020-05-04 21:02:24.718012 (Thread-1): finished collecting timing info
2020-05-04 21:02:24.718693 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "from"
  LINE 57:     from customers
               ^
  compiled SQL at target/run/customer_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "from"
LINE 57:     from customers
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 62, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "from"
  LINE 57:     from customers
               ^
  compiled SQL at target/run/customer_history/customers.sql
2020-05-04 21:02:24.734249 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44d1473e-2d66-4ad4-a717-00e5f86b9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d85890>]}
2020-05-04 21:02:24.734560 (Thread-1): 14:02:24 | 8 of 8 ERROR creating table model data_science.customers............. [ERROR in 0.72s]
2020-05-04 21:02:24.734712 (Thread-1): Finished running node model.customer_history.customers
2020-05-04 21:02:24.799732 (MainThread): Using postgres connection "master".
2020-05-04 21:02:24.800037 (MainThread): On master: BEGIN
2020-05-04 21:02:24.837734 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:24.838035 (MainThread): On master: COMMIT
2020-05-04 21:02:24.838205 (MainThread): Using postgres connection "master".
2020-05-04 21:02:24.838358 (MainThread): On master: COMMIT
2020-05-04 21:02:24.877846 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-04 21:02:24.878781 (MainThread): 14:02:24 | 
2020-05-04 21:02:24.879020 (MainThread): 14:02:24 | Finished running 7 view models, 1 table model in 19.79s.
2020-05-04 21:02:24.879211 (MainThread): Connection 'master' was left open.
2020-05-04 21:02:24.879377 (MainThread): On master: Close
2020-05-04 21:02:24.879758 (MainThread): Connection 'model.customer_history.customers' was left open.
2020-05-04 21:02:24.879920 (MainThread): On model.customer_history.customers: Close
2020-05-04 21:02:24.904930 (MainThread): 
2020-05-04 21:02:24.905138 (MainThread): Completed with 1 error and 0 warnings:
2020-05-04 21:02:24.905281 (MainThread): 
2020-05-04 21:02:24.905415 (MainThread): Database Error in model customers (models/customers.sql)
2020-05-04 21:02:24.905547 (MainThread):   syntax error at or near "from"
2020-05-04 21:02:24.905664 (MainThread):   LINE 57:     from customers
2020-05-04 21:02:24.905775 (MainThread):                ^
2020-05-04 21:02:24.905885 (MainThread):   compiled SQL at target/run/customer_history/customers.sql
2020-05-04 21:02:24.906010 (MainThread): 
Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
2020-05-04 21:02:24.906260 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a11390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105bba810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a18bd0>]}
2020-05-04 21:02:24.906512 (MainThread): Flushing usage events
2020-05-04 21:02:51.482495 (MainThread): Running with dbt=0.16.1
2020-05-04 21:02:51.548484 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-04 21:02:51.549363 (MainThread): Tracking: tracking
2020-05-04 21:02:51.554575 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089c22d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10877f350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089c26d0>]}
2020-05-04 21:02:51.573130 (MainThread): Partial parsing not enabled
2020-05-04 21:02:51.574987 (MainThread): Parsing macros/core.sql
2020-05-04 21:02:51.579497 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-04 21:02:51.587450 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-04 21:02:51.589228 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-04 21:02:51.607268 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-04 21:02:51.641479 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-04 21:02:51.668405 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-04 21:02:51.670464 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-04 21:02:51.677102 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-04 21:02:51.690126 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-04 21:02:51.697111 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-04 21:02:51.703551 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-04 21:02:51.708630 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-04 21:02:51.709611 (MainThread): Parsing macros/etc/query.sql
2020-05-04 21:02:51.710724 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-04 21:02:51.712446 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-04 21:02:51.714583 (MainThread): Parsing macros/etc/datetime.sql
2020-05-04 21:02:51.723924 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-04 21:02:51.725965 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-04 21:02:51.727051 (MainThread): Parsing macros/adapters/common.sql
2020-05-04 21:02:51.769327 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-04 21:02:51.770529 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-04 21:02:51.771634 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-04 21:02:51.772714 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-04 21:02:51.774906 (MainThread): Parsing macros/catalog.sql
2020-05-04 21:02:51.777315 (MainThread): Parsing macros/relations.sql
2020-05-04 21:02:51.778650 (MainThread): Parsing macros/adapters.sql
2020-05-04 21:02:51.795165 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-04 21:02:51.812396 (MainThread): Partial parsing not enabled
2020-05-04 21:02:51.839118 (MainThread): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 21:02:51.839219 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:51.854972 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:51.855059 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:51.858940 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:51.859024 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:51.863305 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:51.863391 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:51.867163 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 21:02:51.867246 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:51.872047 (MainThread): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 21:02:51.872135 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:51.876833 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 21:02:51.876916 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:51.882479 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:02:51.882562 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:52.030999 (MainThread): Found 8 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-04 21:02:52.035219 (MainThread): 
2020-05-04 21:02:52.035582 (MainThread): Acquiring new postgres connection "master".
2020-05-04 21:02:52.035674 (MainThread): Opening a new connection, currently in state init
2020-05-04 21:02:52.059764 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-04 21:02:52.059895 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-04 21:02:52.142672 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-04 21:02:52.142808 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-04 21:02:52.550864 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.41 seconds
2020-05-04 21:02:52.590647 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-04 21:02:52.590863 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-04 21:02:52.592961 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 21:02:52.593082 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-04 21:02:52.630497 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:52.630953 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-04 21:02:52.631183 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-04 21:02:52.805195 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.17 seconds
2020-05-04 21:02:52.815046 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-04 21:02:52.889458 (MainThread): Using postgres connection "master".
2020-05-04 21:02:52.889616 (MainThread): On master: BEGIN
2020-05-04 21:02:53.262919 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-05-04 21:02:53.263150 (MainThread): Using postgres connection "master".
2020-05-04 21:02:53.263283 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-04 21:02:53.490126 (MainThread): SQL status: SELECT in 0.23 seconds
2020-05-04 21:02:53.574443 (MainThread): On master: ROLLBACK
2020-05-04 21:02:53.612996 (MainThread): Using postgres connection "master".
2020-05-04 21:02:53.613268 (MainThread): On master: BEGIN
2020-05-04 21:02:53.689586 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-05-04 21:02:53.689805 (MainThread): On master: COMMIT
2020-05-04 21:02:53.689936 (MainThread): Using postgres connection "master".
2020-05-04 21:02:53.690055 (MainThread): On master: COMMIT
2020-05-04 21:02:53.729182 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-04 21:02:53.729593 (MainThread): 14:02:53 | Concurrency: 1 threads (target='dev')
2020-05-04 21:02:53.729740 (MainThread): 14:02:53 | 
2020-05-04 21:02:53.731274 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-04 21:02:53.731475 (Thread-1): 14:02:53 | 1 of 8 START view model data_science.stg_flash....................... [RUN]
2020-05-04 21:02:53.731776 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:53.731875 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-04 21:02:53.731981 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-04 21:02:53.744603 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-04 21:02:53.744997 (Thread-1): finished collecting timing info
2020-05-04 21:02:53.777390 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:53.777556 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-04 21:02:53.852308 (Thread-1): SQL status: DROP VIEW in 0.07 seconds
2020-05-04 21:02:53.855972 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:53.856124 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 21:02:53.893470 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-04 21:02:53.895812 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-04 21:02:53.896406 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:53.896564 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-04 21:02:53.933548 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:53.933987 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:53.934264 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-04 21:02:54.206154 (Thread-1): SQL status: CREATE VIEW in 0.27 seconds
2020-05-04 21:02:54.212597 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:54.212852 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-04 21:02:54.278374 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2020-05-04 21:02:54.282765 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:54.282961 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-04 21:02:54.328988 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-04 21:02:54.330171 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 21:02:54.330309 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:54.330422 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-04 21:02:54.871241 (Thread-1): SQL status: COMMIT in 0.54 seconds
2020-05-04 21:02:54.874151 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-04 21:02:54.874335 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-04 21:02:55.417563 (Thread-1): SQL status: DROP VIEW in 0.54 seconds
2020-05-04 21:02:55.421425 (Thread-1): finished collecting timing info
2020-05-04 21:02:55.422321 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9329fe8-45b9-4b45-8e56-ee363957ff11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b8b610>]}
2020-05-04 21:02:55.422643 (Thread-1): 14:02:55 | 1 of 8 OK created view model data_science.stg_flash.................. [CREATE VIEW in 1.69s]
2020-05-04 21:02:55.422831 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-04 21:02:55.423023 (Thread-1): Began running node model.customer_history.stg_order
2020-05-04 21:02:55.423303 (Thread-1): 14:02:55 | 2 of 8 START view model data_science.stg_order....................... [RUN]
2020-05-04 21:02:55.423667 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:55.423788 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-04 21:02:55.423897 (Thread-1): Compiling model.customer_history.stg_order
2020-05-04 21:02:55.429767 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-04 21:02:55.430209 (Thread-1): finished collecting timing info
2020-05-04 21:02:55.436914 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:55.437055 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-04 21:02:55.798363 (Thread-1): SQL status: DROP VIEW in 0.36 seconds
2020-05-04 21:02:55.802576 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:55.802737 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 21:02:56.496588 (Thread-1): SQL status: DROP VIEW in 0.69 seconds
2020-05-04 21:02:56.500929 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-04 21:02:56.501536 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:56.501686 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-04 21:02:56.540450 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:56.540887 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:56.541180 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-04 21:02:56.602675 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-05-04 21:02:56.639578 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:56.639795 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-04 21:02:56.678072 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:02:56.682445 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:56.682606 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-04 21:02:57.113439 (Thread-1): SQL status: ALTER TABLE in 0.43 seconds
2020-05-04 21:02:57.115177 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 21:02:57.115366 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:57.115526 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-04 21:02:57.665848 (Thread-1): SQL status: COMMIT in 0.55 seconds
2020-05-04 21:02:57.668849 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-04 21:02:57.669007 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-04 21:02:58.254769 (Thread-1): SQL status: DROP VIEW in 0.59 seconds
2020-05-04 21:02:58.257861 (Thread-1): finished collecting timing info
2020-05-04 21:02:58.258571 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9329fe8-45b9-4b45-8e56-ee363957ff11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089c2d10>]}
2020-05-04 21:02:58.258826 (Thread-1): 14:02:58 | 2 of 8 OK created view model data_science.stg_order.................. [CREATE VIEW in 2.83s]
2020-05-04 21:02:58.258976 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-04 21:02:58.259136 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-04 21:02:58.259427 (Thread-1): 14:02:58 | 3 of 8 START view model data_science.stg_customers................... [RUN]
2020-05-04 21:02:58.259935 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:58.260063 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-04 21:02:58.260175 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-04 21:02:58.265843 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-04 21:02:58.266316 (Thread-1): finished collecting timing info
2020-05-04 21:02:58.273478 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:58.273622 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-04 21:02:58.747325 (Thread-1): SQL status: DROP VIEW in 0.47 seconds
2020-05-04 21:02:58.751459 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:58.751619 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 21:02:59.028078 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2020-05-04 21:02:59.029917 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-04 21:02:59.030435 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:59.030564 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-04 21:02:59.067971 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:02:59.068257 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:59.068443 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-05-04 21:02:59.189384 (Thread-1): SQL status: CREATE VIEW in 0.12 seconds
2020-05-04 21:02:59.194766 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:59.194935 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-04 21:02:59.579133 (Thread-1): SQL status: ALTER TABLE in 0.38 seconds
2020-05-04 21:02:59.584628 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:59.584778 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-04 21:02:59.626505 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:02:59.628606 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 21:02:59.628803 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:02:59.628961 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-04 21:03:01.925111 (Thread-1): SQL status: COMMIT in 2.30 seconds
2020-05-04 21:03:01.928304 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-04 21:03:01.928466 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-04 21:03:02.463926 (Thread-1): SQL status: DROP VIEW in 0.54 seconds
2020-05-04 21:03:02.468164 (Thread-1): finished collecting timing info
2020-05-04 21:03:02.469010 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9329fe8-45b9-4b45-8e56-ee363957ff11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cd3cd0>]}
2020-05-04 21:03:02.469311 (Thread-1): 14:03:02 | 3 of 8 OK created view model data_science.stg_customers.............. [CREATE VIEW in 4.21s]
2020-05-04 21:03:02.469491 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-04 21:03:02.469675 (Thread-1): Began running node model.customer_history.stg_events
2020-05-04 21:03:02.469906 (Thread-1): 14:03:02 | 4 of 8 START view model data_science.stg_events...................... [RUN]
2020-05-04 21:03:02.470497 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:02.470791 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-04 21:03:02.471030 (Thread-1): Compiling model.customer_history.stg_events
2020-05-04 21:03:02.477601 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-04 21:03:02.478101 (Thread-1): finished collecting timing info
2020-05-04 21:03:02.485735 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:02.485870 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-04 21:03:02.692347 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-05-04 21:03:02.696475 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:02.696627 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 21:03:03.344957 (Thread-1): SQL status: DROP VIEW in 0.65 seconds
2020-05-04 21:03:03.346912 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-04 21:03:03.347477 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:03.347599 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-04 21:03:03.387651 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:03:03.388113 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:03.388413 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-04 21:03:03.443561 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-05-04 21:03:03.449843 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:03.450003 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-04 21:03:03.493508 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:03:03.497259 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:03.497419 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-04 21:03:03.542141 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:03:03.543629 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 21:03:03.543800 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:03.543926 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-04 21:03:04.413850 (Thread-1): SQL status: COMMIT in 0.87 seconds
2020-05-04 21:03:04.415948 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-04 21:03:04.416068 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-04 21:03:04.983532 (Thread-1): SQL status: DROP VIEW in 0.57 seconds
2020-05-04 21:03:04.989180 (Thread-1): finished collecting timing info
2020-05-04 21:03:04.990034 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9329fe8-45b9-4b45-8e56-ee363957ff11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a0ef10>]}
2020-05-04 21:03:04.990353 (Thread-1): 14:03:04 | 4 of 8 OK created view model data_science.stg_events................. [CREATE VIEW in 2.52s]
2020-05-04 21:03:04.990523 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-04 21:03:04.990760 (Thread-1): Began running node model.customer_history.order_flash
2020-05-04 21:03:04.991199 (Thread-1): 14:03:04 | 5 of 8 START view model data_science.order_flash..................... [RUN]
2020-05-04 21:03:04.991566 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash".
2020-05-04 21:03:04.991697 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-04 21:03:04.991839 (Thread-1): Compiling model.customer_history.order_flash
2020-05-04 21:03:05.001043 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash"
2020-05-04 21:03:05.001512 (Thread-1): finished collecting timing info
2020-05-04 21:03:05.009123 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:03:05.009271 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-05-04 21:03:05.304013 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-05-04 21:03:05.308174 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:03:05.308328 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-05-04 21:03:05.718423 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-05-04 21:03:05.721485 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash"
2020-05-04 21:03:05.722181 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:03:05.722336 (Thread-1): On model.customer_history.order_flash: BEGIN
2020-05-04 21:03:05.759809 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:03:05.760217 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:03:05.760475 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-05-04 21:03:06.290426 (Thread-1): SQL status: CREATE VIEW in 0.53 seconds
2020-05-04 21:03:06.293692 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:03:06.293830 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-05-04 21:03:06.333674 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:03:06.335544 (Thread-1): On model.customer_history.order_flash: COMMIT
2020-05-04 21:03:06.335737 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:03:06.335896 (Thread-1): On model.customer_history.order_flash: COMMIT
2020-05-04 21:03:06.841853 (Thread-1): SQL status: COMMIT in 0.51 seconds
2020-05-04 21:03:06.844625 (Thread-1): Using postgres connection "model.customer_history.order_flash".
2020-05-04 21:03:06.844780 (Thread-1): On model.customer_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-05-04 21:03:07.041598 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-05-04 21:03:07.044414 (Thread-1): finished collecting timing info
2020-05-04 21:03:07.045137 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9329fe8-45b9-4b45-8e56-ee363957ff11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a363d0>]}
2020-05-04 21:03:07.045387 (Thread-1): 14:03:07 | 5 of 8 OK created view model data_science.order_flash................ [CREATE VIEW in 2.05s]
2020-05-04 21:03:07.045532 (Thread-1): Finished running node model.customer_history.order_flash
2020-05-04 21:03:07.045692 (Thread-1): Began running node model.customer_history.customer_broker
2020-05-04 21:03:07.046112 (Thread-1): 14:03:07 | 6 of 8 START view model data_science.customer_broker................. [RUN]
2020-05-04 21:03:07.046567 (Thread-1): Acquiring new postgres connection "model.customer_history.customer_broker".
2020-05-04 21:03:07.046685 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash).
2020-05-04 21:03:07.046793 (Thread-1): Compiling model.customer_history.customer_broker
2020-05-04 21:03:07.054015 (Thread-1): Writing injected SQL for node "model.customer_history.customer_broker"
2020-05-04 21:03:07.054514 (Thread-1): finished collecting timing info
2020-05-04 21:03:07.061712 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:03:07.061863 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-05-04 21:03:07.439210 (Thread-1): SQL status: DROP VIEW in 0.38 seconds
2020-05-04 21:03:07.442732 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:03:07.442891 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-05-04 21:03:07.650764 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-05-04 21:03:07.655370 (Thread-1): Writing runtime SQL for node "model.customer_history.customer_broker"
2020-05-04 21:03:07.656038 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:03:07.656227 (Thread-1): On model.customer_history.customer_broker: BEGIN
2020-05-04 21:03:07.693627 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:03:07.694059 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:03:07.694337 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-04 21:03:08.043943 (Thread-1): SQL status: CREATE VIEW in 0.35 seconds
2020-05-04 21:03:08.046592 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:03:08.046707 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-05-04 21:03:08.206113 (Thread-1): SQL status: ALTER TABLE in 0.16 seconds
2020-05-04 21:03:08.207487 (Thread-1): On model.customer_history.customer_broker: COMMIT
2020-05-04 21:03:08.207649 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:03:08.207778 (Thread-1): On model.customer_history.customer_broker: COMMIT
2020-05-04 21:03:08.387236 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-05-04 21:03:08.389262 (Thread-1): Using postgres connection "model.customer_history.customer_broker".
2020-05-04 21:03:08.389391 (Thread-1): On model.customer_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-05-04 21:03:09.469421 (Thread-1): SQL status: DROP VIEW in 1.08 seconds
2020-05-04 21:03:09.472341 (Thread-1): finished collecting timing info
2020-05-04 21:03:09.473044 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9329fe8-45b9-4b45-8e56-ee363957ff11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d7be10>]}
2020-05-04 21:03:09.473288 (Thread-1): 14:03:09 | 6 of 8 OK created view model data_science.customer_broker............ [CREATE VIEW in 2.43s]
2020-05-04 21:03:09.473431 (Thread-1): Finished running node model.customer_history.customer_broker
2020-05-04 21:03:09.473581 (Thread-1): Began running node model.customer_history.order_flash_event
2020-05-04 21:03:09.473840 (Thread-1): 14:03:09 | 7 of 8 START view model data_science.order_flash_event............... [RUN]
2020-05-04 21:03:09.474147 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:03:09.474251 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.customer_broker).
2020-05-04 21:03:09.474356 (Thread-1): Compiling model.customer_history.order_flash_event
2020-05-04 21:03:09.482604 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_event"
2020-05-04 21:03:09.483038 (Thread-1): finished collecting timing info
2020-05-04 21:03:09.489744 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:03:09.489890 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" cascade
2020-05-04 21:03:09.730598 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-05-04 21:03:09.734012 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:03:09.734211 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-05-04 21:03:10.526781 (Thread-1): SQL status: DROP VIEW in 0.79 seconds
2020-05-04 21:03:10.528939 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_event"
2020-05-04 21:03:10.529513 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:03:10.529656 (Thread-1): On model.customer_history.order_flash_event: BEGIN
2020-05-04 21:03:10.566423 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:03:10.566611 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:03:10.566721 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */

  create view "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" as (
    with order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
)

select * FROM order_flash INNER JOIN events USING (event_unique_id)
  );

2020-05-04 21:03:10.797386 (Thread-1): SQL status: CREATE VIEW in 0.23 seconds
2020-05-04 21:03:10.801684 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:03:10.801841 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
alter table "data_platform_prod"."data_science"."order_flash_event__dbt_tmp" rename to "order_flash_event"
2020-05-04 21:03:10.843750 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:03:10.845697 (Thread-1): On model.customer_history.order_flash_event: COMMIT
2020-05-04 21:03:10.845892 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:03:10.846052 (Thread-1): On model.customer_history.order_flash_event: COMMIT
2020-05-04 21:03:11.202159 (Thread-1): SQL status: COMMIT in 0.36 seconds
2020-05-04 21:03:11.205409 (Thread-1): Using postgres connection "model.customer_history.order_flash_event".
2020-05-04 21:03:11.205557 (Thread-1): On model.customer_history.order_flash_event: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_event"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_event__dbt_backup" cascade
2020-05-04 21:03:12.341420 (Thread-1): SQL status: DROP VIEW in 1.14 seconds
2020-05-04 21:03:12.345608 (Thread-1): finished collecting timing info
2020-05-04 21:03:12.346760 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9329fe8-45b9-4b45-8e56-ee363957ff11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f86bd0>]}
2020-05-04 21:03:12.347142 (Thread-1): 14:03:12 | 7 of 8 OK created view model data_science.order_flash_event.......... [CREATE VIEW in 2.87s]
2020-05-04 21:03:12.347360 (Thread-1): Finished running node model.customer_history.order_flash_event
2020-05-04 21:03:12.347878 (Thread-1): Began running node model.customer_history.customers
2020-05-04 21:03:12.348185 (Thread-1): 14:03:12 | 8 of 8 START table model data_science.customers...................... [RUN]
2020-05-04 21:03:12.348713 (Thread-1): Acquiring new postgres connection "model.customer_history.customers".
2020-05-04 21:03:12.348910 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_event).
2020-05-04 21:03:12.349090 (Thread-1): Compiling model.customer_history.customers
2020-05-04 21:03:12.363191 (Thread-1): Writing injected SQL for node "model.customer_history.customers"
2020-05-04 21:03:12.363648 (Thread-1): finished collecting timing info
2020-05-04 21:03:12.386494 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:03:12.386653 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
drop table if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-05-04 21:03:12.551402 (Thread-1): SQL status: DROP TABLE in 0.16 seconds
2020-05-04 21:03:12.554867 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:03:12.555019 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
drop table if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-05-04 21:03:12.633149 (Thread-1): SQL status: DROP TABLE in 0.08 seconds
2020-05-04 21:03:12.635125 (Thread-1): Writing runtime SQL for node "model.customer_history.customers"
2020-05-04 21:03:12.635698 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:03:12.635888 (Thread-1): On model.customer_history.customers: BEGIN
2020-05-04 21:03:12.697594 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-05-04 21:03:12.697847 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:03:12.697961 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */


  create  table "data_platform_prod"."data_science"."customers__dbt_tmp"
  as (
    

with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash_event"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        COUNT(DISTINCT event_unique_id) AS number_of_events,
        SUM(amount_gross) AS total_revenue,

        SUM(FLOOR(COALESCE(datediff(days, onsale_date, sale_datetime), 0))) / COUNT(DISTINCT CASE WHEN (datediff(days, onsale_date, sale_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE(datediff(days, sale_datetime, event_datetime), 0)))/ COUNT(DISTINCT CASE WHEN (datediff(days, sale_datetime, event_datetime))IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.number_of_events,
        customer_orders.total_revenue,
        average_days_sold_after_onsale,
        average_days_sold_before_event,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );
2020-05-04 21:15:22.090340 (Thread-1): SQL status: SELECT in 729.39 seconds
2020-05-04 21:15:22.096380 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:15:22.096569 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-05-04 21:15:22.286852 (Thread-1): SQL status: ALTER TABLE in 0.19 seconds
2020-05-04 21:15:22.290499 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:15:22.290656 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-05-04 21:15:22.332769 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-04 21:15:22.334718 (Thread-1): On model.customer_history.customers: COMMIT
2020-05-04 21:15:22.334880 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:15:22.335009 (Thread-1): On model.customer_history.customers: COMMIT
2020-05-04 21:15:23.115695 (Thread-1): SQL status: COMMIT in 0.78 seconds
2020-05-04 21:15:23.119209 (Thread-1): Using postgres connection "model.customer_history.customers".
2020-05-04 21:15:23.119368 (Thread-1): On model.customer_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.customers"} */
drop table if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-05-04 21:15:23.548958 (Thread-1): SQL status: DROP TABLE in 0.43 seconds
2020-05-04 21:15:23.553201 (Thread-1): finished collecting timing info
2020-05-04 21:15:23.554073 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9329fe8-45b9-4b45-8e56-ee363957ff11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d6fad0>]}
2020-05-04 21:15:23.554380 (Thread-1): 14:15:23 | 8 of 8 OK created table model data_science.customers................. [SELECT in 731.21s]
2020-05-04 21:15:23.554557 (Thread-1): Finished running node model.customer_history.customers
2020-05-04 21:15:23.647056 (MainThread): Using postgres connection "master".
2020-05-04 21:15:23.647218 (MainThread): On master: BEGIN
2020-05-04 21:15:23.687459 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-04 21:15:23.687761 (MainThread): On master: COMMIT
2020-05-04 21:15:23.687934 (MainThread): Using postgres connection "master".
2020-05-04 21:15:23.688090 (MainThread): On master: COMMIT
2020-05-04 21:15:23.727370 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-04 21:15:23.728077 (MainThread): 14:15:23 | 
2020-05-04 21:15:23.728321 (MainThread): 14:15:23 | Finished running 7 view models, 1 table model in 751.69s.
2020-05-04 21:15:23.728516 (MainThread): Connection 'master' was left open.
2020-05-04 21:15:23.728670 (MainThread): On master: Close
2020-05-04 21:15:23.729070 (MainThread): Connection 'model.customer_history.customers' was left open.
2020-05-04 21:15:23.729231 (MainThread): On model.customer_history.customers: Close
2020-05-04 21:15:23.754102 (MainThread): 
2020-05-04 21:15:23.754308 (MainThread): Completed successfully
2020-05-04 21:15:23.754454 (MainThread): 
Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
2020-05-04 21:15:23.754656 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089f8b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a7e490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10873f290>]}
2020-05-04 21:15:23.754870 (MainThread): Flushing usage events
2020-05-06 00:41:02.437210 (MainThread): Running with dbt=0.16.1
2020-05-06 00:41:02.502542 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:41:02.504140 (MainThread): Tracking: tracking
2020-05-06 00:41:02.509217 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba5f090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bcbae50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba5f610>]}
2020-05-06 00:41:02.529106 (MainThread): Partial parsing not enabled
2020-05-06 00:41:02.532312 (MainThread): Parsing macros/core.sql
2020-05-06 00:41:02.539229 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:41:02.548798 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:41:02.552224 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:41:02.573508 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:41:02.608179 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:41:02.631175 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:41:02.635810 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:41:02.645797 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:41:02.660463 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:41:02.668621 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:41:02.677973 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:41:02.684696 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:41:02.686868 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:41:02.689087 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:41:02.691755 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:41:02.695197 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:41:02.707283 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:41:02.710518 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:41:02.713139 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:41:02.766513 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:41:02.768795 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:41:02.771628 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:41:02.774706 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:41:02.778792 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:41:02.782695 (MainThread): Parsing macros/relations.sql
2020-05-06 00:41:02.786374 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:41:02.808708 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:41:02.826848 (MainThread): Partial parsing not enabled
2020-05-06 00:41:02.854272 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:41:02.854385 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:02.871006 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:02.871111 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:02.875125 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:02.875215 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:02.879548 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:02.879636 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:02.883613 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:02.883700 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:02.887599 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:02.887685 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:02.938093 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be63590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf3dfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf2b750>]}
2020-05-06 00:41:02.938302 (MainThread): Flushing usage events
2020-05-06 00:41:03.249775 (MainThread): Connection 'model.customer_history.order_flash_events' was properly closed.
2020-05-06 00:41:03.250018 (MainThread): Encountered an error:
2020-05-06 00:41:03.250217 (MainThread): Compilation Error in model dim_customers (models/dim_customers.sql)
  Model 'model.customer_history.dim_customers' depends on a node named 'stg_customer' which was not found or is disabled
2020-05-06 00:41:03.263506 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 302, in process_manifest
    process_refs(manifest, project_name)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 553, in process_refs
    _process_refs_for_node(manifest, current_project, node)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 534, in _process_refs_for_node
    disabled=(isinstance(target_model, Disabled))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/utils.py", line 338, in invalid_ref_fail_unless_test
    target_model_package)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 488, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model dim_customers (models/dim_customers.sql)
  Model 'model.customer_history.dim_customers' depends on a node named 'stg_customer' which was not found or is disabled

2020-05-06 00:41:21.184083 (MainThread): Running with dbt=0.16.1
2020-05-06 00:41:21.248546 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:41:21.249451 (MainThread): Tracking: tracking
2020-05-06 00:41:21.254396 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3c6910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e64bc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e64bd50>]}
2020-05-06 00:41:21.275672 (MainThread): Partial parsing not enabled
2020-05-06 00:41:21.277482 (MainThread): Parsing macros/core.sql
2020-05-06 00:41:21.282062 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:41:21.290131 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:41:21.291901 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:41:21.309862 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:41:21.343351 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:41:21.364682 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:41:21.366615 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:41:21.373024 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:41:21.385801 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:41:21.392862 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:41:21.399325 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:41:21.404418 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:41:21.405394 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:41:21.406504 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:41:21.408218 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:41:21.410358 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:41:21.419866 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:41:21.422068 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:41:21.423144 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:41:21.472397 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:41:21.473609 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:41:21.474538 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:41:21.475635 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:41:21.477882 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:41:21.480217 (MainThread): Parsing macros/relations.sql
2020-05-06 00:41:21.482173 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:41:21.499703 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:41:21.517560 (MainThread): Partial parsing not enabled
2020-05-06 00:41:21.545012 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:41:21.545126 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:21.561753 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:21.561855 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:21.565874 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:21.565963 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:21.570322 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:21.570410 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:21.574354 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:21.574441 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:21.578354 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:21.578443 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:21.623198 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8e4090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7f7a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7f7c10>]}
2020-05-06 00:41:21.623406 (MainThread): Flushing usage events
2020-05-06 00:41:21.924534 (MainThread): Connection 'model.customer_history.order_flash_events' was properly closed.
2020-05-06 00:41:21.924783 (MainThread): Encountered an error:
2020-05-06 00:41:21.924985 (MainThread): Compilation Error in model dim_customers (models/dim_customers.sql)
  Model 'model.customer_history.dim_customers' depends on a node named 'stg_customer' which was not found or is disabled
2020-05-06 00:41:21.927036 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 302, in process_manifest
    process_refs(manifest, project_name)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 553, in process_refs
    _process_refs_for_node(manifest, current_project, node)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 534, in _process_refs_for_node
    disabled=(isinstance(target_model, Disabled))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/utils.py", line 338, in invalid_ref_fail_unless_test
    target_model_package)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 488, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model dim_customers (models/dim_customers.sql)
  Model 'model.customer_history.dim_customers' depends on a node named 'stg_customer' which was not found or is disabled

2020-05-06 00:41:26.605861 (MainThread): Running with dbt=0.16.1
2020-05-06 00:41:26.670902 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:41:26.671644 (MainThread): Tracking: tracking
2020-05-06 00:41:26.676427 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de33910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de3bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de3b590>]}
2020-05-06 00:41:26.695282 (MainThread): Partial parsing not enabled
2020-05-06 00:41:26.697163 (MainThread): Parsing macros/core.sql
2020-05-06 00:41:26.701841 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:41:26.710110 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:41:26.711899 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:41:26.730233 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:41:26.763924 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:41:26.785525 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:41:26.787484 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:41:26.793943 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:41:26.806867 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:41:26.813807 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:41:26.820108 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:41:26.825334 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:41:26.826315 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:41:26.827421 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:41:26.829130 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:41:26.831248 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:41:26.840419 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:41:26.842458 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:41:26.843548 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:41:26.885678 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:41:26.886908 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:41:26.887859 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:41:26.888978 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:41:26.891365 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:41:26.894663 (MainThread): Parsing macros/relations.sql
2020-05-06 00:41:26.896649 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:41:26.918757 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:41:26.937256 (MainThread): Partial parsing not enabled
2020-05-06 00:41:26.964793 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:41:26.964901 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:26.981233 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:26.981327 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:26.985320 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:26.985409 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:26.989726 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:26.989815 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:26.993745 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:26.993832 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:26.997745 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:26.997834 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:27.041978 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3329d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb7c7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e335550>]}
2020-05-06 00:41:27.042218 (MainThread): Flushing usage events
2020-05-06 00:41:27.352700 (MainThread): Connection 'model.customer_history.order_flash_events' was properly closed.
2020-05-06 00:41:27.352947 (MainThread): Encountered an error:
2020-05-06 00:41:27.353139 (MainThread): Compilation Error in model dim_customers (models/dim_customers.sql)
  Model 'model.customer_history.dim_customers' depends on a node named 'order_flash_event' which was not found or is disabled
2020-05-06 00:41:27.355134 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 302, in process_manifest
    process_refs(manifest, project_name)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 553, in process_refs
    _process_refs_for_node(manifest, current_project, node)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 534, in _process_refs_for_node
    disabled=(isinstance(target_model, Disabled))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/utils.py", line 338, in invalid_ref_fail_unless_test
    target_model_package)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 488, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model dim_customers (models/dim_customers.sql)
  Model 'model.customer_history.dim_customers' depends on a node named 'order_flash_event' which was not found or is disabled

2020-05-06 00:41:37.910752 (MainThread): Running with dbt=0.16.1
2020-05-06 00:41:37.977469 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:41:37.978531 (MainThread): Tracking: tracking
2020-05-06 00:41:37.985019 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084d7bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082841d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084ffd10>]}
2020-05-06 00:41:38.005326 (MainThread): Partial parsing not enabled
2020-05-06 00:41:38.007205 (MainThread): Parsing macros/core.sql
2020-05-06 00:41:38.012147 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:41:38.020308 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:41:38.022114 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:41:38.040396 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:41:38.074107 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:41:38.096222 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:41:38.098184 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:41:38.104612 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:41:38.117473 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:41:38.124393 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:41:38.130787 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:41:38.135892 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:41:38.136871 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:41:38.137961 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:41:38.139650 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:41:38.141743 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:41:38.151077 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:41:38.153296 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:41:38.154367 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:41:38.197797 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:41:38.199012 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:41:38.199953 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:41:38.201055 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:41:38.203309 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:41:38.205664 (MainThread): Parsing macros/relations.sql
2020-05-06 00:41:38.207144 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:41:38.224550 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:41:38.242863 (MainThread): Partial parsing not enabled
2020-05-06 00:41:38.270929 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:41:38.271053 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:38.288107 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:38.288236 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:38.292402 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:38.292507 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:38.297814 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:38.298049 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:38.302449 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:38.302562 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:38.308171 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:38.308339 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:38.456088 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-06 00:41:38.459179 (MainThread): 
2020-05-06 00:41:38.459553 (MainThread): Acquiring new postgres connection "master".
2020-05-06 00:41:38.459652 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:41:38.479687 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-06 00:41:38.479886 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-06 00:41:38.563774 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-06 00:41:38.563913 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-06 00:41:39.016030 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.45 seconds
2020-05-06 00:41:39.052746 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:41:39.052948 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-06 00:41:39.054552 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:41:39.054664 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-06 00:41:39.093895 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:41:39.094329 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:41:39.094502 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-06 00:41:39.162929 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.07 seconds
2020-05-06 00:41:39.171098 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-06 00:41:39.242594 (MainThread): Using postgres connection "master".
2020-05-06 00:41:39.242768 (MainThread): On master: BEGIN
2020-05-06 00:41:39.632602 (MainThread): SQL status: BEGIN in 0.39 seconds
2020-05-06 00:41:39.632804 (MainThread): Using postgres connection "master".
2020-05-06 00:41:39.632919 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-06 00:41:39.742336 (MainThread): SQL status: SELECT in 0.11 seconds
2020-05-06 00:41:39.816245 (MainThread): On master: ROLLBACK
2020-05-06 00:41:39.882318 (MainThread): Using postgres connection "master".
2020-05-06 00:41:39.882577 (MainThread): On master: BEGIN
2020-05-06 00:41:40.042352 (MainThread): SQL status: BEGIN in 0.16 seconds
2020-05-06 00:41:40.042669 (MainThread): On master: COMMIT
2020-05-06 00:41:40.042853 (MainThread): Using postgres connection "master".
2020-05-06 00:41:40.043013 (MainThread): On master: COMMIT
2020-05-06 00:41:40.125283 (MainThread): SQL status: COMMIT in 0.08 seconds
2020-05-06 00:41:40.125725 (MainThread): 17:41:40 | Concurrency: 1 threads (target='dev')
2020-05-06 00:41:40.125868 (MainThread): 17:41:40 | 
2020-05-06 00:41:40.128950 (Thread-1): Began running node model.customer_history.stg_events
2020-05-06 00:41:40.129148 (Thread-1): 17:41:40 | 1 of 6 START view model data_science.stg_events...................... [RUN]
2020-05-06 00:41:40.129449 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:40.129549 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-06 00:41:40.129654 (Thread-1): Compiling model.customer_history.stg_events
2020-05-06 00:41:40.143443 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-06 00:41:40.144320 (Thread-1): finished collecting timing info
2020-05-06 00:41:40.176799 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:40.176937 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-06 00:41:40.258419 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-06 00:41:40.262237 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:40.262398 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:41:40.304289 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-06 00:41:40.308118 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-06 00:41:40.308753 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:40.308906 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-06 00:41:40.349307 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:41:40.349611 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:40.349788 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-06 00:41:40.405059 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-05-06 00:41:40.409385 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:40.409540 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-06 00:41:40.452677 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:41:40.454169 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:41:40.454322 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:40.454450 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:41:41.501663 (Thread-1): SQL status: COMMIT in 1.05 seconds
2020-05-06 00:41:41.505098 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:41:41.505268 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:41:41.776029 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2020-05-06 00:41:41.780440 (Thread-1): finished collecting timing info
2020-05-06 00:41:41.781299 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2808dd29-edb7-4274-9a42-7a7a6c814bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108788890>]}
2020-05-06 00:41:41.781619 (Thread-1): 17:41:41 | 1 of 6 OK created view model data_science.stg_events................. [CREATE VIEW in 1.65s]
2020-05-06 00:41:41.781805 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-06 00:41:41.782058 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-06 00:41:41.782516 (Thread-1): 17:41:41 | 2 of 6 START view model data_science.stg_flash....................... [RUN]
2020-05-06 00:41:41.783022 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:41.783197 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-06 00:41:41.783334 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-06 00:41:41.789982 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-06 00:41:41.790472 (Thread-1): finished collecting timing info
2020-05-06 00:41:41.798257 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:41.798393 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-06 00:41:42.686414 (Thread-1): SQL status: DROP VIEW in 0.89 seconds
2020-05-06 00:41:42.690451 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:42.690638 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:41:42.929977 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-05-06 00:41:42.933049 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-06 00:41:42.933709 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:42.933880 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-06 00:41:42.973755 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:41:42.974185 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:42.974477 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-06 00:41:44.105171 (Thread-1): SQL status: CREATE VIEW in 1.13 seconds
2020-05-06 00:41:44.111100 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:44.111304 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-06 00:41:44.405743 (Thread-1): SQL status: ALTER TABLE in 0.29 seconds
2020-05-06 00:41:44.410216 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:44.410389 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-06 00:41:44.451979 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:41:44.453206 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:41:44.453338 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:44.453448 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:41:45.267945 (Thread-1): SQL status: COMMIT in 0.81 seconds
2020-05-06 00:41:45.271000 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:41:45.271181 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:41:45.961880 (Thread-1): SQL status: DROP VIEW in 0.69 seconds
2020-05-06 00:41:45.965580 (Thread-1): finished collecting timing info
2020-05-06 00:41:45.966450 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2808dd29-edb7-4274-9a42-7a7a6c814bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ae6510>]}
2020-05-06 00:41:45.966782 (Thread-1): 17:41:45 | 2 of 6 OK created view model data_science.stg_flash.................. [CREATE VIEW in 4.18s]
2020-05-06 00:41:45.966938 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-06 00:41:45.967096 (Thread-1): Began running node model.customer_history.stg_order
2020-05-06 00:41:45.967364 (Thread-1): 17:41:45 | 3 of 6 START view model data_science.stg_order....................... [RUN]
2020-05-06 00:41:45.967704 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:45.967829 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-06 00:41:45.967957 (Thread-1): Compiling model.customer_history.stg_order
2020-05-06 00:41:45.974356 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-06 00:41:45.974878 (Thread-1): finished collecting timing info
2020-05-06 00:41:46.016129 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:46.016310 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-06 00:41:46.546416 (Thread-1): SQL status: DROP VIEW in 0.53 seconds
2020-05-06 00:41:46.550095 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:46.550325 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:41:46.735562 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-06 00:41:46.738550 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-06 00:41:46.739231 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:46.739385 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-06 00:41:46.782655 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:41:46.782883 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:46.783007 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-06 00:41:48.551975 (Thread-1): SQL status: CREATE VIEW in 1.77 seconds
2020-05-06 00:41:48.557513 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:48.557671 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-06 00:41:49.044954 (Thread-1): SQL status: ALTER TABLE in 0.49 seconds
2020-05-06 00:41:49.049403 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:49.049569 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-06 00:41:49.094044 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:41:49.095764 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:41:49.095925 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:49.096057 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:41:49.703287 (Thread-1): SQL status: COMMIT in 0.61 seconds
2020-05-06 00:41:49.705647 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:41:49.705786 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:41:50.364615 (Thread-1): SQL status: DROP VIEW in 0.66 seconds
2020-05-06 00:41:50.367970 (Thread-1): finished collecting timing info
2020-05-06 00:41:50.368710 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2808dd29-edb7-4274-9a42-7a7a6c814bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b6e210>]}
2020-05-06 00:41:50.368976 (Thread-1): 17:41:50 | 3 of 6 OK created view model data_science.stg_order.................. [CREATE VIEW in 4.40s]
2020-05-06 00:41:50.369133 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-06 00:41:50.369293 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-06 00:41:50.369466 (Thread-1): 17:41:50 | 4 of 6 START view model data_science.stg_customers................... [RUN]
2020-05-06 00:41:50.370559 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:50.370736 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-06 00:41:50.370858 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-06 00:41:50.377334 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-06 00:41:50.377830 (Thread-1): finished collecting timing info
2020-05-06 00:41:50.385666 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:50.385835 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-06 00:41:50.918106 (Thread-1): SQL status: DROP VIEW in 0.53 seconds
2020-05-06 00:41:50.923423 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:50.923582 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:41:51.535364 (Thread-1): SQL status: DROP VIEW in 0.61 seconds
2020-05-06 00:41:51.537584 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-06 00:41:51.538158 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:51.538308 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-06 00:41:52.241105 (Thread-1): SQL status: BEGIN in 0.70 seconds
2020-05-06 00:41:52.241425 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:52.241606 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    with customers as (
    SELECT
        customer_unique_id,
        email,
        first_name,
        last_name
    From ticketing.customers
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
        customer_unique_id,
        email,
        CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
        first_name,
        last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-06 00:41:52.418242 (Thread-1): SQL status: CREATE VIEW in 0.18 seconds
2020-05-06 00:41:52.424683 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:52.424861 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-06 00:41:52.599058 (Thread-1): SQL status: ALTER TABLE in 0.17 seconds
2020-05-06 00:41:52.602960 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:52.603135 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-06 00:41:52.777974 (Thread-1): SQL status: ALTER TABLE in 0.17 seconds
2020-05-06 00:41:52.779522 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:41:52.779714 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:52.779854 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:41:52.983319 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-05-06 00:41:52.986065 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:41:52.986261 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:41:53.452567 (Thread-1): SQL status: DROP VIEW in 0.47 seconds
2020-05-06 00:41:53.456112 (Thread-1): finished collecting timing info
2020-05-06 00:41:53.457112 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2808dd29-edb7-4274-9a42-7a7a6c814bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087cd410>]}
2020-05-06 00:41:53.457375 (Thread-1): 17:41:53 | 4 of 6 OK created view model data_science.stg_customers.............. [CREATE VIEW in 3.09s]
2020-05-06 00:41:53.457526 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-06 00:41:53.457677 (Thread-1): Began running node model.customer_history.order_flash_events
2020-05-06 00:41:53.457915 (Thread-1): 17:41:53 | 5 of 6 START view model data_science.order_flash_events.............. [RUN]
2020-05-06 00:41:53.458218 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:53.458326 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-06 00:41:53.458433 (Thread-1): Compiling model.customer_history.order_flash_events
2020-05-06 00:41:53.467950 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:41:53.468361 (Thread-1): finished collecting timing info
2020-05-06 00:41:53.475096 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:53.475263 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" cascade
2020-05-06 00:41:55.374479 (Thread-1): SQL status: DROP VIEW in 1.90 seconds
2020-05-06 00:41:55.378900 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:55.379102 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_backup" cascade
2020-05-06 00:41:55.859001 (Thread-1): SQL status: DROP VIEW in 0.48 seconds
2020-05-06 00:41:55.860938 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:41:55.861458 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:55.861596 (Thread-1): On model.customer_history.order_flash_events: BEGIN
2020-05-06 00:41:55.925175 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-05-06 00:41:55.925407 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:41:55.925540 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */

  create view "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
order_flash as (
    SELECT
    *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state,
    event_unique_id,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event
    FROM order_flash INNER JOIN events USING (event_unique_id)
),

SELECT * FROM final
  );

2020-05-06 00:41:55.992339 (Thread-1): Postgres error: syntax error at or near "SELECT"
LINE 37: SELECT * FROM final
         ^

2020-05-06 00:41:55.992654 (Thread-1): On model.customer_history.order_flash_events: ROLLBACK
2020-05-06 00:41:56.042714 (Thread-1): finished collecting timing info
2020-05-06 00:41:56.043539 (Thread-1): Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
  syntax error at or near "SELECT"
  LINE 37: SELECT * FROM final
           ^
  compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "SELECT"
LINE 37: SELECT * FROM final
         ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
  syntax error at or near "SELECT"
  LINE 37: SELECT * FROM final
           ^
  compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
2020-05-06 00:41:56.061346 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2808dd29-edb7-4274-9a42-7a7a6c814bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088c7fd0>]}
2020-05-06 00:41:56.061625 (Thread-1): 17:41:56 | 5 of 6 ERROR creating view model data_science.order_flash_events..... [ERROR in 2.60s]
2020-05-06 00:41:56.061783 (Thread-1): Finished running node model.customer_history.order_flash_events
2020-05-06 00:41:56.062191 (Thread-1): Began running node model.customer_history.dim_customers
2020-05-06 00:41:56.062350 (Thread-1): 17:41:56 | 6 of 6 SKIP relation data_science.dim_customers...................... [SKIP]
2020-05-06 00:41:56.062494 (Thread-1): Finished running node model.customer_history.dim_customers
2020-05-06 00:41:56.083610 (MainThread): Using postgres connection "master".
2020-05-06 00:41:56.083890 (MainThread): On master: BEGIN
2020-05-06 00:41:56.122647 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:41:56.123006 (MainThread): On master: COMMIT
2020-05-06 00:41:56.123183 (MainThread): Using postgres connection "master".
2020-05-06 00:41:56.123340 (MainThread): On master: COMMIT
2020-05-06 00:41:56.161679 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:41:56.162633 (MainThread): 17:41:56 | 
2020-05-06 00:41:56.162883 (MainThread): 17:41:56 | Finished running 5 view models, 1 table model in 17.70s.
2020-05-06 00:41:56.163079 (MainThread): Connection 'master' was left open.
2020-05-06 00:41:56.163235 (MainThread): On master: Close
2020-05-06 00:41:56.163626 (MainThread): Connection 'model.customer_history.order_flash_events' was left open.
2020-05-06 00:41:56.163790 (MainThread): On model.customer_history.order_flash_events: Close
2020-05-06 00:41:56.185594 (MainThread): 
2020-05-06 00:41:56.185940 (MainThread): Completed with 1 error and 0 warnings:
2020-05-06 00:41:56.186192 (MainThread): 
2020-05-06 00:41:56.186349 (MainThread): Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
2020-05-06 00:41:56.186478 (MainThread):   syntax error at or near "SELECT"
2020-05-06 00:41:56.186595 (MainThread):   LINE 37: SELECT * FROM final
2020-05-06 00:41:56.186710 (MainThread):            ^
2020-05-06 00:41:56.186825 (MainThread):   compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
2020-05-06 00:41:56.186953 (MainThread): 
Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
2020-05-06 00:41:56.187161 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10868cf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085a6c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086e4610>]}
2020-05-06 00:41:56.187387 (MainThread): Flushing usage events
2020-05-06 00:44:37.121301 (MainThread): Running with dbt=0.16.1
2020-05-06 00:44:37.186846 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:44:37.187655 (MainThread): Tracking: tracking
2020-05-06 00:44:37.193142 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4afa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b7178d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4afe10>]}
2020-05-06 00:44:37.211565 (MainThread): Partial parsing not enabled
2020-05-06 00:44:37.214167 (MainThread): Parsing macros/core.sql
2020-05-06 00:44:37.218685 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:44:37.226985 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:44:37.228755 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:44:37.246970 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:44:37.280467 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:44:37.302918 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:44:37.305005 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:44:37.311833 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:44:37.332550 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:44:37.339842 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:44:37.346359 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:44:37.351494 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:44:37.352478 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:44:37.353592 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:44:37.355303 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:44:37.357620 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:44:37.366897 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:44:37.369732 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:44:37.370952 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:44:37.414284 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:44:37.415523 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:44:37.416481 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:44:37.417611 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:44:37.419911 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:44:37.422303 (MainThread): Parsing macros/relations.sql
2020-05-06 00:44:37.423822 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:44:37.441202 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:44:37.459414 (MainThread): Partial parsing not enabled
2020-05-06 00:44:37.487083 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:44:37.487188 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:44:37.503720 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:37.503814 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:44:37.507975 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:37.508065 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:44:37.512423 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:37.512521 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:44:37.516413 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:37.516497 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:44:37.520264 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:44:37.520348 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:44:37.664664 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-06 00:44:37.667267 (MainThread): 
2020-05-06 00:44:37.667553 (MainThread): Acquiring new postgres connection "master".
2020-05-06 00:44:37.667641 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:44:37.685799 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-06 00:44:37.685935 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-06 00:44:37.769539 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-06 00:44:37.769678 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-06 00:44:38.267646 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.50 seconds
2020-05-06 00:44:38.299891 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:44:38.300031 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-06 00:44:38.301797 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:44:38.301927 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-06 00:44:38.341714 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:44:38.341987 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:44:38.342152 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-06 00:44:38.403696 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.06 seconds
2020-05-06 00:44:38.408491 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-06 00:44:38.476356 (MainThread): Using postgres connection "master".
2020-05-06 00:44:38.476512 (MainThread): On master: BEGIN
2020-05-06 00:44:38.817243 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-05-06 00:44:38.817665 (MainThread): Using postgres connection "master".
2020-05-06 00:44:38.817938 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-06 00:44:38.904796 (MainThread): SQL status: SELECT in 0.09 seconds
2020-05-06 00:44:38.983806 (MainThread): On master: ROLLBACK
2020-05-06 00:44:39.021297 (MainThread): Using postgres connection "master".
2020-05-06 00:44:39.021704 (MainThread): On master: BEGIN
2020-05-06 00:44:39.096070 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-05-06 00:44:39.096321 (MainThread): On master: COMMIT
2020-05-06 00:44:39.096462 (MainThread): Using postgres connection "master".
2020-05-06 00:44:39.096585 (MainThread): On master: COMMIT
2020-05-06 00:44:39.133365 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:44:39.133796 (MainThread): 17:44:39 | Concurrency: 1 threads (target='dev')
2020-05-06 00:44:39.133955 (MainThread): 17:44:39 | 
2020-05-06 00:44:39.135502 (Thread-1): Began running node model.customer_history.stg_events
2020-05-06 00:44:39.135863 (Thread-1): 17:44:39 | 1 of 6 START view model data_science.stg_events...................... [RUN]
2020-05-06 00:44:39.136174 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:39.136276 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-06 00:44:39.136433 (Thread-1): Compiling model.customer_history.stg_events
2020-05-06 00:44:39.150396 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-06 00:44:39.150794 (Thread-1): finished collecting timing info
2020-05-06 00:44:39.182943 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:39.183084 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-06 00:44:39.263856 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-06 00:44:39.268079 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:39.268227 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:44:39.309311 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-06 00:44:39.312421 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-06 00:44:39.314197 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:39.314452 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-06 00:44:39.354604 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:44:39.354905 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:39.355081 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-06 00:44:40.073517 (Thread-1): SQL status: CREATE VIEW in 0.72 seconds
2020-05-06 00:44:40.078941 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:40.079083 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-06 00:44:40.121529 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:44:40.124985 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:40.125128 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-06 00:44:40.334510 (Thread-1): SQL status: ALTER TABLE in 0.21 seconds
2020-05-06 00:44:40.336409 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:44:40.336607 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:40.336765 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:44:40.560334 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-05-06 00:44:40.562821 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:44:40.563041 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:44:41.217530 (Thread-1): SQL status: DROP VIEW in 0.65 seconds
2020-05-06 00:44:41.220947 (Thread-1): finished collecting timing info
2020-05-06 00:44:41.221690 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bd32222-12c5-421f-a306-055acb5c67c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8eead0>]}
2020-05-06 00:44:41.221962 (Thread-1): 17:44:41 | 1 of 6 OK created view model data_science.stg_events................. [CREATE VIEW in 2.09s]
2020-05-06 00:44:41.222126 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-06 00:44:41.222295 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-06 00:44:41.222625 (Thread-1): 17:44:41 | 2 of 6 START view model data_science.stg_flash....................... [RUN]
2020-05-06 00:44:41.223058 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:41.223193 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-06 00:44:41.223311 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-06 00:44:41.230005 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-06 00:44:41.230512 (Thread-1): finished collecting timing info
2020-05-06 00:44:41.238196 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:41.238328 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-06 00:44:41.859674 (Thread-1): SQL status: DROP VIEW in 0.62 seconds
2020-05-06 00:44:41.863998 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:41.864159 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:44:42.036868 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-06 00:44:42.039669 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-06 00:44:42.040158 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:42.040285 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-06 00:44:42.080831 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:44:42.081262 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:42.081529 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-06 00:44:43.454103 (Thread-1): SQL status: CREATE VIEW in 1.37 seconds
2020-05-06 00:44:43.457970 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:43.458069 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-06 00:44:43.502641 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:44:43.505075 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:43.505178 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-06 00:44:43.545613 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:44:43.546557 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:44:43.546655 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:43.546733 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:44:43.941798 (Thread-1): SQL status: COMMIT in 0.39 seconds
2020-05-06 00:44:43.945193 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:44:43.945352 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:44:44.586183 (Thread-1): SQL status: DROP VIEW in 0.64 seconds
2020-05-06 00:44:44.588811 (Thread-1): finished collecting timing info
2020-05-06 00:44:44.589457 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bd32222-12c5-421f-a306-055acb5c67c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd2f850>]}
2020-05-06 00:44:44.589690 (Thread-1): 17:44:44 | 2 of 6 OK created view model data_science.stg_flash.................. [CREATE VIEW in 3.37s]
2020-05-06 00:44:44.589828 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-06 00:44:44.589967 (Thread-1): Began running node model.customer_history.stg_order
2020-05-06 00:44:44.590274 (Thread-1): 17:44:44 | 3 of 6 START view model data_science.stg_order....................... [RUN]
2020-05-06 00:44:44.590599 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:44.590708 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-06 00:44:44.590808 (Thread-1): Compiling model.customer_history.stg_order
2020-05-06 00:44:44.596245 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-06 00:44:44.596663 (Thread-1): finished collecting timing info
2020-05-06 00:44:44.632785 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:44.632976 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-06 00:44:45.191233 (Thread-1): SQL status: DROP VIEW in 0.56 seconds
2020-05-06 00:44:45.195357 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:45.195519 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:44:45.367854 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-06 00:44:45.370911 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-06 00:44:45.371587 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:45.371744 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-06 00:44:45.413927 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:44:45.414385 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:45.414686 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-06 00:44:45.756517 (Thread-1): SQL status: CREATE VIEW in 0.34 seconds
2020-05-06 00:44:45.762635 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:45.762793 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-06 00:44:45.807225 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:44:45.811615 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:45.811770 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-06 00:44:45.852801 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:44:45.854689 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:44:45.854886 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:45.855044 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:44:46.287135 (Thread-1): SQL status: COMMIT in 0.43 seconds
2020-05-06 00:44:46.289987 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:44:46.290145 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:44:46.844149 (Thread-1): SQL status: DROP VIEW in 0.55 seconds
2020-05-06 00:44:46.848013 (Thread-1): finished collecting timing info
2020-05-06 00:44:46.848879 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bd32222-12c5-421f-a306-055acb5c67c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdadc10>]}
2020-05-06 00:44:46.849203 (Thread-1): 17:44:46 | 3 of 6 OK created view model data_science.stg_order.................. [CREATE VIEW in 2.26s]
2020-05-06 00:44:46.849406 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-06 00:44:46.849612 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-06 00:44:46.849996 (Thread-1): 17:44:46 | 4 of 6 START view model data_science.stg_customers................... [RUN]
2020-05-06 00:44:46.850484 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:46.850614 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-06 00:44:46.850728 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-06 00:44:46.857395 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-06 00:44:46.857873 (Thread-1): finished collecting timing info
2020-05-06 00:44:46.864981 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:46.865123 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-06 00:44:47.134631 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2020-05-06 00:44:47.140049 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:47.140203 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:44:47.313738 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-06 00:44:47.316521 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-06 00:44:47.318198 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:47.318438 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-06 00:44:47.359048 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:44:47.359331 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:47.359497 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    with customers as (
    SELECT
        customer_unique_id,
        email,
        first_name,
        last_name
    From ticketing.customers
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
        customer_unique_id,
        email,
        CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
        first_name,
        last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-06 00:44:47.761930 (Thread-1): SQL status: CREATE VIEW in 0.40 seconds
2020-05-06 00:44:47.767202 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:47.767411 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-06 00:44:47.810146 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:44:47.812753 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:47.812869 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-06 00:44:48.780125 (Thread-1): SQL status: ALTER TABLE in 0.97 seconds
2020-05-06 00:44:48.781959 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:44:48.782163 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:48.782324 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:44:49.194873 (Thread-1): SQL status: COMMIT in 0.41 seconds
2020-05-06 00:44:49.198169 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:44:49.198333 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:44:49.675632 (Thread-1): SQL status: DROP VIEW in 0.48 seconds
2020-05-06 00:44:49.679389 (Thread-1): finished collecting timing info
2020-05-06 00:44:49.680452 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bd32222-12c5-421f-a306-055acb5c67c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdadc10>]}
2020-05-06 00:44:49.680736 (Thread-1): 17:44:49 | 4 of 6 OK created view model data_science.stg_customers.............. [CREATE VIEW in 2.83s]
2020-05-06 00:44:49.680916 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-06 00:44:49.681072 (Thread-1): Began running node model.customer_history.order_flash_events
2020-05-06 00:44:49.681228 (Thread-1): 17:44:49 | 5 of 6 START view model data_science.order_flash_events.............. [RUN]
2020-05-06 00:44:49.681930 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:44:49.682052 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-06 00:44:49.682161 (Thread-1): Compiling model.customer_history.order_flash_events
2020-05-06 00:44:49.691644 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:44:49.692113 (Thread-1): finished collecting timing info
2020-05-06 00:44:49.698624 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:44:49.698742 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" cascade
2020-05-06 00:44:49.980956 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2020-05-06 00:44:49.985503 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:44:49.985741 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_backup" cascade
2020-05-06 00:44:50.157609 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-06 00:44:50.160422 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:44:50.161151 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:44:50.161311 (Thread-1): On model.customer_history.order_flash_events: BEGIN
2020-05-06 00:44:50.201197 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:44:50.201627 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:44:50.201893 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */

  create view "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
order_flash as (
    SELECT
    *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state,
    event_unique_id,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
WHERE is_canceled is FALSE -- shall this condition live else?
  );

2020-05-06 00:44:50.244553 (Thread-1): Postgres error: column "is_canceled" does not exist in final

2020-05-06 00:44:50.244985 (Thread-1): On model.customer_history.order_flash_events: ROLLBACK
2020-05-06 00:44:50.285377 (Thread-1): finished collecting timing info
2020-05-06 00:44:50.286440 (Thread-1): Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
  column "is_canceled" does not exist in final
  compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column "is_canceled" does not exist in final


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
  column "is_canceled" does not exist in final
  compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
2020-05-06 00:44:50.289374 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bd32222-12c5-421f-a306-055acb5c67c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba4eb10>]}
2020-05-06 00:44:50.289670 (Thread-1): 17:44:50 | 5 of 6 ERROR creating view model data_science.order_flash_events..... [ERROR in 0.61s]
2020-05-06 00:44:50.289853 (Thread-1): Finished running node model.customer_history.order_flash_events
2020-05-06 00:44:50.290327 (Thread-1): Began running node model.customer_history.dim_customers
2020-05-06 00:44:50.290537 (Thread-1): 17:44:50 | 6 of 6 SKIP relation data_science.dim_customers...................... [SKIP]
2020-05-06 00:44:50.290713 (Thread-1): Finished running node model.customer_history.dim_customers
2020-05-06 00:44:50.372848 (MainThread): Using postgres connection "master".
2020-05-06 00:44:50.373252 (MainThread): On master: BEGIN
2020-05-06 00:44:50.411684 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:44:50.412196 (MainThread): On master: COMMIT
2020-05-06 00:44:50.412486 (MainThread): Using postgres connection "master".
2020-05-06 00:44:50.412662 (MainThread): On master: COMMIT
2020-05-06 00:44:50.450908 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:44:50.451587 (MainThread): 17:44:50 | 
2020-05-06 00:44:50.451834 (MainThread): 17:44:50 | Finished running 5 view models, 1 table model in 12.78s.
2020-05-06 00:44:50.452029 (MainThread): Connection 'master' was left open.
2020-05-06 00:44:50.452183 (MainThread): On master: Close
2020-05-06 00:44:50.452587 (MainThread): Connection 'model.customer_history.order_flash_events' was left open.
2020-05-06 00:44:50.452756 (MainThread): On model.customer_history.order_flash_events: Close
2020-05-06 00:44:50.471992 (MainThread): 
2020-05-06 00:44:50.472201 (MainThread): Completed with 1 error and 0 warnings:
2020-05-06 00:44:50.472340 (MainThread): 
2020-05-06 00:44:50.472472 (MainThread): Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
2020-05-06 00:44:50.472592 (MainThread):   column "is_canceled" does not exist in final
2020-05-06 00:44:50.472706 (MainThread):   compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
2020-05-06 00:44:50.472828 (MainThread): 
Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
2020-05-06 00:44:50.473027 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8fa8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b7ecc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b7ecb10>]}
2020-05-06 00:44:50.473244 (MainThread): Flushing usage events
2020-05-06 00:45:34.229593 (MainThread): Running with dbt=0.16.1
2020-05-06 00:45:34.294116 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:45:34.294855 (MainThread): Tracking: tracking
2020-05-06 00:45:34.300562 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c1c6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c434250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c4387d0>]}
2020-05-06 00:45:34.320686 (MainThread): Partial parsing not enabled
2020-05-06 00:45:34.322942 (MainThread): Parsing macros/core.sql
2020-05-06 00:45:34.327605 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:45:34.335766 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:45:34.337548 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:45:34.355624 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:45:34.389458 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:45:34.411122 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:45:34.413108 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:45:34.419461 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:45:34.432663 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:45:34.439508 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:45:34.445843 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:45:34.450861 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:45:34.451832 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:45:34.452932 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:45:34.454692 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:45:34.456801 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:45:34.466080 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:45:34.468093 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:45:34.469172 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:45:34.521612 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:45:34.522911 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:45:34.523906 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:45:34.525116 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:45:34.527575 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:45:34.530183 (MainThread): Parsing macros/relations.sql
2020-05-06 00:45:34.531676 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:45:34.549754 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:45:34.568350 (MainThread): Partial parsing not enabled
2020-05-06 00:45:34.596628 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:45:34.596738 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:34.613515 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:34.613615 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:34.617681 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:34.617770 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:34.622110 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:34.622198 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:34.626170 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:34.626258 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:34.630208 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:45:34.630295 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:34.777176 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-06 00:45:34.780093 (MainThread): 
2020-05-06 00:45:34.780386 (MainThread): Acquiring new postgres connection "master".
2020-05-06 00:45:34.780476 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:34.799056 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-06 00:45:34.799200 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-06 00:45:34.894421 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-06 00:45:34.894548 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-06 00:45:35.305963 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.41 seconds
2020-05-06 00:45:35.337846 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:45:35.338052 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-06 00:45:35.339705 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:45:35.339821 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-06 00:45:35.377876 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:35.378300 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:45:35.378568 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-06 00:45:35.438461 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.06 seconds
2020-05-06 00:45:35.445773 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-06 00:45:35.516005 (MainThread): Using postgres connection "master".
2020-05-06 00:45:35.516176 (MainThread): On master: BEGIN
2020-05-06 00:45:35.859788 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-05-06 00:45:35.860062 (MainThread): Using postgres connection "master".
2020-05-06 00:45:35.860224 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-06 00:45:35.948711 (MainThread): SQL status: SELECT in 0.09 seconds
2020-05-06 00:45:36.018482 (MainThread): On master: ROLLBACK
2020-05-06 00:45:36.055987 (MainThread): Using postgres connection "master".
2020-05-06 00:45:36.056392 (MainThread): On master: BEGIN
2020-05-06 00:45:36.131353 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-05-06 00:45:36.131667 (MainThread): On master: COMMIT
2020-05-06 00:45:36.131852 (MainThread): Using postgres connection "master".
2020-05-06 00:45:36.132009 (MainThread): On master: COMMIT
2020-05-06 00:45:36.171618 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:45:36.172498 (MainThread): 17:45:36 | Concurrency: 1 threads (target='dev')
2020-05-06 00:45:36.172748 (MainThread): 17:45:36 | 
2020-05-06 00:45:36.175142 (Thread-1): Began running node model.customer_history.stg_events
2020-05-06 00:45:36.175615 (Thread-1): 17:45:36 | 1 of 6 START view model data_science.stg_events...................... [RUN]
2020-05-06 00:45:36.176003 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:36.176142 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-06 00:45:36.176284 (Thread-1): Compiling model.customer_history.stg_events
2020-05-06 00:45:36.193178 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-06 00:45:36.193737 (Thread-1): finished collecting timing info
2020-05-06 00:45:36.235799 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:36.235976 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-06 00:45:36.312243 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-06 00:45:36.316601 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:36.316756 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:45:36.355259 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-06 00:45:36.358300 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-06 00:45:36.358960 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:36.359114 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-06 00:45:36.396918 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:36.397377 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:36.397612 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-06 00:45:36.603702 (Thread-1): SQL status: CREATE VIEW in 0.21 seconds
2020-05-06 00:45:36.609177 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:36.609342 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-06 00:45:36.650266 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:45:36.652920 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:36.653038 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-06 00:45:36.691391 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:45:36.692554 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:45:36.692682 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:36.692787 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:45:37.284062 (Thread-1): SQL status: COMMIT in 0.59 seconds
2020-05-06 00:45:37.287141 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:37.287311 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:45:37.466437 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-06 00:45:37.469343 (Thread-1): finished collecting timing info
2020-05-06 00:45:37.470067 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e4b884be-3d33-4d31-891c-b74187f4675f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c8e7d0>]}
2020-05-06 00:45:37.470337 (Thread-1): 17:45:37 | 1 of 6 OK created view model data_science.stg_events................. [CREATE VIEW in 1.29s]
2020-05-06 00:45:37.470495 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-06 00:45:37.498540 (MainThread): Cancelling query 'model.customer_history.stg_events' (28511)
2020-05-06 00:45:37.498813 (MainThread): Using postgres connection "master".
2020-05-06 00:45:37.498984 (MainThread): On master: BEGIN
2020-05-06 00:45:37.536845 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:37.537212 (MainThread): Using postgres connection "master".
2020-05-06 00:45:37.537357 (MainThread): On master: select pg_terminate_backend(28511)
2020-05-06 00:45:37.576455 (MainThread): SQL status: SELECT in 0.04 seconds
2020-05-06 00:45:37.576738 (MainThread): Cancel query 'model.customer_history.stg_events': (1,)
2020-05-06 00:45:37.576953 (MainThread): 17:45:37 | CANCEL query model.customer_history.stg_events....................... [CANCEL]
2020-05-06 00:45:37.577263 (MainThread): 
2020-05-06 00:45:37.577487 (MainThread): Exited because of keyboard interrupt.
2020-05-06 00:45:37.577678 (MainThread): 
Done. PASS=0 WARN=0 ERROR=0 SKIP=0 TOTAL=0
2020-05-06 00:45:37.577881 (MainThread): Connection 'master' was left open.
2020-05-06 00:45:37.615770 (MainThread): On master: Close
2020-05-06 00:45:37.616226 (MainThread): Connection 'model.customer_history.stg_events' was left open.
2020-05-06 00:45:37.616377 (MainThread): On model.customer_history.stg_events: Close
2020-05-06 00:45:37.616755 (MainThread): Flushing usage events
2020-05-06 00:45:37.919446 (MainThread): ctrl-c
2020-05-06 00:45:43.868766 (MainThread): Running with dbt=0.16.1
2020-05-06 00:45:43.932235 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:45:43.932972 (MainThread): Tracking: tracking
2020-05-06 00:45:43.937787 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f88bb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f60d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f88b190>]}
2020-05-06 00:45:43.956441 (MainThread): Partial parsing not enabled
2020-05-06 00:45:43.958374 (MainThread): Parsing macros/core.sql
2020-05-06 00:45:43.962949 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:45:43.971497 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:45:43.973321 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:45:43.994342 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:45:44.030255 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:45:44.051730 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:45:44.053713 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:45:44.060185 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:45:44.073358 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:45:44.080449 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:45:44.086887 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:45:44.092020 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:45:44.093006 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:45:44.094122 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:45:44.095868 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:45:44.098018 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:45:44.107301 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:45:44.109374 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:45:44.110468 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:45:44.153456 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:45:44.154665 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:45:44.155575 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:45:44.156660 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:45:44.158877 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:45:44.161162 (MainThread): Parsing macros/relations.sql
2020-05-06 00:45:44.162559 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:45:44.179044 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:45:44.196336 (MainThread): Partial parsing not enabled
2020-05-06 00:45:44.223327 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:45:44.223431 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:44.239874 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:44.239970 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:44.243890 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:44.243974 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:44.248379 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:44.248462 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:44.252311 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:44.252392 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:44.256293 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:45:44.256376 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:44.402776 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-06 00:45:44.405891 (MainThread): 
2020-05-06 00:45:44.406293 (MainThread): Acquiring new postgres connection "master".
2020-05-06 00:45:44.406376 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:45:44.424780 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-06 00:45:44.424919 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-06 00:45:44.510379 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-06 00:45:44.510516 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-06 00:45:44.887237 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.38 seconds
2020-05-06 00:45:44.919735 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:45:44.919874 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-06 00:45:44.921649 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:45:44.921776 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-06 00:45:44.961989 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:44.962419 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:45:44.962755 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-06 00:45:45.022482 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.06 seconds
2020-05-06 00:45:45.029763 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-06 00:45:45.100774 (MainThread): Using postgres connection "master".
2020-05-06 00:45:45.100919 (MainThread): On master: BEGIN
2020-05-06 00:45:45.457310 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-05-06 00:45:45.457523 (MainThread): Using postgres connection "master".
2020-05-06 00:45:45.457637 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-06 00:45:45.558335 (MainThread): SQL status: SELECT in 0.10 seconds
2020-05-06 00:45:45.633215 (MainThread): On master: ROLLBACK
2020-05-06 00:45:45.687850 (MainThread): Using postgres connection "master".
2020-05-06 00:45:45.688111 (MainThread): On master: BEGIN
2020-05-06 00:45:45.823495 (MainThread): SQL status: BEGIN in 0.14 seconds
2020-05-06 00:45:45.823943 (MainThread): On master: COMMIT
2020-05-06 00:45:45.824220 (MainThread): Using postgres connection "master".
2020-05-06 00:45:45.824369 (MainThread): On master: COMMIT
2020-05-06 00:45:45.863519 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:45:45.864392 (MainThread): 17:45:45 | Concurrency: 1 threads (target='dev')
2020-05-06 00:45:45.864627 (MainThread): 17:45:45 | 
2020-05-06 00:45:45.866870 (Thread-1): Began running node model.customer_history.stg_events
2020-05-06 00:45:45.867325 (Thread-1): 17:45:45 | 1 of 6 START view model data_science.stg_events...................... [RUN]
2020-05-06 00:45:45.867710 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:45.867847 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-06 00:45:45.867982 (Thread-1): Compiling model.customer_history.stg_events
2020-05-06 00:45:45.884475 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-06 00:45:45.885019 (Thread-1): finished collecting timing info
2020-05-06 00:45:45.925217 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:45.925372 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-06 00:45:46.007339 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-06 00:45:46.011640 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:46.011792 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:45:46.054184 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-06 00:45:46.057248 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-06 00:45:46.057927 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:46.058101 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-06 00:45:46.097945 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:46.098173 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:46.098307 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-06 00:45:46.531355 (Thread-1): SQL status: CREATE VIEW in 0.43 seconds
2020-05-06 00:45:46.538515 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:46.538658 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-06 00:45:46.581841 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:45:46.586250 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:46.586469 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-06 00:45:46.626731 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:45:46.628615 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:45:46.628811 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:46.628989 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:45:47.250045 (Thread-1): SQL status: COMMIT in 0.62 seconds
2020-05-06 00:45:47.252048 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:45:47.252172 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:45:47.964086 (Thread-1): SQL status: DROP VIEW in 0.71 seconds
2020-05-06 00:45:47.966933 (Thread-1): finished collecting timing info
2020-05-06 00:45:47.967637 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2a1c67a-862c-4cdd-80c6-23182ee9f087', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd92d10>]}
2020-05-06 00:45:47.967893 (Thread-1): 17:45:47 | 1 of 6 OK created view model data_science.stg_events................. [CREATE VIEW in 2.10s]
2020-05-06 00:45:47.968041 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-06 00:45:47.968196 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-06 00:45:47.968501 (Thread-1): 17:45:47 | 2 of 6 START view model data_science.stg_flash....................... [RUN]
2020-05-06 00:45:47.968795 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:47.968892 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-06 00:45:47.968985 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-06 00:45:47.974402 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-06 00:45:47.974842 (Thread-1): finished collecting timing info
2020-05-06 00:45:47.981578 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:47.981699 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-06 00:45:48.961457 (Thread-1): SQL status: DROP VIEW in 0.98 seconds
2020-05-06 00:45:48.964445 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:48.964587 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:45:49.158431 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-05-06 00:45:49.161527 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-06 00:45:49.162201 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:49.162377 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-06 00:45:49.214416 (Thread-1): SQL status: BEGIN in 0.05 seconds
2020-05-06 00:45:49.214744 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:49.214928 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-06 00:45:49.758318 (Thread-1): SQL status: CREATE VIEW in 0.54 seconds
2020-05-06 00:45:49.763085 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:49.763214 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-06 00:45:49.805242 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:45:49.810077 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:49.810232 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-06 00:45:49.851464 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:45:49.853439 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:45:49.853637 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:49.853797 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:45:50.303122 (Thread-1): SQL status: COMMIT in 0.45 seconds
2020-05-06 00:45:50.306540 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:45:50.306692 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:45:50.535477 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-05-06 00:45:50.539101 (Thread-1): finished collecting timing info
2020-05-06 00:45:50.539949 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2a1c67a-862c-4cdd-80c6-23182ee9f087', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd99790>]}
2020-05-06 00:45:50.540255 (Thread-1): 17:45:50 | 2 of 6 OK created view model data_science.stg_flash.................. [CREATE VIEW in 2.57s]
2020-05-06 00:45:50.540436 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-06 00:45:50.540675 (Thread-1): Began running node model.customer_history.stg_order
2020-05-06 00:45:50.541077 (Thread-1): 17:45:50 | 3 of 6 START view model data_science.stg_order....................... [RUN]
2020-05-06 00:45:50.541652 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:50.541785 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-06 00:45:50.541902 (Thread-1): Compiling model.customer_history.stg_order
2020-05-06 00:45:50.548318 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-06 00:45:50.548794 (Thread-1): finished collecting timing info
2020-05-06 00:45:50.588086 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:50.588276 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-06 00:45:51.290029 (Thread-1): SQL status: DROP VIEW in 0.70 seconds
2020-05-06 00:45:51.294098 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:51.294251 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:45:51.862456 (Thread-1): SQL status: DROP VIEW in 0.57 seconds
2020-05-06 00:45:51.865467 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-06 00:45:51.866160 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:51.866337 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-06 00:45:51.907218 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:51.907453 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:51.907588 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-06 00:45:52.408943 (Thread-1): SQL status: CREATE VIEW in 0.50 seconds
2020-05-06 00:45:52.415179 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:52.415330 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-06 00:45:52.589471 (Thread-1): SQL status: ALTER TABLE in 0.17 seconds
2020-05-06 00:45:52.593916 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:52.594090 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-06 00:45:52.771568 (Thread-1): SQL status: ALTER TABLE in 0.18 seconds
2020-05-06 00:45:52.773329 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:45:52.773522 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:52.773681 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:45:52.971098 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-05-06 00:45:52.974124 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:45:52.974291 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:45:53.664514 (Thread-1): SQL status: DROP VIEW in 0.69 seconds
2020-05-06 00:45:53.668292 (Thread-1): finished collecting timing info
2020-05-06 00:45:53.669141 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2a1c67a-862c-4cdd-80c6-23182ee9f087', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f87fe10>]}
2020-05-06 00:45:53.669447 (Thread-1): 17:45:53 | 3 of 6 OK created view model data_science.stg_order.................. [CREATE VIEW in 3.13s]
2020-05-06 00:45:53.669627 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-06 00:45:53.669812 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-06 00:45:53.670175 (Thread-1): 17:45:53 | 4 of 6 START view model data_science.stg_customers................... [RUN]
2020-05-06 00:45:53.670720 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:53.670994 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-06 00:45:53.671144 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-06 00:45:53.677378 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-06 00:45:53.677862 (Thread-1): finished collecting timing info
2020-05-06 00:45:53.685204 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:53.685337 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-06 00:45:54.181931 (Thread-1): SQL status: DROP VIEW in 0.50 seconds
2020-05-06 00:45:54.187299 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:54.187451 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:45:54.755908 (Thread-1): SQL status: DROP VIEW in 0.57 seconds
2020-05-06 00:45:54.758558 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-06 00:45:54.759161 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:54.759317 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-06 00:45:54.799636 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:54.799914 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:54.800080 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    with customers as (
    SELECT
        customer_unique_id,
        email,
        first_name,
        last_name
    From ticketing.customers
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
        customer_unique_id,
        email,
        CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
        first_name,
        last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-06 00:45:55.118963 (Thread-1): SQL status: CREATE VIEW in 0.32 seconds
2020-05-06 00:45:55.125362 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:55.125516 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-06 00:45:55.170613 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:45:55.174476 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:55.174631 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-06 00:45:55.216161 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:45:55.218220 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:45:55.218526 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:55.218752 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:45:56.303222 (Thread-1): SQL status: COMMIT in 1.08 seconds
2020-05-06 00:45:56.306862 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:45:56.307082 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:45:56.745233 (Thread-1): SQL status: DROP VIEW in 0.44 seconds
2020-05-06 00:45:56.749491 (Thread-1): finished collecting timing info
2020-05-06 00:45:56.750542 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2a1c67a-862c-4cdd-80c6-23182ee9f087', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fca2750>]}
2020-05-06 00:45:56.750848 (Thread-1): 17:45:56 | 4 of 6 OK created view model data_science.stg_customers.............. [CREATE VIEW in 3.08s]
2020-05-06 00:45:56.751027 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-06 00:45:56.751210 (Thread-1): Began running node model.customer_history.order_flash_events
2020-05-06 00:45:56.751395 (Thread-1): 17:45:56 | 5 of 6 START view model data_science.order_flash_events.............. [RUN]
2020-05-06 00:45:56.752041 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:45:56.752212 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-06 00:45:56.752337 (Thread-1): Compiling model.customer_history.order_flash_events
2020-05-06 00:45:56.762559 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:45:56.763045 (Thread-1): finished collecting timing info
2020-05-06 00:45:56.770427 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:45:56.770577 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" cascade
2020-05-06 00:45:57.450588 (Thread-1): SQL status: DROP VIEW in 0.68 seconds
2020-05-06 00:45:57.453326 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:45:57.453467 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_backup" cascade
2020-05-06 00:45:58.613609 (Thread-1): SQL status: DROP VIEW in 1.16 seconds
2020-05-06 00:45:58.616241 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:45:58.616959 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:45:58.617119 (Thread-1): On model.customer_history.order_flash_events: BEGIN
2020-05-06 00:45:58.658263 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:58.658559 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:45:58.658736 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */

  create view "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
order_flash as (
    SELECT
    *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state,
    event_unique_id,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
WHERE is_canceled is FALSE -- shall this condition live else?
  );

2020-05-06 00:45:58.900031 (Thread-1): Postgres error: column "event_unique_id" duplicated

2020-05-06 00:45:58.900472 (Thread-1): On model.customer_history.order_flash_events: ROLLBACK
2020-05-06 00:45:58.941189 (Thread-1): finished collecting timing info
2020-05-06 00:45:58.942245 (Thread-1): Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
  column "event_unique_id" duplicated
  compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.DuplicateColumn: column "event_unique_id" duplicated


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
  column "event_unique_id" duplicated
  compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
2020-05-06 00:45:58.945166 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2a1c67a-862c-4cdd-80c6-23182ee9f087', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc8bad0>]}
2020-05-06 00:45:58.945461 (Thread-1): 17:45:58 | 5 of 6 ERROR creating view model data_science.order_flash_events..... [ERROR in 2.19s]
2020-05-06 00:45:58.945656 (Thread-1): Finished running node model.customer_history.order_flash_events
2020-05-06 00:45:58.946233 (Thread-1): Began running node model.customer_history.dim_customers
2020-05-06 00:45:58.946520 (Thread-1): 17:45:58 | 6 of 6 SKIP relation data_science.dim_customers...................... [SKIP]
2020-05-06 00:45:58.946788 (Thread-1): Finished running node model.customer_history.dim_customers
2020-05-06 00:45:59.022620 (MainThread): Using postgres connection "master".
2020-05-06 00:45:59.022937 (MainThread): On master: BEGIN
2020-05-06 00:45:59.062382 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:45:59.062875 (MainThread): On master: COMMIT
2020-05-06 00:45:59.063178 (MainThread): Using postgres connection "master".
2020-05-06 00:45:59.063354 (MainThread): On master: COMMIT
2020-05-06 00:45:59.102701 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:45:59.103629 (MainThread): 17:45:59 | 
2020-05-06 00:45:59.103876 (MainThread): 17:45:59 | Finished running 5 view models, 1 table model in 14.70s.
2020-05-06 00:45:59.104072 (MainThread): Connection 'master' was left open.
2020-05-06 00:45:59.104228 (MainThread): On master: Close
2020-05-06 00:45:59.104620 (MainThread): Connection 'model.customer_history.order_flash_events' was left open.
2020-05-06 00:45:59.104784 (MainThread): On model.customer_history.order_flash_events: Close
2020-05-06 00:45:59.125578 (MainThread): 
2020-05-06 00:45:59.125794 (MainThread): Completed with 1 error and 0 warnings:
2020-05-06 00:45:59.125946 (MainThread): 
2020-05-06 00:45:59.126084 (MainThread): Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
2020-05-06 00:45:59.126209 (MainThread):   column "event_unique_id" duplicated
2020-05-06 00:45:59.126324 (MainThread):   compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
2020-05-06 00:45:59.126449 (MainThread): 
Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
2020-05-06 00:45:59.126648 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa75cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f8a6cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f8d4350>]}
2020-05-06 00:45:59.126865 (MainThread): Flushing usage events
2020-05-06 00:46:22.676705 (MainThread): Running with dbt=0.16.1
2020-05-06 00:46:22.742038 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:46:22.742964 (MainThread): Tracking: tracking
2020-05-06 00:46:22.747966 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112880450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128a7250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112880bd0>]}
2020-05-06 00:46:22.766277 (MainThread): Partial parsing not enabled
2020-05-06 00:46:22.768165 (MainThread): Parsing macros/core.sql
2020-05-06 00:46:22.772720 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:46:22.780835 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:46:22.782620 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:46:22.802294 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:46:22.836296 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:46:22.858078 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:46:22.860051 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:46:22.866509 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:46:22.879413 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:46:22.886460 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:46:22.892905 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:46:22.898003 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:46:22.898989 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:46:22.900113 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:46:22.901839 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:46:22.903989 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:46:22.913268 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:46:22.915331 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:46:22.916422 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:46:22.958095 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:46:22.959271 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:46:22.960210 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:46:22.961319 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:46:22.963594 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:46:22.965943 (MainThread): Parsing macros/relations.sql
2020-05-06 00:46:22.967314 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:46:22.986083 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:46:23.010071 (MainThread): Partial parsing not enabled
2020-05-06 00:46:23.038718 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:46:23.038839 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:46:23.055327 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:23.055423 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:46:23.059426 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:23.059514 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:46:23.063852 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:23.063939 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:46:23.067885 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:23.067972 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:46:23.071888 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:46:23.071978 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:46:23.207711 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-06 00:46:23.210188 (MainThread): 
2020-05-06 00:46:23.210449 (MainThread): Acquiring new postgres connection "master".
2020-05-06 00:46:23.210532 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:46:23.229481 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-06 00:46:23.229711 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-06 00:46:23.330219 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-06 00:46:23.330362 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-06 00:46:23.813132 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.48 seconds
2020-05-06 00:46:23.845534 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:46:23.845679 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-06 00:46:23.847473 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:46:23.847603 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-06 00:46:23.886215 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:46:23.886614 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:46:23.886871 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-06 00:46:23.944490 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.06 seconds
2020-05-06 00:46:23.950972 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-06 00:46:24.019468 (MainThread): Using postgres connection "master".
2020-05-06 00:46:24.019603 (MainThread): On master: BEGIN
2020-05-06 00:46:24.378957 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-05-06 00:46:24.379147 (MainThread): Using postgres connection "master".
2020-05-06 00:46:24.379268 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-06 00:46:24.469713 (MainThread): SQL status: SELECT in 0.09 seconds
2020-05-06 00:46:24.552437 (MainThread): On master: ROLLBACK
2020-05-06 00:46:24.593485 (MainThread): Using postgres connection "master".
2020-05-06 00:46:24.593752 (MainThread): On master: BEGIN
2020-05-06 00:46:24.673187 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-05-06 00:46:24.673438 (MainThread): On master: COMMIT
2020-05-06 00:46:24.673581 (MainThread): Using postgres connection "master".
2020-05-06 00:46:24.673706 (MainThread): On master: COMMIT
2020-05-06 00:46:24.714556 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:46:24.715183 (MainThread): 17:46:24 | Concurrency: 1 threads (target='dev')
2020-05-06 00:46:24.715380 (MainThread): 17:46:24 | 
2020-05-06 00:46:24.717231 (Thread-1): Began running node model.customer_history.stg_events
2020-05-06 00:46:24.717617 (Thread-1): 17:46:24 | 1 of 6 START view model data_science.stg_events...................... [RUN]
2020-05-06 00:46:24.717952 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:24.718064 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-06 00:46:24.718179 (Thread-1): Compiling model.customer_history.stg_events
2020-05-06 00:46:24.732255 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-06 00:46:24.732679 (Thread-1): finished collecting timing info
2020-05-06 00:46:24.765230 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:24.765370 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-06 00:46:24.843287 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-06 00:46:24.846907 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:24.847055 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:46:24.886127 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-06 00:46:24.889174 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-06 00:46:24.889814 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:24.889966 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-06 00:46:24.928161 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:46:24.928590 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:24.928865 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-06 00:46:25.245287 (Thread-1): SQL status: CREATE VIEW in 0.32 seconds
2020-05-06 00:46:25.252229 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:25.252369 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-06 00:46:25.292699 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:46:25.295860 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:25.296001 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-06 00:46:25.335589 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:46:25.337498 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:46:25.337697 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:25.337855 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:46:26.986511 (Thread-1): SQL status: COMMIT in 1.65 seconds
2020-05-06 00:46:26.990048 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:46:26.990212 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:46:28.294895 (Thread-1): SQL status: DROP VIEW in 1.30 seconds
2020-05-06 00:46:28.297822 (Thread-1): finished collecting timing info
2020-05-06 00:46:28.298633 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '979cdc54-7b55-4ed4-9672-83158e3adb3e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ed5ad0>]}
2020-05-06 00:46:28.299009 (Thread-1): 17:46:28 | 1 of 6 OK created view model data_science.stg_events................. [CREATE VIEW in 3.58s]
2020-05-06 00:46:28.299188 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-06 00:46:28.299365 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-06 00:46:28.299627 (Thread-1): 17:46:28 | 2 of 6 START view model data_science.stg_flash....................... [RUN]
2020-05-06 00:46:28.300021 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:28.300149 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-06 00:46:28.300264 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-06 00:46:28.306750 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-06 00:46:28.307281 (Thread-1): finished collecting timing info
2020-05-06 00:46:28.315523 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:28.315702 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-06 00:46:28.957723 (Thread-1): SQL status: DROP VIEW in 0.64 seconds
2020-05-06 00:46:28.961963 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:28.962118 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:46:29.459958 (Thread-1): SQL status: DROP VIEW in 0.50 seconds
2020-05-06 00:46:29.461742 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-06 00:46:29.462209 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:29.462324 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-06 00:46:29.501776 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:46:29.502208 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:29.502476 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-06 00:46:29.754836 (Thread-1): SQL status: CREATE VIEW in 0.25 seconds
2020-05-06 00:46:29.760455 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:29.760686 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-06 00:46:29.799687 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:46:29.802481 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:29.802604 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-06 00:46:31.350241 (Thread-1): SQL status: ALTER TABLE in 1.55 seconds
2020-05-06 00:46:31.351986 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:46:31.352154 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:31.352283 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:46:31.667352 (Thread-1): SQL status: COMMIT in 0.31 seconds
2020-05-06 00:46:31.670321 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:46:31.670474 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:46:32.581633 (Thread-1): SQL status: DROP VIEW in 0.91 seconds
2020-05-06 00:46:32.585862 (Thread-1): finished collecting timing info
2020-05-06 00:46:32.586718 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '979cdc54-7b55-4ed4-9672-83158e3adb3e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113124490>]}
2020-05-06 00:46:32.587022 (Thread-1): 17:46:32 | 2 of 6 OK created view model data_science.stg_flash.................. [CREATE VIEW in 4.29s]
2020-05-06 00:46:32.587204 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-06 00:46:32.587390 (Thread-1): Began running node model.customer_history.stg_order
2020-05-06 00:46:32.587692 (Thread-1): 17:46:32 | 3 of 6 START view model data_science.stg_order....................... [RUN]
2020-05-06 00:46:32.588070 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:32.588215 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-06 00:46:32.588329 (Thread-1): Compiling model.customer_history.stg_order
2020-05-06 00:46:32.594734 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-06 00:46:32.595232 (Thread-1): finished collecting timing info
2020-05-06 00:46:32.633955 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:32.634160 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-06 00:46:34.439372 (Thread-1): SQL status: DROP VIEW in 1.81 seconds
2020-05-06 00:46:34.442716 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:34.442857 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:46:34.812782 (Thread-1): SQL status: DROP VIEW in 0.37 seconds
2020-05-06 00:46:34.815273 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-06 00:46:34.815917 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:34.816070 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-06 00:46:34.855982 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:46:34.856176 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:34.856278 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-06 00:46:35.205703 (Thread-1): SQL status: CREATE VIEW in 0.35 seconds
2020-05-06 00:46:35.212066 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:35.212214 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-06 00:46:35.265700 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-06 00:46:35.269697 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:35.269873 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-06 00:46:35.311440 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:46:35.313352 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:46:35.313544 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:35.313708 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:46:36.437731 (Thread-1): SQL status: COMMIT in 1.12 seconds
2020-05-06 00:46:36.440988 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:46:36.441148 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:46:36.727614 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-05-06 00:46:36.731812 (Thread-1): finished collecting timing info
2020-05-06 00:46:36.732667 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '979cdc54-7b55-4ed4-9672-83158e3adb3e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11318d910>]}
2020-05-06 00:46:36.732976 (Thread-1): 17:46:36 | 3 of 6 OK created view model data_science.stg_order.................. [CREATE VIEW in 4.14s]
2020-05-06 00:46:36.733160 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-06 00:46:36.733400 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-06 00:46:36.733864 (Thread-1): 17:46:36 | 4 of 6 START view model data_science.stg_customers................... [RUN]
2020-05-06 00:46:36.734579 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:36.734743 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-06 00:46:36.734876 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-06 00:46:36.741409 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-06 00:46:36.741923 (Thread-1): finished collecting timing info
2020-05-06 00:46:36.749535 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:36.749670 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-06 00:46:37.207259 (Thread-1): SQL status: DROP VIEW in 0.46 seconds
2020-05-06 00:46:37.212640 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:37.212801 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:46:37.379667 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-06 00:46:37.381265 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-06 00:46:37.381667 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:37.381774 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-06 00:46:37.420435 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:46:37.420654 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:37.420787 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    with customers as (
    SELECT
        customer_unique_id,
        email,
        first_name,
        last_name
    From ticketing.customers
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
        customer_unique_id,
        email,
        CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
        first_name,
        last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-06 00:46:37.797840 (Thread-1): SQL status: CREATE VIEW in 0.38 seconds
2020-05-06 00:46:37.802883 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:37.803012 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-06 00:46:37.845665 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:46:37.849914 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:37.850067 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-06 00:46:37.888131 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:46:37.889856 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:46:37.890047 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:37.890176 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:46:38.328376 (Thread-1): SQL status: COMMIT in 0.44 seconds
2020-05-06 00:46:38.330526 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:46:38.330651 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:46:38.528299 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-05-06 00:46:38.532126 (Thread-1): finished collecting timing info
2020-05-06 00:46:38.533177 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '979cdc54-7b55-4ed4-9672-83158e3adb3e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11318d910>]}
2020-05-06 00:46:38.533499 (Thread-1): 17:46:38 | 4 of 6 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.80s]
2020-05-06 00:46:38.533693 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-06 00:46:38.533898 (Thread-1): Began running node model.customer_history.order_flash_events
2020-05-06 00:46:38.534219 (Thread-1): 17:46:38 | 5 of 6 START view model data_science.order_flash_events.............. [RUN]
2020-05-06 00:46:38.534619 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:46:38.534833 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-06 00:46:38.534978 (Thread-1): Compiling model.customer_history.order_flash_events
2020-05-06 00:46:38.545898 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:46:38.546386 (Thread-1): finished collecting timing info
2020-05-06 00:46:38.554151 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:46:38.554316 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" cascade
2020-05-06 00:46:39.050835 (Thread-1): SQL status: DROP VIEW in 0.50 seconds
2020-05-06 00:46:39.053455 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:46:39.053578 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_backup" cascade
2020-05-06 00:46:39.611588 (Thread-1): SQL status: DROP VIEW in 0.56 seconds
2020-05-06 00:46:39.614400 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:46:39.615078 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:46:39.615230 (Thread-1): On model.customer_history.order_flash_events: BEGIN
2020-05-06 00:46:39.656071 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:46:39.656515 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:46:39.656696 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */

  create view "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
order_flash as (
    SELECT
    *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    event_unique_id,
    ticket_id,
    ticket_state,
    event_unique_id,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
WHERE is_canceled is FALSE -- shall this condition live else?
  );

2020-05-06 00:46:39.712068 (Thread-1): Postgres error: column "event_unique_id" duplicated

2020-05-06 00:46:39.712349 (Thread-1): On model.customer_history.order_flash_events: ROLLBACK
2020-05-06 00:46:39.750060 (Thread-1): finished collecting timing info
2020-05-06 00:46:39.750910 (Thread-1): Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
  column "event_unique_id" duplicated
  compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.DuplicateColumn: column "event_unique_id" duplicated


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
  column "event_unique_id" duplicated
  compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
2020-05-06 00:46:39.754328 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '979cdc54-7b55-4ed4-9672-83158e3adb3e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112df6790>]}
2020-05-06 00:46:39.754642 (Thread-1): 17:46:39 | 5 of 6 ERROR creating view model data_science.order_flash_events..... [ERROR in 1.22s]
2020-05-06 00:46:39.754848 (Thread-1): Finished running node model.customer_history.order_flash_events
2020-05-06 00:46:39.755491 (Thread-1): Began running node model.customer_history.dim_customers
2020-05-06 00:46:39.755792 (Thread-1): 17:46:39 | 6 of 6 SKIP relation data_science.dim_customers...................... [SKIP]
2020-05-06 00:46:39.756038 (Thread-1): Finished running node model.customer_history.dim_customers
2020-05-06 00:46:39.794600 (MainThread): Using postgres connection "master".
2020-05-06 00:46:39.794863 (MainThread): On master: BEGIN
2020-05-06 00:46:39.834688 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:46:39.835005 (MainThread): On master: COMMIT
2020-05-06 00:46:39.835180 (MainThread): Using postgres connection "master".
2020-05-06 00:46:39.835332 (MainThread): On master: COMMIT
2020-05-06 00:46:39.874775 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:46:39.875668 (MainThread): 17:46:39 | 
2020-05-06 00:46:39.875955 (MainThread): 17:46:39 | Finished running 5 view models, 1 table model in 16.67s.
2020-05-06 00:46:39.876157 (MainThread): Connection 'master' was left open.
2020-05-06 00:46:39.876335 (MainThread): On master: Close
2020-05-06 00:46:39.876729 (MainThread): Connection 'model.customer_history.order_flash_events' was left open.
2020-05-06 00:46:39.876966 (MainThread): On model.customer_history.order_flash_events: Close
2020-05-06 00:46:39.897857 (MainThread): 
2020-05-06 00:46:39.898084 (MainThread): Completed with 1 error and 0 warnings:
2020-05-06 00:46:39.898234 (MainThread): 
2020-05-06 00:46:39.898372 (MainThread): Database Error in model order_flash_events (models/intermediate/order_flash_events.sql)
2020-05-06 00:46:39.898494 (MainThread):   column "event_unique_id" duplicated
2020-05-06 00:46:39.898609 (MainThread):   compiled SQL at target/run/customer_history/intermediate/order_flash_events.sql
2020-05-06 00:46:39.898734 (MainThread): 
Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
2020-05-06 00:46:39.898935 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f296d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f29050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bb6c90>]}
2020-05-06 00:46:39.899155 (MainThread): Flushing usage events
2020-05-06 00:50:39.070366 (MainThread): Running with dbt=0.16.1
2020-05-06 00:50:39.135490 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:50:39.136319 (MainThread): Tracking: tracking
2020-05-06 00:50:39.141733 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f8f6b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f6ae290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d0a8cd0>]}
2020-05-06 00:50:39.159848 (MainThread): Partial parsing not enabled
2020-05-06 00:50:39.161705 (MainThread): Parsing macros/core.sql
2020-05-06 00:50:39.166261 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:50:39.174199 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:50:39.175926 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:50:39.193949 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:50:39.226819 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:50:39.248853 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:50:39.250871 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:50:39.257373 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:50:39.270374 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:50:39.277354 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:50:39.283820 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:50:39.289061 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:50:39.290349 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:50:39.291681 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:50:39.293444 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:50:39.295622 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:50:39.305203 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:50:39.307326 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:50:39.308860 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:50:39.356239 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:50:39.357509 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:50:39.358479 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:50:39.359637 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:50:39.361980 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:50:39.364409 (MainThread): Parsing macros/relations.sql
2020-05-06 00:50:39.365823 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:50:39.383173 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:50:39.401475 (MainThread): Partial parsing not enabled
2020-05-06 00:50:39.429207 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:50:39.429325 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:50:39.446297 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:39.446471 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:50:39.450843 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:39.450932 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:50:39.455394 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:39.455482 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:50:39.459435 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:39.459523 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:50:39.463383 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:39.463474 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:50:39.600637 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-06 00:50:39.604068 (MainThread): 
2020-05-06 00:50:39.604456 (MainThread): Acquiring new postgres connection "master".
2020-05-06 00:50:39.604545 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:50:39.623543 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-06 00:50:39.623770 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-06 00:50:39.713856 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-06 00:50:39.713988 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-06 00:50:40.272034 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.56 seconds
2020-05-06 00:50:40.305043 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:50:40.305265 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-06 00:50:40.307077 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:50:40.307206 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-06 00:50:40.346391 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:50:40.346670 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:50:40.346856 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-06 00:50:40.414455 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.07 seconds
2020-05-06 00:50:40.421624 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-06 00:50:40.491949 (MainThread): Using postgres connection "master".
2020-05-06 00:50:40.492113 (MainThread): On master: BEGIN
2020-05-06 00:50:40.850587 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-05-06 00:50:40.851033 (MainThread): Using postgres connection "master".
2020-05-06 00:50:40.851352 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-06 00:50:40.943518 (MainThread): SQL status: SELECT in 0.09 seconds
2020-05-06 00:50:41.025315 (MainThread): On master: ROLLBACK
2020-05-06 00:50:41.063520 (MainThread): Using postgres connection "master".
2020-05-06 00:50:41.063788 (MainThread): On master: BEGIN
2020-05-06 00:50:41.140252 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-05-06 00:50:41.140715 (MainThread): On master: COMMIT
2020-05-06 00:50:41.141031 (MainThread): Using postgres connection "master".
2020-05-06 00:50:41.141200 (MainThread): On master: COMMIT
2020-05-06 00:50:41.179415 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:50:41.180330 (MainThread): 17:50:41 | Concurrency: 1 threads (target='dev')
2020-05-06 00:50:41.180591 (MainThread): 17:50:41 | 
2020-05-06 00:50:41.182928 (Thread-1): Began running node model.customer_history.stg_events
2020-05-06 00:50:41.183400 (Thread-1): 17:50:41 | 1 of 6 START view model data_science.stg_events...................... [RUN]
2020-05-06 00:50:41.183799 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:41.183943 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-06 00:50:41.184086 (Thread-1): Compiling model.customer_history.stg_events
2020-05-06 00:50:41.200347 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-06 00:50:41.200841 (Thread-1): finished collecting timing info
2020-05-06 00:50:41.240037 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:41.240197 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-06 00:50:41.319651 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-06 00:50:41.323977 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:41.324132 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:50:41.364885 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-06 00:50:41.367822 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-06 00:50:41.368504 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:41.368658 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-06 00:50:41.407722 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:50:41.408000 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:41.408170 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-06 00:50:41.540194 (Thread-1): SQL status: CREATE VIEW in 0.13 seconds
2020-05-06 00:50:41.547174 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:41.547312 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-06 00:50:41.588188 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:50:41.592516 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:41.592673 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-06 00:50:41.880242 (Thread-1): SQL status: ALTER TABLE in 0.29 seconds
2020-05-06 00:50:41.882215 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:50:41.882420 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:41.882580 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:50:42.434127 (Thread-1): SQL status: COMMIT in 0.55 seconds
2020-05-06 00:50:42.437630 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:50:42.437800 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:50:43.971171 (Thread-1): SQL status: DROP VIEW in 1.53 seconds
2020-05-06 00:50:43.976261 (Thread-1): finished collecting timing info
2020-05-06 00:50:43.977466 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87f64e8d-208f-4a9b-83f0-802124ea94f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f946f50>]}
2020-05-06 00:50:43.977877 (Thread-1): 17:50:43 | 1 of 6 OK created view model data_science.stg_events................. [CREATE VIEW in 2.79s]
2020-05-06 00:50:43.978135 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-06 00:50:43.978388 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-06 00:50:43.978748 (Thread-1): 17:50:43 | 2 of 6 START view model data_science.stg_flash....................... [RUN]
2020-05-06 00:50:43.979233 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:43.979405 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-06 00:50:43.979590 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-06 00:50:43.987158 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-06 00:50:43.987656 (Thread-1): finished collecting timing info
2020-05-06 00:50:43.994726 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:43.994869 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-06 00:50:44.333306 (Thread-1): SQL status: DROP VIEW in 0.34 seconds
2020-05-06 00:50:44.336035 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:44.336165 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:50:44.645842 (Thread-1): SQL status: DROP VIEW in 0.31 seconds
2020-05-06 00:50:44.648600 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-06 00:50:44.649200 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:44.649359 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-06 00:50:44.688493 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:50:44.688803 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:44.688999 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-06 00:50:45.342033 (Thread-1): SQL status: CREATE VIEW in 0.65 seconds
2020-05-06 00:50:45.348337 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:45.348516 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-06 00:50:45.394295 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-06 00:50:45.397885 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:45.398032 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-06 00:50:45.438063 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:50:45.439436 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:50:45.439591 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:45.439720 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:50:46.069147 (Thread-1): SQL status: COMMIT in 0.63 seconds
2020-05-06 00:50:46.072659 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:50:46.072823 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:50:46.324887 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-05-06 00:50:46.329267 (Thread-1): finished collecting timing info
2020-05-06 00:50:46.330143 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87f64e8d-208f-4a9b-83f0-802124ea94f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd02ad0>]}
2020-05-06 00:50:46.330460 (Thread-1): 17:50:46 | 2 of 6 OK created view model data_science.stg_flash.................. [CREATE VIEW in 2.35s]
2020-05-06 00:50:46.330649 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-06 00:50:46.330844 (Thread-1): Began running node model.customer_history.stg_order
2020-05-06 00:50:46.331048 (Thread-1): 17:50:46 | 3 of 6 START view model data_science.stg_order....................... [RUN]
2020-05-06 00:50:46.331634 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:46.331789 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-06 00:50:46.332042 (Thread-1): Compiling model.customer_history.stg_order
2020-05-06 00:50:46.339141 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-06 00:50:46.339632 (Thread-1): finished collecting timing info
2020-05-06 00:50:46.378732 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:46.378960 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-06 00:50:47.397817 (Thread-1): SQL status: DROP VIEW in 1.02 seconds
2020-05-06 00:50:47.401634 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:47.401793 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:50:47.578878 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-06 00:50:47.581540 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-06 00:50:47.582148 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:47.582350 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-06 00:50:47.624315 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:50:47.624620 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:47.624806 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-06 00:50:47.999700 (Thread-1): SQL status: CREATE VIEW in 0.37 seconds
2020-05-06 00:50:48.005768 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:48.005923 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-06 00:50:48.079918 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2020-05-06 00:50:48.083846 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:48.084010 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-06 00:50:48.149391 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2020-05-06 00:50:48.151348 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:50:48.151549 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:48.151709 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:50:49.030443 (Thread-1): SQL status: COMMIT in 0.88 seconds
2020-05-06 00:50:49.033457 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:50:49.033621 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:50:49.578840 (Thread-1): SQL status: DROP VIEW in 0.54 seconds
2020-05-06 00:50:49.583144 (Thread-1): finished collecting timing info
2020-05-06 00:50:49.583986 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87f64e8d-208f-4a9b-83f0-802124ea94f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff75f50>]}
2020-05-06 00:50:49.584293 (Thread-1): 17:50:49 | 3 of 6 OK created view model data_science.stg_order.................. [CREATE VIEW in 3.25s]
2020-05-06 00:50:49.584474 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-06 00:50:49.584660 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-06 00:50:49.584847 (Thread-1): 17:50:49 | 4 of 6 START view model data_science.stg_customers................... [RUN]
2020-05-06 00:50:49.585670 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:49.585846 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-06 00:50:49.585972 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-06 00:50:49.592483 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-06 00:50:49.592941 (Thread-1): finished collecting timing info
2020-05-06 00:50:49.600462 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:49.600602 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-06 00:50:50.185668 (Thread-1): SQL status: DROP VIEW in 0.58 seconds
2020-05-06 00:50:50.191197 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:50.191389 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:50:50.367693 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-05-06 00:50:50.370093 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-06 00:50:50.370697 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:50.370851 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-06 00:50:50.433775 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-05-06 00:50:50.434065 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:50.434238 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    with customers as (
    SELECT
        customer_unique_id,
        email,
        first_name,
        last_name
    From ticketing.customers
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
        customer_unique_id,
        email,
        CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
        first_name,
        last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-06 00:50:50.840001 (Thread-1): SQL status: CREATE VIEW in 0.41 seconds
2020-05-06 00:50:50.846351 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:50.846531 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-06 00:50:50.889677 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:50:50.893389 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:50.893549 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-06 00:50:50.936108 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:50:50.937302 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:50:50.937431 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:50.937535 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:50:51.533348 (Thread-1): SQL status: COMMIT in 0.60 seconds
2020-05-06 00:50:51.536145 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:50:51.536299 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:50:51.764077 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-05-06 00:50:51.767839 (Thread-1): finished collecting timing info
2020-05-06 00:50:51.768879 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87f64e8d-208f-4a9b-83f0-802124ea94f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa9eb10>]}
2020-05-06 00:50:51.769188 (Thread-1): 17:50:51 | 4 of 6 OK created view model data_science.stg_customers.............. [CREATE VIEW in 2.18s]
2020-05-06 00:50:51.769367 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-06 00:50:51.769563 (Thread-1): Began running node model.customer_history.order_flash_events
2020-05-06 00:50:51.769816 (Thread-1): 17:50:51 | 5 of 6 START view model data_science.order_flash_events.............. [RUN]
2020-05-06 00:50:51.770356 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:51.770513 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-06 00:50:51.770651 (Thread-1): Compiling model.customer_history.order_flash_events
2020-05-06 00:50:51.781098 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:50:51.781609 (Thread-1): finished collecting timing info
2020-05-06 00:50:51.789083 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:51.789282 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" cascade
2020-05-06 00:50:52.465138 (Thread-1): SQL status: DROP VIEW in 0.68 seconds
2020-05-06 00:50:52.469575 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:52.469745 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_backup" cascade
2020-05-06 00:50:53.012681 (Thread-1): SQL status: DROP VIEW in 0.54 seconds
2020-05-06 00:50:53.015452 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:50:53.016156 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:53.016324 (Thread-1): On model.customer_history.order_flash_events: BEGIN
2020-05-06 00:50:53.056231 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:50:53.056669 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:53.056946 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */

  create view "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
WHERE is_canceled is FALSE -- shall this condition live else?
  );

2020-05-06 00:50:53.652865 (Thread-1): SQL status: CREATE VIEW in 0.60 seconds
2020-05-06 00:50:53.658293 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:53.658463 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
alter table "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" rename to "order_flash_events"
2020-05-06 00:50:53.743243 (Thread-1): SQL status: ALTER TABLE in 0.08 seconds
2020-05-06 00:50:53.744644 (Thread-1): On model.customer_history.order_flash_events: COMMIT
2020-05-06 00:50:53.744804 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:53.744932 (Thread-1): On model.customer_history.order_flash_events: COMMIT
2020-05-06 00:50:54.022053 (Thread-1): SQL status: COMMIT in 0.28 seconds
2020-05-06 00:50:54.025242 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:50:54.025396 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_backup" cascade
2020-05-06 00:50:54.930525 (Thread-1): SQL status: DROP VIEW in 0.90 seconds
2020-05-06 00:50:54.934216 (Thread-1): finished collecting timing info
2020-05-06 00:50:54.935006 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87f64e8d-208f-4a9b-83f0-802124ea94f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff1d9d0>]}
2020-05-06 00:50:54.935284 (Thread-1): 17:50:54 | 5 of 6 OK created view model data_science.order_flash_events......... [CREATE VIEW in 3.16s]
2020-05-06 00:50:54.935440 (Thread-1): Finished running node model.customer_history.order_flash_events
2020-05-06 00:50:54.935925 (Thread-1): Began running node model.customer_history.dim_customers
2020-05-06 00:50:54.936182 (Thread-1): 17:50:54 | 6 of 6 START table model data_science.dim_customers.................. [RUN]
2020-05-06 00:50:54.936665 (Thread-1): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:50:54.936802 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events).
2020-05-06 00:50:54.936922 (Thread-1): Compiling model.customer_history.dim_customers
2020-05-06 00:50:54.947950 (Thread-1): Writing injected SQL for node "model.customer_history.dim_customers"
2020-05-06 00:50:54.948555 (Thread-1): finished collecting timing info
2020-05-06 00:50:54.976716 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 00:50:54.976880 (Thread-1): On model.customer_history.dim_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers__dbt_tmp" cascade
2020-05-06 00:50:55.472946 (Thread-1): SQL status: DROP TABLE in 0.50 seconds
2020-05-06 00:50:55.476527 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 00:50:55.476680 (Thread-1): On model.customer_history.dim_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers__dbt_backup" cascade
2020-05-06 00:50:55.647675 (Thread-1): SQL status: DROP TABLE in 0.17 seconds
2020-05-06 00:50:55.650396 (Thread-1): Writing runtime SQL for node "model.customer_history.dim_customers"
2020-05-06 00:50:55.651003 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 00:50:55.651162 (Thread-1): On model.customer_history.dim_customers: BEGIN
2020-05-06 00:50:55.825876 (Thread-1): SQL status: BEGIN in 0.17 seconds
2020-05-06 00:50:55.826219 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 00:50:55.826395 (Thread-1): On model.customer_history.dim_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers"} */


  create  table "data_platform_prod"."data_science"."dim_customers__dbt_tmp"
  as (
    

with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
orders as (
    select * from "data_platform_prod"."data_science"."order_flash_events"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        COUNT(DISTINCT event_unique_id) AS number_of_events,
        SUM(amount_gross) AS total_revenue,

        SUM(FLOOR(COALESCE(days_sold_after_onsale, 0))) / COUNT(DISTINCT CASE WHEN (days_sold_after_onsale IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE(days_sold_before_event, 0)))/ COUNT(DISTINCT CASE WHEN (days_sold_before_event IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from orders
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.number_of_events,
        customer_orders.total_revenue,
        average_days_sold_after_onsale,
        average_days_sold_before_event,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
),
select * from final
  );
2020-05-06 00:50:55.989266 (Thread-1): Postgres error: syntax error at or near "THEN"
LINE 27: ...DISTINCT CASE WHEN (days_sold_after_onsale IS NOT NULL THEN 
                                                                   ^

2020-05-06 00:50:55.989517 (Thread-1): On model.customer_history.dim_customers: ROLLBACK
2020-05-06 00:50:56.028836 (Thread-1): finished collecting timing info
2020-05-06 00:50:56.029518 (Thread-1): Database Error in model dim_customers (models/dim_customers.sql)
  syntax error at or near "THEN"
  LINE 27: ...DISTINCT CASE WHEN (days_sold_after_onsale IS NOT NULL THEN 
                                                                     ^
  compiled SQL at target/run/customer_history/dim_customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "THEN"
LINE 27: ...DISTINCT CASE WHEN (days_sold_after_onsale IS NOT NULL THEN 
                                                                   ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 62, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model dim_customers (models/dim_customers.sql)
  syntax error at or near "THEN"
  LINE 27: ...DISTINCT CASE WHEN (days_sold_after_onsale IS NOT NULL THEN 
                                                                     ^
  compiled SQL at target/run/customer_history/dim_customers.sql
2020-05-06 00:50:56.031706 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87f64e8d-208f-4a9b-83f0-802124ea94f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f8f6d10>]}
2020-05-06 00:50:56.031964 (Thread-1): 17:50:56 | 6 of 6 ERROR creating table model data_science.dim_customers......... [ERROR in 1.10s]
2020-05-06 00:50:56.032135 (Thread-1): Finished running node model.customer_history.dim_customers
2020-05-06 00:50:56.107130 (MainThread): Using postgres connection "master".
2020-05-06 00:50:56.107469 (MainThread): On master: BEGIN
2020-05-06 00:50:56.205040 (MainThread): SQL status: BEGIN in 0.10 seconds
2020-05-06 00:50:56.205508 (MainThread): On master: COMMIT
2020-05-06 00:50:56.205800 (MainThread): Using postgres connection "master".
2020-05-06 00:50:56.205958 (MainThread): On master: COMMIT
2020-05-06 00:50:56.350057 (MainThread): SQL status: COMMIT in 0.14 seconds
2020-05-06 00:50:56.350707 (MainThread): 17:50:56 | 
2020-05-06 00:50:56.350952 (MainThread): 17:50:56 | Finished running 5 view models, 1 table model in 16.75s.
2020-05-06 00:50:56.351159 (MainThread): Connection 'master' was left open.
2020-05-06 00:50:56.351326 (MainThread): On master: Close
2020-05-06 00:50:56.351713 (MainThread): Connection 'model.customer_history.dim_customers' was left open.
2020-05-06 00:50:56.351880 (MainThread): On model.customer_history.dim_customers: Close
2020-05-06 00:50:56.370943 (MainThread): 
2020-05-06 00:50:56.371165 (MainThread): Completed with 1 error and 0 warnings:
2020-05-06 00:50:56.371320 (MainThread): 
2020-05-06 00:50:56.371456 (MainThread): Database Error in model dim_customers (models/dim_customers.sql)
2020-05-06 00:50:56.371580 (MainThread):   syntax error at or near "THEN"
2020-05-06 00:50:56.371694 (MainThread):   LINE 27: ...DISTINCT CASE WHEN (days_sold_after_onsale IS NOT NULL THEN 
2020-05-06 00:50:56.371808 (MainThread):                                                                      ^
2020-05-06 00:50:56.371968 (MainThread):   compiled SQL at target/run/customer_history/dim_customers.sql
2020-05-06 00:50:56.372161 (MainThread): 
Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
2020-05-06 00:50:56.372396 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff39a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f684650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f9adb90>]}
2020-05-06 00:50:56.372632 (MainThread): Flushing usage events
2020-05-06 00:52:04.160623 (MainThread): Running with dbt=0.16.1
2020-05-06 00:52:04.225901 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-05-06 00:52:04.226765 (MainThread): Tracking: tracking
2020-05-06 00:52:04.232311 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110951a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bd6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bd6f50>]}
2020-05-06 00:52:04.258323 (MainThread): Partial parsing not enabled
2020-05-06 00:52:04.260448 (MainThread): Parsing macros/core.sql
2020-05-06 00:52:04.265138 (MainThread): Parsing macros/materializations/helpers.sql
2020-05-06 00:52:04.273292 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-05-06 00:52:04.275228 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-05-06 00:52:04.293401 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-05-06 00:52:04.327028 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-05-06 00:52:04.348730 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-05-06 00:52:04.350672 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-05-06 00:52:04.357188 (MainThread): Parsing macros/materializations/common/merge.sql
2020-05-06 00:52:04.370041 (MainThread): Parsing macros/materializations/table/table.sql
2020-05-06 00:52:04.376951 (MainThread): Parsing macros/materializations/view/view.sql
2020-05-06 00:52:04.383454 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-05-06 00:52:04.388503 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-05-06 00:52:04.389483 (MainThread): Parsing macros/etc/query.sql
2020-05-06 00:52:04.390590 (MainThread): Parsing macros/etc/is_incremental.sql
2020-05-06 00:52:04.392308 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-05-06 00:52:04.394448 (MainThread): Parsing macros/etc/datetime.sql
2020-05-06 00:52:04.403640 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-05-06 00:52:04.405695 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-05-06 00:52:04.406785 (MainThread): Parsing macros/adapters/common.sql
2020-05-06 00:52:04.449308 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-05-06 00:52:04.450779 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-05-06 00:52:04.451724 (MainThread): Parsing macros/schema_tests/unique.sql
2020-05-06 00:52:04.452875 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-05-06 00:52:04.455158 (MainThread): Parsing macros/catalog.sql
2020-05-06 00:52:04.457591 (MainThread): Parsing macros/relations.sql
2020-05-06 00:52:04.458963 (MainThread): Parsing macros/adapters.sql
2020-05-06 00:52:04.476515 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-05-06 00:52:04.494741 (MainThread): Partial parsing not enabled
2020-05-06 00:52:04.521488 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:52:04.521595 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:52:04.537818 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:04.537915 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:52:04.541790 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:04.541916 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:52:04.546194 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:04.546281 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:52:04.550314 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:04.550518 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:52:04.554770 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:04.554875 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:52:04.701357 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-05-06 00:52:04.704681 (MainThread): 
2020-05-06 00:52:04.704976 (MainThread): Acquiring new postgres connection "master".
2020-05-06 00:52:04.705068 (MainThread): Opening a new connection, currently in state init
2020-05-06 00:52:04.723409 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-05-06 00:52:04.723611 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-05-06 00:52:04.807395 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-05-06 00:52:04.807524 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-05-06 00:52:05.317521 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.51 seconds
2020-05-06 00:52:05.341192 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:52:05.341313 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-05-06 00:52:05.342649 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:52:05.342746 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-05-06 00:52:05.380477 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:52:05.380894 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-05-06 00:52:05.381158 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-05-06 00:52:05.439418 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.06 seconds
2020-05-06 00:52:05.447577 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-05-06 00:52:05.517118 (MainThread): Using postgres connection "master".
2020-05-06 00:52:05.517284 (MainThread): On master: BEGIN
2020-05-06 00:52:05.860563 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-05-06 00:52:05.860991 (MainThread): Using postgres connection "master".
2020-05-06 00:52:05.861254 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-05-06 00:52:05.955603 (MainThread): SQL status: SELECT in 0.09 seconds
2020-05-06 00:52:06.045111 (MainThread): On master: ROLLBACK
2020-05-06 00:52:06.083275 (MainThread): Using postgres connection "master".
2020-05-06 00:52:06.083682 (MainThread): On master: BEGIN
2020-05-06 00:52:06.160642 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-05-06 00:52:06.161106 (MainThread): On master: COMMIT
2020-05-06 00:52:06.161416 (MainThread): Using postgres connection "master".
2020-05-06 00:52:06.161573 (MainThread): On master: COMMIT
2020-05-06 00:52:06.200530 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 00:52:06.201378 (MainThread): 17:52:06 | Concurrency: 1 threads (target='dev')
2020-05-06 00:52:06.201641 (MainThread): 17:52:06 | 
2020-05-06 00:52:06.204220 (Thread-1): Began running node model.customer_history.stg_events
2020-05-06 00:52:06.204503 (Thread-1): 17:52:06 | 1 of 6 START view model data_science.stg_events...................... [RUN]
2020-05-06 00:52:06.204885 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.205023 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-05-06 00:52:06.205166 (Thread-1): Compiling model.customer_history.stg_events
2020-05-06 00:52:06.222355 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2020-05-06 00:52:06.223742 (Thread-1): finished collecting timing info
2020-05-06 00:52:06.264471 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.264633 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-05-06 00:52:06.340024 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-05-06 00:52:06.343591 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.343792 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:52:06.381005 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-05-06 00:52:06.383311 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2020-05-06 00:52:06.383761 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.383877 (Thread-1): On model.customer_history.stg_events: BEGIN
2020-05-06 00:52:06.421553 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:52:06.421995 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.422275 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id,
    onsale_date,
    event_datetime
FROM
    ticketing.events
    INNER JOIN analytics.event_onsale USING (event_unique_id)
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
      AND (nvl(ticketing.events.is_exclude,false)) is false
  );

2020-05-06 00:52:06.474881 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-05-06 00:52:06.481136 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.481295 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-05-06 00:52:06.522296 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:52:06.525964 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.526125 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-05-06 00:52:06.564785 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:52:06.566669 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:52:06.566871 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.567034 (Thread-1): On model.customer_history.stg_events: COMMIT
2020-05-06 00:52:06.804434 (Thread-1): SQL status: COMMIT in 0.24 seconds
2020-05-06 00:52:06.807909 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2020-05-06 00:52:06.808085 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-05-06 00:52:07.336701 (Thread-1): SQL status: DROP VIEW in 0.53 seconds
2020-05-06 00:52:07.339921 (Thread-1): finished collecting timing info
2020-05-06 00:52:07.340859 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d21164b-f32e-43ba-a807-3b35c6e7e536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110fd1710>]}
2020-05-06 00:52:07.341171 (Thread-1): 17:52:07 | 1 of 6 OK created view model data_science.stg_events................. [CREATE VIEW in 1.14s]
2020-05-06 00:52:07.341342 (Thread-1): Finished running node model.customer_history.stg_events
2020-05-06 00:52:07.341517 (Thread-1): Began running node model.customer_history.stg_flash
2020-05-06 00:52:07.341892 (Thread-1): 17:52:07 | 2 of 6 START view model data_science.stg_flash....................... [RUN]
2020-05-06 00:52:07.342533 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:07.342689 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2020-05-06 00:52:07.342861 (Thread-1): Compiling model.customer_history.stg_flash
2020-05-06 00:52:07.354357 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2020-05-06 00:52:07.354950 (Thread-1): finished collecting timing info
2020-05-06 00:52:07.367678 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:07.367966 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-05-06 00:52:07.720573 (Thread-1): SQL status: DROP VIEW in 0.35 seconds
2020-05-06 00:52:07.723643 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:07.723830 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:52:08.201888 (Thread-1): SQL status: DROP VIEW in 0.48 seconds
2020-05-06 00:52:08.203944 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2020-05-06 00:52:08.204488 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:08.204615 (Thread-1): On model.customer_history.stg_flash: BEGIN
2020-05-06 00:52:08.249546 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:52:08.249988 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:08.250201 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-05-06 00:52:08.398628 (Thread-1): SQL status: CREATE VIEW in 0.15 seconds
2020-05-06 00:52:08.403823 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:08.403976 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-05-06 00:52:08.450707 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-06 00:52:08.454908 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:08.455104 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-05-06 00:52:08.498470 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:52:08.499800 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:52:08.499951 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:08.500095 (Thread-1): On model.customer_history.stg_flash: COMMIT
2020-05-06 00:52:10.358074 (Thread-1): SQL status: COMMIT in 1.86 seconds
2020-05-06 00:52:10.361601 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2020-05-06 00:52:10.361761 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-05-06 00:52:10.772852 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-05-06 00:52:10.776433 (Thread-1): finished collecting timing info
2020-05-06 00:52:10.777274 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d21164b-f32e-43ba-a807-3b35c6e7e536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e569d0>]}
2020-05-06 00:52:10.777579 (Thread-1): 17:52:10 | 2 of 6 OK created view model data_science.stg_flash.................. [CREATE VIEW in 3.43s]
2020-05-06 00:52:10.777734 (Thread-1): Finished running node model.customer_history.stg_flash
2020-05-06 00:52:10.777886 (Thread-1): Began running node model.customer_history.stg_order
2020-05-06 00:52:10.778074 (Thread-1): 17:52:10 | 3 of 6 START view model data_science.stg_order....................... [RUN]
2020-05-06 00:52:10.778478 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:10.778602 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2020-05-06 00:52:10.778712 (Thread-1): Compiling model.customer_history.stg_order
2020-05-06 00:52:10.785812 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order"
2020-05-06 00:52:10.786257 (Thread-1): finished collecting timing info
2020-05-06 00:52:10.824502 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:10.824690 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-05-06 00:52:11.420994 (Thread-1): SQL status: DROP VIEW in 0.60 seconds
2020-05-06 00:52:11.424633 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:11.424800 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:52:11.687815 (Thread-1): SQL status: DROP VIEW in 0.26 seconds
2020-05-06 00:52:11.690543 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order"
2020-05-06 00:52:11.691148 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:11.691302 (Thread-1): On model.customer_history.stg_order: BEGIN
2020-05-06 00:52:11.729304 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:52:11.729606 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:11.729818 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
lower(zone_type_description)  in ('admissions', 'premium seating') AND 
is_canceled is FALSE -- where shall this condition lives?
  );

2020-05-06 00:52:11.798291 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-05-06 00:52:11.804518 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:11.804677 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-05-06 00:52:11.843949 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:52:11.848349 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:11.848506 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-05-06 00:52:11.886641 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:52:11.888334 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:52:11.888540 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:11.888669 (Thread-1): On model.customer_history.stg_order: COMMIT
2020-05-06 00:52:12.783727 (Thread-1): SQL status: COMMIT in 0.89 seconds
2020-05-06 00:52:12.786527 (Thread-1): Using postgres connection "model.customer_history.stg_order".
2020-05-06 00:52:12.786709 (Thread-1): On model.customer_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-05-06 00:52:12.975208 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-05-06 00:52:12.979510 (Thread-1): finished collecting timing info
2020-05-06 00:52:12.980473 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d21164b-f32e-43ba-a807-3b35c6e7e536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111f8c90>]}
2020-05-06 00:52:12.980822 (Thread-1): 17:52:12 | 3 of 6 OK created view model data_science.stg_order.................. [CREATE VIEW in 2.20s]
2020-05-06 00:52:12.981012 (Thread-1): Finished running node model.customer_history.stg_order
2020-05-06 00:52:12.981244 (Thread-1): Began running node model.customer_history.stg_customers
2020-05-06 00:52:12.981536 (Thread-1): 17:52:12 | 4 of 6 START view model data_science.stg_customers................... [RUN]
2020-05-06 00:52:12.982446 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:12.982603 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order).
2020-05-06 00:52:12.982739 (Thread-1): Compiling model.customer_history.stg_customers
2020-05-06 00:52:12.989363 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2020-05-06 00:52:12.989848 (Thread-1): finished collecting timing info
2020-05-06 00:52:12.998193 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:12.998334 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-05-06 00:52:13.610471 (Thread-1): SQL status: DROP VIEW in 0.61 seconds
2020-05-06 00:52:13.614568 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:13.614726 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:52:13.789887 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-06 00:52:13.792095 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2020-05-06 00:52:13.792722 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:13.792893 (Thread-1): On model.customer_history.stg_customers: BEGIN
2020-05-06 00:52:13.847848 (Thread-1): SQL status: BEGIN in 0.05 seconds
2020-05-06 00:52:13.848096 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:13.848270 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    with customers as (
    SELECT
        customer_unique_id,
        email,
        first_name,
        last_name
    From ticketing.customers
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
        customer_unique_id,
        email,
        CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
        first_name,
        last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-05-06 00:52:14.737348 (Thread-1): SQL status: CREATE VIEW in 0.89 seconds
2020-05-06 00:52:14.743200 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:14.743351 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-05-06 00:52:14.782479 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:52:14.785816 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:14.785990 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-05-06 00:52:14.832162 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-05-06 00:52:14.834113 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:52:14.834312 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:14.834474 (Thread-1): On model.customer_history.stg_customers: COMMIT
2020-05-06 00:52:15.050862 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-05-06 00:52:15.053857 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2020-05-06 00:52:15.054013 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-05-06 00:52:15.695049 (Thread-1): SQL status: DROP VIEW in 0.64 seconds
2020-05-06 00:52:15.698921 (Thread-1): finished collecting timing info
2020-05-06 00:52:15.699799 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d21164b-f32e-43ba-a807-3b35c6e7e536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ec05d0>]}
2020-05-06 00:52:15.700103 (Thread-1): 17:52:15 | 4 of 6 OK created view model data_science.stg_customers.............. [CREATE VIEW in 2.72s]
2020-05-06 00:52:15.700297 (Thread-1): Finished running node model.customer_history.stg_customers
2020-05-06 00:52:15.700490 (Thread-1): Began running node model.customer_history.order_flash_events
2020-05-06 00:52:15.700796 (Thread-1): 17:52:15 | 5 of 6 START view model data_science.order_flash_events.............. [RUN]
2020-05-06 00:52:15.701234 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:15.701390 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2020-05-06 00:52:15.701555 (Thread-1): Compiling model.customer_history.order_flash_events
2020-05-06 00:52:15.712451 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:52:15.712948 (Thread-1): finished collecting timing info
2020-05-06 00:52:15.720932 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:15.721128 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" cascade
2020-05-06 00:52:16.507704 (Thread-1): SQL status: DROP VIEW in 0.79 seconds
2020-05-06 00:52:16.511179 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:16.511301 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_backup" cascade
2020-05-06 00:52:16.681880 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-06 00:52:16.684629 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events"
2020-05-06 00:52:16.685325 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:16.685479 (Thread-1): On model.customer_history.order_flash_events: BEGIN
2020-05-06 00:52:16.747567 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-05-06 00:52:16.747796 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:16.747934 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */

  create view "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
WHERE is_canceled is FALSE -- shall this condition live else?
  );

2020-05-06 00:52:17.081807 (Thread-1): SQL status: CREATE VIEW in 0.33 seconds
2020-05-06 00:52:17.086270 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:17.086436 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
alter table "data_platform_prod"."data_science"."order_flash_events__dbt_tmp" rename to "order_flash_events"
2020-05-06 00:52:17.127871 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-05-06 00:52:17.129463 (Thread-1): On model.customer_history.order_flash_events: COMMIT
2020-05-06 00:52:17.129638 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:17.129769 (Thread-1): On model.customer_history.order_flash_events: COMMIT
2020-05-06 00:52:17.611784 (Thread-1): SQL status: COMMIT in 0.48 seconds
2020-05-06 00:52:17.613963 (Thread-1): Using postgres connection "model.customer_history.order_flash_events".
2020-05-06 00:52:17.614091 (Thread-1): On model.customer_history.order_flash_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events__dbt_backup" cascade
2020-05-06 00:52:17.783491 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-05-06 00:52:17.787044 (Thread-1): finished collecting timing info
2020-05-06 00:52:17.787802 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d21164b-f32e-43ba-a807-3b35c6e7e536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d77690>]}
2020-05-06 00:52:17.788083 (Thread-1): 17:52:17 | 5 of 6 OK created view model data_science.order_flash_events......... [CREATE VIEW in 2.09s]
2020-05-06 00:52:17.788249 (Thread-1): Finished running node model.customer_history.order_flash_events
2020-05-06 00:52:17.788666 (Thread-1): Began running node model.customer_history.dim_customers
2020-05-06 00:52:17.788861 (Thread-1): 17:52:17 | 6 of 6 START table model data_science.dim_customers.................. [RUN]
2020-05-06 00:52:17.789379 (Thread-1): Acquiring new postgres connection "model.customer_history.dim_customers".
2020-05-06 00:52:17.789515 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events).
2020-05-06 00:52:17.789637 (Thread-1): Compiling model.customer_history.dim_customers
2020-05-06 00:52:17.799610 (Thread-1): Writing injected SQL for node "model.customer_history.dim_customers"
2020-05-06 00:52:17.800160 (Thread-1): finished collecting timing info
2020-05-06 00:52:17.824041 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 00:52:17.824220 (Thread-1): On model.customer_history.dim_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers__dbt_tmp" cascade
2020-05-06 00:52:19.219791 (Thread-1): SQL status: DROP TABLE in 1.40 seconds
2020-05-06 00:52:19.223939 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 00:52:19.224130 (Thread-1): On model.customer_history.dim_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers__dbt_backup" cascade
2020-05-06 00:52:19.383240 (Thread-1): SQL status: DROP TABLE in 0.16 seconds
2020-05-06 00:52:19.385188 (Thread-1): Writing runtime SQL for node "model.customer_history.dim_customers"
2020-05-06 00:52:19.385705 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 00:52:19.385816 (Thread-1): On model.customer_history.dim_customers: BEGIN
2020-05-06 00:52:19.424245 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-05-06 00:52:19.424742 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 00:52:19.424924 (Thread-1): On model.customer_history.dim_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers"} */


  create  table "data_platform_prod"."data_science"."dim_customers__dbt_tmp"
  as (
    

with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
orders as (
    select * from "data_platform_prod"."data_science"."order_flash_events"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        COUNT(DISTINCT event_unique_id) AS number_of_events,
        SUM(amount_gross) AS total_revenue,

        SUM(FLOOR(COALESCE(days_sold_after_onsale, 0))) / COUNT(DISTINCT CASE WHEN days_sold_after_onsale IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE(days_sold_before_event, 0)))/ COUNT(DISTINCT CASE WHEN days_sold_before_event IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from orders
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customers.is_broker,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        customer_orders.tickets_sold_no_comps,
        customer_orders.number_of_orders,
        customer_orders.number_of_tickets_sold,
        customer_orders.number_of_events,
        customer_orders.total_revenue,
        average_days_sold_after_onsale,
        average_days_sold_before_event,
        customer_orders.count_transferred_tickets,
        customer_orders.count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );
2020-05-06 01:08:12.692312 (Thread-1): SQL status: SELECT in 953.27 seconds
2020-05-06 01:08:12.698107 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 01:08:12.698307 (Thread-1): On model.customer_history.dim_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers"} */
alter table "data_platform_prod"."data_science"."dim_customers__dbt_tmp" rename to "dim_customers"
2020-05-06 01:08:13.057760 (Thread-1): SQL status: ALTER TABLE in 0.36 seconds
2020-05-06 01:08:13.059738 (Thread-1): On model.customer_history.dim_customers: COMMIT
2020-05-06 01:08:13.059938 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 01:08:13.060101 (Thread-1): On model.customer_history.dim_customers: COMMIT
2020-05-06 01:08:14.194649 (Thread-1): SQL status: COMMIT in 1.13 seconds
2020-05-06 01:08:14.197371 (Thread-1): Using postgres connection "model.customer_history.dim_customers".
2020-05-06 01:08:14.197532 (Thread-1): On model.customer_history.dim_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers__dbt_backup" cascade
2020-05-06 01:08:14.418222 (Thread-1): SQL status: DROP TABLE in 0.22 seconds
2020-05-06 01:08:14.422748 (Thread-1): finished collecting timing info
2020-05-06 01:08:14.423630 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d21164b-f32e-43ba-a807-3b35c6e7e536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110efb190>]}
2020-05-06 01:08:14.423958 (Thread-1): 18:08:14 | 6 of 6 OK created table model data_science.dim_customers............. [SELECT in 956.63s]
2020-05-06 01:08:14.424149 (Thread-1): Finished running node model.customer_history.dim_customers
2020-05-06 01:08:14.479889 (MainThread): Using postgres connection "master".
2020-05-06 01:08:14.480171 (MainThread): On master: BEGIN
2020-05-06 01:08:14.520047 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-05-06 01:08:14.520503 (MainThread): On master: COMMIT
2020-05-06 01:08:14.520787 (MainThread): Using postgres connection "master".
2020-05-06 01:08:14.520975 (MainThread): On master: COMMIT
2020-05-06 01:08:14.558137 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-05-06 01:08:14.558790 (MainThread): 18:08:14 | 
2020-05-06 01:08:14.559042 (MainThread): 18:08:14 | Finished running 5 view models, 1 table model in 969.85s.
2020-05-06 01:08:14.559253 (MainThread): Connection 'master' was left open.
2020-05-06 01:08:14.559382 (MainThread): On master: Close
2020-05-06 01:08:14.559730 (MainThread): Connection 'model.customer_history.dim_customers' was left open.
2020-05-06 01:08:14.559865 (MainThread): On model.customer_history.dim_customers: Close
2020-05-06 01:08:14.580795 (MainThread): 
2020-05-06 01:08:14.581012 (MainThread): Completed successfully
2020-05-06 01:08:14.581168 (MainThread): 
Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
2020-05-06 01:08:14.581390 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e70b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10daf3810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ebc690>]}
2020-05-06 01:08:14.581616 (MainThread): Flushing usage events
