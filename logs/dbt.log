2020-04-27 22:39:52.042131 (MainThread): Running with dbt=0.16.1
2020-04-27 22:39:52.113931 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:39:52.115049 (MainThread): Tracking: tracking
2020-04-27 22:39:52.121281 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11281fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112828510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112828990>]}
2020-04-27 22:39:52.142575 (MainThread): Partial parsing not enabled
2020-04-27 22:39:52.146197 (MainThread): Parsing macros/core.sql
2020-04-27 22:39:52.153287 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:39:52.162754 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:39:52.166139 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:39:52.187242 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:39:52.221968 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:39:52.246754 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:39:52.251189 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:39:52.261106 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:39:52.275508 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:39:52.284214 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:39:52.292079 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:39:52.298336 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:39:52.300152 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:39:52.302039 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:39:52.304528 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:39:52.307422 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:39:52.317902 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:39:52.320710 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:39:52.323552 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:39:52.369041 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:39:52.371318 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:39:52.372942 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:39:52.374776 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:39:52.377906 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:39:52.381114 (MainThread): Parsing macros/relations.sql
2020-04-27 22:39:52.383323 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:39:52.401336 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:39:52.419373 (MainThread): Partial parsing not enabled
2020-04-27 22:39:52.446077 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:39:52.446177 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:39:52.461226 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:39:52.461317 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:39:52.467563 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:39:52.467805 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:39:52.607089 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:39:52.608371 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:39:52.608540 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d522d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d1b190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3bf10>]}
2020-04-27 22:39:52.608698 (MainThread): Flushing usage events
2020-04-27 22:39:52.925870 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:40:24.655588 (MainThread): Running with dbt=0.16.1
2020-04-27 22:40:24.717701 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models/staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:40:24.718402 (MainThread): Tracking: tracking
2020-04-27 22:40:24.723250 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108197fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10819b410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108188690>]}
2020-04-27 22:40:24.741250 (MainThread): Partial parsing not enabled
2020-04-27 22:40:24.743085 (MainThread): Parsing macros/core.sql
2020-04-27 22:40:24.747527 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:40:24.755424 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:40:24.757204 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:40:24.775508 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:40:24.816528 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:40:24.838032 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:40:24.839948 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:40:24.846366 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:40:24.859244 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:40:24.866229 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:40:24.872700 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:40:24.877900 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:40:24.878884 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:40:24.879995 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:40:24.881746 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:40:24.883901 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:40:24.893152 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:40:24.895215 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:40:24.896312 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:40:24.938530 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:40:24.939734 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:40:24.940694 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:40:24.941881 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:40:24.944175 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:40:24.946475 (MainThread): Parsing macros/relations.sql
2020-04-27 22:40:24.947810 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:40:24.964219 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:40:24.981655 (MainThread): Partial parsing not enabled
2020-04-27 22:40:25.009918 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:40:25.010018 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:25.025207 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:40:25.025294 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:25.029282 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:40:25.029363 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:25.158546 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:40:25.160696 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:40:25.160960 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086bf7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108708910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10876a4d0>]}
2020-04-27 22:40:25.161165 (MainThread): Flushing usage events
2020-04-27 22:40:25.470004 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:40:32.161212 (MainThread): Running with dbt=0.16.1
2020-04-27 22:40:32.233420 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:40:32.234180 (MainThread): Tracking: tracking
2020-04-27 22:40:32.238862 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114bbb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b3f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b34d0>]}
2020-04-27 22:40:32.261058 (MainThread): Partial parsing not enabled
2020-04-27 22:40:32.263210 (MainThread): Parsing macros/core.sql
2020-04-27 22:40:32.268007 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:40:32.276384 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:40:32.278216 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:40:32.296983 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:40:32.331332 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:40:32.353072 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:40:32.355018 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:40:32.361419 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:40:32.374615 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:40:32.381628 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:40:32.388218 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:40:32.393291 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:40:32.394265 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:40:32.395388 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:40:32.397095 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:40:32.399230 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:40:32.408419 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:40:32.410471 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:40:32.411561 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:40:32.453955 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:40:32.455153 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:40:32.456084 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:40:32.457188 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:40:32.459492 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:40:32.462561 (MainThread): Parsing macros/relations.sql
2020-04-27 22:40:32.464132 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:40:32.487015 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:40:32.505129 (MainThread): Partial parsing not enabled
2020-04-27 22:40:32.531841 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:40:32.531946 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:32.547555 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:40:32.547641 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:32.551552 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:40:32.551636 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:40:32.673820 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:40:32.675159 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:40:32.675314 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a6bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119ebf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a38450>]}
2020-04-27 22:40:32.675467 (MainThread): Flushing usage events
2020-04-27 22:40:32.983675 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:41:21.278144 (MainThread): Running with dbt=0.16.1
2020-04-27 22:41:21.352066 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:41:21.352817 (MainThread): Tracking: tracking
2020-04-27 22:41:21.357579 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075d1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075d1ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107856290>]}
2020-04-27 22:41:21.376205 (MainThread): Partial parsing not enabled
2020-04-27 22:41:21.378025 (MainThread): Parsing macros/core.sql
2020-04-27 22:41:21.382740 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:41:21.390768 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:41:21.392543 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:41:21.410536 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:41:21.444044 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:41:21.465234 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:41:21.467122 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:41:21.473408 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:41:21.486112 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:41:21.492875 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:41:21.499124 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:41:21.504060 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:41:21.505011 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:41:21.506086 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:41:21.507761 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:41:21.509837 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:41:21.518770 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:41:21.520760 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:41:21.521824 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:41:21.562444 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:41:21.563576 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:41:21.564478 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:41:21.565595 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:41:21.567794 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:41:21.570069 (MainThread): Parsing macros/relations.sql
2020-04-27 22:41:21.571398 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:41:21.588160 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:41:21.606453 (MainThread): Partial parsing not enabled
2020-04-27 22:41:21.641959 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:41:21.642088 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:41:21.658570 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:41:21.658674 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:41:21.662637 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:41:21.662727 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:41:21.788849 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:41:21.790522 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:41:21.790814 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107befe10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107befb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bef250>]}
2020-04-27 22:41:21.791007 (MainThread): Flushing usage events
2020-04-27 22:41:22.169616 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:43:55.998498 (MainThread): Running with dbt=0.16.1
2020-04-27 22:43:56.062845 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:43:56.063755 (MainThread): Tracking: tracking
2020-04-27 22:43:56.069527 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d31bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d270d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d42490>]}
2020-04-27 22:43:56.089438 (MainThread): Partial parsing not enabled
2020-04-27 22:43:56.091205 (MainThread): Parsing macros/core.sql
2020-04-27 22:43:56.095578 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:43:56.103771 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:43:56.105467 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:43:56.122828 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:43:56.155702 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:43:56.177941 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:43:56.180544 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:43:56.190655 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:43:56.203940 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:43:56.210963 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:43:56.217408 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:43:56.222520 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:43:56.223504 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:43:56.225016 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:43:56.226993 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:43:56.229454 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:43:56.238790 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:43:56.240846 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:43:56.241947 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:43:56.284490 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:43:56.285679 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:43:56.286625 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:43:56.287762 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:43:56.290079 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:43:56.292470 (MainThread): Parsing macros/relations.sql
2020-04-27 22:43:56.293865 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:43:56.311031 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:43:56.329287 (MainThread): Partial parsing not enabled
2020-04-27 22:43:56.356362 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:43:56.356458 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:43:56.372232 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:43:56.372331 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:43:56.376174 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:43:56.376259 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:43:56.507054 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:43:56.509029 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:43:56.509362 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11226a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122ca150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112274350>]}
2020-04-27 22:43:56.509544 (MainThread): Flushing usage events
2020-04-27 22:43:56.828615 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:45:44.307277 (MainThread): Running with dbt=0.16.1
2020-04-27 22:45:44.381549 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['models.staging'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:45:44.382292 (MainThread): Tracking: tracking
2020-04-27 22:45:44.386657 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfac650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d227e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfac210>]}
2020-04-27 22:45:44.406244 (MainThread): Partial parsing not enabled
2020-04-27 22:45:44.408197 (MainThread): Parsing macros/core.sql
2020-04-27 22:45:44.412924 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:45:44.420885 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:45:44.422641 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:45:44.440519 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:45:44.473728 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:45:44.494891 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:45:44.496805 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:45:44.503247 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:45:44.516204 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:45:44.523138 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:45:44.529526 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:45:44.534816 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:45:44.535795 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:45:44.536898 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:45:44.538609 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:45:44.540777 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:45:44.549968 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:45:44.551923 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:45:44.553011 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:45:44.595298 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:45:44.596447 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:45:44.597509 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:45:44.598598 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:45:44.600835 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:45:44.603138 (MainThread): Parsing macros/relations.sql
2020-04-27 22:45:44.604488 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:45:44.620940 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:45:44.638654 (MainThread): Partial parsing not enabled
2020-04-27 22:45:44.666267 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:45:44.666395 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:45:44.687130 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:45:44.687317 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:45:44.692816 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:45:44.692917 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:45:44.818216 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:45:44.820200 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2020-04-27 22:45:44.820395 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5773d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4eaa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4d6410>]}
2020-04-27 22:45:44.820570 (MainThread): Flushing usage events
2020-04-27 22:45:45.140926 (MainThread): Connection 'model.order_history.stg_orders_aggregate' was properly closed.
2020-04-27 22:48:37.990790 (MainThread): Running with dbt=0.16.1
2020-04-27 22:48:38.055236 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:48:38.056067 (MainThread): Tracking: tracking
2020-04-27 22:48:38.061211 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a1b6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a18850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a18a10>]}
2020-04-27 22:48:38.083369 (MainThread): Partial parsing not enabled
2020-04-27 22:48:38.085625 (MainThread): Parsing macros/core.sql
2020-04-27 22:48:38.091390 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:48:38.102370 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:48:38.105588 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:48:38.127475 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:48:38.161641 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:48:38.183809 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:48:38.185792 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:48:38.192346 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:48:38.205647 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:48:38.212893 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:48:38.219490 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:48:38.224717 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:48:38.225725 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:48:38.226895 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:48:38.228662 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:48:38.230858 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:48:38.240411 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:48:38.242517 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:48:38.243722 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:48:38.287734 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:48:38.288935 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:48:38.289871 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:48:38.290979 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:48:38.293379 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:48:38.295723 (MainThread): Parsing macros/relations.sql
2020-04-27 22:48:38.297080 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:48:38.313989 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:48:38.331642 (MainThread): Partial parsing not enabled
2020-04-27 22:48:38.359126 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:48:38.359235 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:48:38.375647 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:38.375747 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:48:38.379773 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:38.379859 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:48:38.511822 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:48:38.513931 (MainThread): 
2020-04-27 22:48:38.514219 (MainThread): Acquiring new postgres connection "master".
2020-04-27 22:48:38.514304 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:48:38.525367 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 22:48:38.525476 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 22:48:38.615820 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 22:48:38.615960 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 22:48:39.165381 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.55 seconds
2020-04-27 22:48:39.188459 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:48:39.188685 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 22:48:39.190358 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:48:39.190469 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 22:48:39.254970 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.06 seconds
2020-04-27 22:48:39.255380 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:48:39.255636 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 22:48:39.378616 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-27 22:48:39.380655 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 22:48:39.452209 (MainThread): Using postgres connection "master".
2020-04-27 22:48:39.452371 (MainThread): On master: BEGIN
2020-04-27 22:48:39.856936 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-27 22:48:39.857374 (MainThread): Using postgres connection "master".
2020-04-27 22:48:39.857696 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 22:48:40.075637 (MainThread): SQL status: SELECT in 0.22 seconds
2020-04-27 22:48:40.151720 (MainThread): On master: ROLLBACK
2020-04-27 22:48:40.192948 (MainThread): Using postgres connection "master".
2020-04-27 22:48:40.193341 (MainThread): On master: BEGIN
2020-04-27 22:48:40.300530 (MainThread): SQL status: BEGIN in 0.11 seconds
2020-04-27 22:48:40.300972 (MainThread): On master: COMMIT
2020-04-27 22:48:40.301280 (MainThread): Using postgres connection "master".
2020-04-27 22:48:40.301439 (MainThread): On master: COMMIT
2020-04-27 22:48:40.339843 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 22:48:40.340461 (MainThread): 15:48:40 | Concurrency: 1 threads (target='dev')
2020-04-27 22:48:40.340739 (MainThread): 15:48:40 | 
2020-04-27 22:48:40.344987 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 22:48:40.345209 (Thread-1): 15:48:40 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 22:48:40.345605 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.345740 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 22:48:40.345867 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 22:48:40.361635 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 22:48:40.362398 (Thread-1): finished collecting timing info
2020-04-27 22:48:40.400994 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.401155 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 22:48:40.488170 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-27 22:48:40.492417 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.492569 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 22:48:40.542261 (Thread-1): SQL status: DROP VIEW in 0.05 seconds
2020-04-27 22:48:40.545496 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 22:48:40.546335 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.546486 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 22:48:40.598028 (Thread-1): SQL status: BEGIN in 0.05 seconds
2020-04-27 22:48:40.598420 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.598666 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 22:48:40.662149 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 22:48:40.666359 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.666508 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 22:48:40.709836 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:48:40.711782 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 22:48:40.711966 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:40.712117 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 22:48:41.015200 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-27 22:48:41.018396 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:48:41.018560 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 22:48:41.321997 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-27 22:48:41.326309 (Thread-1): finished collecting timing info
2020-04-27 22:48:41.327163 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa0b3084-a491-4ca0-b7a7-0f675b0c1300', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112039690>]}
2020-04-27 22:48:41.327470 (Thread-1): 15:48:41 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.98s]
2020-04-27 22:48:41.327653 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 22:48:41.327838 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 22:48:41.328122 (Thread-1): 15:48:41 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 22:48:41.328483 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.328609 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 22:48:41.328750 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 22:48:41.334967 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 22:48:41.335391 (Thread-1): finished collecting timing info
2020-04-27 22:48:41.343935 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.344077 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 22:48:41.629067 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2020-04-27 22:48:41.633216 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.633362 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 22:48:41.833378 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-27 22:48:41.836418 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 22:48:41.837044 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.837197 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 22:48:41.880081 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:48:41.880469 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.880720 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 22:48:41.942049 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 22:48:41.946257 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.946406 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 22:48:41.989226 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:48:41.991171 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 22:48:41.991362 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:41.991518 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 22:48:42.346177 (Thread-1): SQL status: COMMIT in 0.35 seconds
2020-04-27 22:48:42.349587 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:48:42.349734 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 22:48:42.653026 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-27 22:48:42.657171 (Thread-1): finished collecting timing info
2020-04-27 22:48:42.658041 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa0b3084-a491-4ca0-b7a7-0f675b0c1300', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e5d450>]}
2020-04-27 22:48:42.658353 (Thread-1): 15:48:42 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.33s]
2020-04-27 22:48:42.658558 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 22:48:42.659092 (Thread-1): Began running node model.order_history.customers
2020-04-27 22:48:42.659425 (Thread-1): 15:48:42 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 22:48:42.659900 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:48:42.660049 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 22:48:42.660208 (Thread-1): Compiling model.order_history.customers
2020-04-27 22:48:42.668434 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 22:48:42.668825 (Thread-1): finished collecting timing info
2020-04-27 22:48:42.675194 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:48:42.675319 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 22:48:42.927263 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-27 22:48:42.931460 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:48:42.931616 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 22:48:43.124347 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-27 22:48:43.126596 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 22:48:43.127106 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:48:43.127251 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 22:48:43.170750 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:48:43.170940 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:48:43.171043 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
orders as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        count(order_unique_id) as number_of_orders
    from orders
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 22:48:43.213118 (Thread-1): Postgres error: syntax error at or near "."
LINE 23:         customer_orders.first_order_date,
                                ^

2020-04-27 22:48:43.213539 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 22:48:43.255871 (Thread-1): finished collecting timing info
2020-04-27 22:48:43.256912 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "."
  LINE 23:         customer_orders.first_order_date,
                                  ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "."
LINE 23:         customer_orders.first_order_date,
                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "."
  LINE 23:         customer_orders.first_order_date,
                                  ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 22:48:43.277933 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa0b3084-a491-4ca0-b7a7-0f675b0c1300', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e5d450>]}
2020-04-27 22:48:43.278216 (Thread-1): 15:48:43 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.62s]
2020-04-27 22:48:43.278372 (Thread-1): Finished running node model.order_history.customers
2020-04-27 22:48:43.311453 (MainThread): Using postgres connection "master".
2020-04-27 22:48:43.311708 (MainThread): On master: BEGIN
2020-04-27 22:48:43.351293 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:48:43.351749 (MainThread): On master: COMMIT
2020-04-27 22:48:43.352015 (MainThread): Using postgres connection "master".
2020-04-27 22:48:43.352214 (MainThread): On master: COMMIT
2020-04-27 22:48:43.392556 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 22:48:43.393469 (MainThread): 15:48:43 | 
2020-04-27 22:48:43.393692 (MainThread): 15:48:43 | Finished running 3 view models in 4.88s.
2020-04-27 22:48:43.393883 (MainThread): Connection 'master' was left open.
2020-04-27 22:48:43.394028 (MainThread): On master: Close
2020-04-27 22:48:43.394403 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 22:48:43.394557 (MainThread): On model.order_history.customers: Close
2020-04-27 22:48:43.405834 (MainThread): 
2020-04-27 22:48:43.406094 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 22:48:43.406273 (MainThread): 
2020-04-27 22:48:43.406414 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 22:48:43.406539 (MainThread):   syntax error at or near "."
2020-04-27 22:48:43.406653 (MainThread):   LINE 23:         customer_orders.first_order_date,
2020-04-27 22:48:43.406765 (MainThread):                                   ^
2020-04-27 22:48:43.406875 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 22:48:43.406997 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-27 22:48:43.407222 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117a6850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112080310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120801d0>]}
2020-04-27 22:48:43.407488 (MainThread): Flushing usage events
2020-04-27 22:49:05.622256 (MainThread): Running with dbt=0.16.1
2020-04-27 22:49:05.688701 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 22:49:05.689490 (MainThread): Tracking: tracking
2020-04-27 22:49:05.694368 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10984bb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109acce50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109accf10>]}
2020-04-27 22:49:05.716281 (MainThread): Partial parsing not enabled
2020-04-27 22:49:05.718236 (MainThread): Parsing macros/core.sql
2020-04-27 22:49:05.723239 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 22:49:05.731866 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 22:49:05.733724 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 22:49:05.753576 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 22:49:05.789702 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 22:49:05.813393 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 22:49:05.815883 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 22:49:05.822599 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 22:49:05.836877 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 22:49:05.844502 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 22:49:05.851042 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 22:49:05.856284 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 22:49:05.857307 (MainThread): Parsing macros/etc/query.sql
2020-04-27 22:49:05.858538 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 22:49:05.860310 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 22:49:05.862503 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 22:49:05.872171 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 22:49:05.874358 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 22:49:05.875524 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 22:49:05.922018 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 22:49:05.923289 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 22:49:05.924269 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 22:49:05.925423 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 22:49:05.927791 (MainThread): Parsing macros/catalog.sql
2020-04-27 22:49:05.930264 (MainThread): Parsing macros/relations.sql
2020-04-27 22:49:05.931702 (MainThread): Parsing macros/adapters.sql
2020-04-27 22:49:05.949152 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 22:49:05.967299 (MainThread): Partial parsing not enabled
2020-04-27 22:49:05.994871 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:49:05.994981 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:49:06.011282 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:06.011376 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:49:06.015360 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:06.015449 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:49:06.138272 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 22:49:06.139878 (MainThread): 
2020-04-27 22:49:06.140139 (MainThread): Acquiring new postgres connection "master".
2020-04-27 22:49:06.140219 (MainThread): Opening a new connection, currently in state init
2020-04-27 22:49:06.149881 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 22:49:06.149980 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 22:49:06.246082 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 22:49:06.246215 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 22:49:06.700647 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.45 seconds
2020-04-27 22:49:06.721681 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:49:06.721818 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 22:49:06.723606 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:49:06.723817 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 22:49:06.764392 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:06.764801 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 22:49:06.765057 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 22:49:06.896409 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-27 22:49:06.900208 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 22:49:06.960000 (MainThread): Using postgres connection "master".
2020-04-27 22:49:06.960154 (MainThread): On master: BEGIN
2020-04-27 22:49:07.359596 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-27 22:49:07.360013 (MainThread): Using postgres connection "master".
2020-04-27 22:49:07.360275 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 22:49:07.534328 (MainThread): SQL status: SELECT in 0.17 seconds
2020-04-27 22:49:07.615015 (MainThread): On master: ROLLBACK
2020-04-27 22:49:07.654730 (MainThread): Using postgres connection "master".
2020-04-27 22:49:07.655135 (MainThread): On master: BEGIN
2020-04-27 22:49:07.734787 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 22:49:07.735236 (MainThread): On master: COMMIT
2020-04-27 22:49:07.735551 (MainThread): Using postgres connection "master".
2020-04-27 22:49:07.735702 (MainThread): On master: COMMIT
2020-04-27 22:49:07.774968 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 22:49:07.775851 (MainThread): 15:49:07 | Concurrency: 1 threads (target='dev')
2020-04-27 22:49:07.776096 (MainThread): 15:49:07 | 
2020-04-27 22:49:07.778511 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 22:49:07.778766 (Thread-1): 15:49:07 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 22:49:07.779148 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.779284 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 22:49:07.779425 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 22:49:07.796067 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 22:49:07.796554 (Thread-1): finished collecting timing info
2020-04-27 22:49:07.837542 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.837700 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 22:49:07.916117 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 22:49:07.919171 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.919321 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 22:49:07.958108 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 22:49:07.959868 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 22:49:07.960301 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.960411 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 22:49:07.999022 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:07.999446 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:07.999711 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 22:49:08.086040 (Thread-1): SQL status: CREATE VIEW in 0.09 seconds
2020-04-27 22:49:08.092364 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:08.092517 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 22:49:08.135415 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:08.139777 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:08.139931 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 22:49:08.181222 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:08.183182 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 22:49:08.183377 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:08.183535 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 22:49:08.457938 (Thread-1): SQL status: COMMIT in 0.27 seconds
2020-04-27 22:49:08.461523 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 22:49:08.461699 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 22:49:08.844013 (Thread-1): SQL status: DROP VIEW in 0.38 seconds
2020-04-27 22:49:08.848488 (Thread-1): finished collecting timing info
2020-04-27 22:49:08.849403 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c385504d-1ac6-4a73-a806-7505d91acc34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f16fd0>]}
2020-04-27 22:49:08.849724 (Thread-1): 15:49:08 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.07s]
2020-04-27 22:49:08.849913 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 22:49:08.850148 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 22:49:08.850540 (Thread-1): 15:49:08 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 22:49:08.851034 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:08.851220 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 22:49:08.851358 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 22:49:08.858116 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 22:49:08.858590 (Thread-1): finished collecting timing info
2020-04-27 22:49:08.867722 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:08.867933 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 22:49:09.061236 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-27 22:49:09.065416 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.065569 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 22:49:09.246761 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-27 22:49:09.249851 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 22:49:09.250509 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.250669 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 22:49:09.289138 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:09.289339 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.289452 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 22:49:09.347426 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 22:49:09.353725 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.353885 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 22:49:09.395343 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:09.399465 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.399611 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 22:49:09.439263 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:09.441207 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 22:49:09.441401 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.441557 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 22:49:09.658961 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-27 22:49:09.661201 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 22:49:09.661329 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 22:49:09.993363 (Thread-1): SQL status: DROP VIEW in 0.33 seconds
2020-04-27 22:49:09.997206 (Thread-1): finished collecting timing info
2020-04-27 22:49:09.998145 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c385504d-1ac6-4a73-a806-7505d91acc34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c6d3d0>]}
2020-04-27 22:49:09.998498 (Thread-1): 15:49:09 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.15s]
2020-04-27 22:49:09.998722 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 22:49:09.999329 (Thread-1): Began running node model.order_history.customers
2020-04-27 22:49:09.999608 (Thread-1): 15:49:09 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 22:49:10.000086 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 22:49:10.000303 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 22:49:10.000475 (Thread-1): Compiling model.order_history.customers
2020-04-27 22:49:10.013611 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 22:49:10.014043 (Thread-1): finished collecting timing info
2020-04-27 22:49:10.021181 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.021363 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 22:49:10.235594 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-27 22:49:10.238327 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.238439 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 22:49:10.505824 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2020-04-27 22:49:10.508912 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 22:49:10.509526 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.509683 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 22:49:10.548686 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:10.549114 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.549386 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
orders as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        count(order_unique_id) as number_of_orders
    from orders
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 22:49:10.616895 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-27 22:49:10.622044 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.622202 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-27 22:49:10.661075 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 22:49:10.662347 (Thread-1): On model.order_history.customers: COMMIT
2020-04-27 22:49:10.662494 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.662604 (Thread-1): On model.order_history.customers: COMMIT
2020-04-27 22:49:10.915402 (Thread-1): SQL status: COMMIT in 0.25 seconds
2020-04-27 22:49:10.953040 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 22:49:10.953242 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 22:49:11.177077 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-27 22:49:11.181315 (Thread-1): finished collecting timing info
2020-04-27 22:49:11.182159 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c385504d-1ac6-4a73-a806-7505d91acc34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad7c90>]}
2020-04-27 22:49:11.182463 (Thread-1): 15:49:11 | 3 of 3 OK created view model data_science.customers.................. [CREATE VIEW in 1.18s]
2020-04-27 22:49:11.182641 (Thread-1): Finished running node model.order_history.customers
2020-04-27 22:49:11.287946 (MainThread): Using postgres connection "master".
2020-04-27 22:49:11.288267 (MainThread): On master: BEGIN
2020-04-27 22:49:11.329661 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 22:49:11.329885 (MainThread): On master: COMMIT
2020-04-27 22:49:11.330005 (MainThread): Using postgres connection "master".
2020-04-27 22:49:11.330112 (MainThread): On master: COMMIT
2020-04-27 22:49:11.367623 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 22:49:11.368079 (MainThread): 15:49:11 | 
2020-04-27 22:49:11.368233 (MainThread): 15:49:11 | Finished running 3 view models in 5.23s.
2020-04-27 22:49:11.368355 (MainThread): Connection 'master' was left open.
2020-04-27 22:49:11.368459 (MainThread): On master: Close
2020-04-27 22:49:11.368726 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 22:49:11.368829 (MainThread): On model.order_history.customers: Close
2020-04-27 22:49:11.377851 (MainThread): 
2020-04-27 22:49:11.378002 (MainThread): Completed successfully
2020-04-27 22:49:11.378130 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2020-04-27 22:49:11.378318 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c47d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0da850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b41950>]}
2020-04-27 22:49:11.378510 (MainThread): Flushing usage events
2020-04-27 23:06:17.500612 (MainThread): Running with dbt=0.16.1
2020-04-27 23:06:17.579773 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:06:17.580603 (MainThread): Tracking: tracking
2020-04-27 23:06:17.585662 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6f4590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c128210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6f4810>]}
2020-04-27 23:06:17.604288 (MainThread): Partial parsing not enabled
2020-04-27 23:06:17.606320 (MainThread): Parsing macros/core.sql
2020-04-27 23:06:17.610907 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:06:17.618910 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:06:17.620674 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:06:17.638655 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:06:17.672136 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:06:17.693450 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:06:17.695371 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:06:17.701872 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:06:17.714627 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:06:17.721560 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:06:17.727917 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:06:17.732947 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:06:17.733920 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:06:17.735417 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:06:17.737377 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:06:17.739787 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:06:17.749324 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:06:17.751306 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:06:17.752539 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:06:17.793128 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:06:17.794271 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:06:17.795181 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:06:17.796296 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:06:17.798494 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:06:17.800773 (MainThread): Parsing macros/relations.sql
2020-04-27 23:06:17.802099 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:06:17.818771 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:06:17.836514 (MainThread): Partial parsing not enabled
2020-04-27 23:06:17.864584 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:06:17.864756 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:06:17.886623 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:17.886794 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:06:17.892986 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:17.893170 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:06:18.018108 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:06:18.019911 (MainThread): 
2020-04-27 23:06:18.020238 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:06:18.020325 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:06:18.030405 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:06:18.030514 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:06:18.117380 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:06:18.117527 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:06:18.573399 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.46 seconds
2020-04-27 23:06:18.591520 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:06:18.591720 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:06:18.593362 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:06:18.593478 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:06:18.630927 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:18.631353 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:06:18.631622 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:06:18.742170 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-27 23:06:18.747399 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:06:18.810372 (MainThread): Using postgres connection "master".
2020-04-27 23:06:18.810538 (MainThread): On master: BEGIN
2020-04-27 23:06:19.166628 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-27 23:06:19.166916 (MainThread): Using postgres connection "master".
2020-04-27 23:06:19.167082 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:06:19.340464 (MainThread): SQL status: SELECT in 0.17 seconds
2020-04-27 23:06:19.409862 (MainThread): On master: ROLLBACK
2020-04-27 23:06:19.449754 (MainThread): Using postgres connection "master".
2020-04-27 23:06:19.450171 (MainThread): On master: BEGIN
2020-04-27 23:06:19.529551 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:06:19.530025 (MainThread): On master: COMMIT
2020-04-27 23:06:19.530261 (MainThread): Using postgres connection "master".
2020-04-27 23:06:19.530416 (MainThread): On master: COMMIT
2020-04-27 23:06:19.570301 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:06:19.570754 (MainThread): 16:06:19 | Concurrency: 1 threads (target='dev')
2020-04-27 23:06:19.570926 (MainThread): 16:06:19 | 
2020-04-27 23:06:19.572633 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 23:06:19.572882 (Thread-1): 16:06:19 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 23:06:19.573417 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.573544 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:06:19.573677 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 23:06:19.592263 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 23:06:19.592930 (Thread-1): finished collecting timing info
2020-04-27 23:06:19.634672 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.634832 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 23:06:19.710707 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 23:06:19.715104 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.715259 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:06:19.753820 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:06:19.757008 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 23:06:19.757622 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.757777 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 23:06:19.797363 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:19.797797 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.798073 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 23:06:19.859445 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:06:19.865163 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.865306 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 23:06:19.904249 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:06:19.907389 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.907537 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 23:06:19.945860 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:06:19.946990 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:06:19.947115 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:19.947213 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:06:20.169582 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-27 23:06:20.173094 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:06:20.173255 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:06:20.410322 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-27 23:06:20.414818 (Thread-1): finished collecting timing info
2020-04-27 23:06:20.415681 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5f9cdf2-d4ce-429a-9566-0d2c76d70060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd60e50>]}
2020-04-27 23:06:20.416003 (Thread-1): 16:06:20 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.84s]
2020-04-27 23:06:20.416194 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 23:06:20.416381 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 23:06:20.416570 (Thread-1): 16:06:20 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 23:06:20.416918 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:20.417068 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 23:06:20.417182 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 23:06:20.423439 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:06:20.423876 (Thread-1): finished collecting timing info
2020-04-27 23:06:20.432387 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:20.432521 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 23:06:20.853570 (Thread-1): SQL status: DROP VIEW in 0.42 seconds
2020-04-27 23:06:20.857754 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:20.857910 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:06:21.467620 (Thread-1): SQL status: DROP VIEW in 0.61 seconds
2020-04-27 23:06:21.469679 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:06:21.470178 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.470312 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 23:06:21.509986 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:21.510289 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.510469 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 23:06:21.570243 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:06:21.576463 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.576616 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 23:06:21.615287 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:06:21.619816 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.620001 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 23:06:21.658440 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:06:21.659956 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:06:21.660135 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:21.660277 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:06:22.082405 (Thread-1): SQL status: COMMIT in 0.42 seconds
2020-04-27 23:06:22.085871 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:06:22.086031 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:06:22.311419 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-27 23:06:22.314798 (Thread-1): finished collecting timing info
2020-04-27 23:06:22.315569 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5f9cdf2-d4ce-429a-9566-0d2c76d70060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10df73710>]}
2020-04-27 23:06:22.315835 (Thread-1): 16:06:22 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.90s]
2020-04-27 23:06:22.315990 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 23:06:22.316390 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:06:22.316606 (Thread-1): 16:06:22 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 23:06:22.317016 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:06:22.317161 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 23:06:22.317285 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:06:22.326788 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:06:22.327262 (Thread-1): finished collecting timing info
2020-04-27 23:06:22.334941 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:06:22.335117 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:06:22.561275 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-27 23:06:22.565453 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:06:22.565607 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:06:22.797998 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-27 23:06:22.800655 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:06:22.801244 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:06:22.801410 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:06:22.840453 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:22.840919 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:06:22.841153 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(order_unique_id) as number_of_orders,
        COUNT(order_ticket_unique_id) AS tickets_sold,
    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:06:22.880243 (Thread-1): Postgres error: syntax error at or near "from"
LINE 17:     from order_tickets
             ^

2020-04-27 23:06:22.880680 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 23:06:22.921101 (Thread-1): finished collecting timing info
2020-04-27 23:06:22.922188 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "from"
  LINE 17:     from order_tickets
               ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "from"
LINE 17:     from order_tickets
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "from"
  LINE 17:     from order_tickets
               ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:06:22.925323 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5f9cdf2-d4ce-429a-9566-0d2c76d70060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbd9850>]}
2020-04-27 23:06:22.925619 (Thread-1): 16:06:22 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.61s]
2020-04-27 23:06:22.925803 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:06:22.976113 (MainThread): Using postgres connection "master".
2020-04-27 23:06:22.976488 (MainThread): On master: BEGIN
2020-04-27 23:06:23.015374 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:06:23.015587 (MainThread): On master: COMMIT
2020-04-27 23:06:23.015710 (MainThread): Using postgres connection "master".
2020-04-27 23:06:23.015814 (MainThread): On master: COMMIT
2020-04-27 23:06:23.068217 (MainThread): SQL status: COMMIT in 0.05 seconds
2020-04-27 23:06:23.069144 (MainThread): 16:06:23 | 
2020-04-27 23:06:23.069387 (MainThread): 16:06:23 | Finished running 3 view models in 5.05s.
2020-04-27 23:06:23.069589 (MainThread): Connection 'master' was left open.
2020-04-27 23:06:23.069744 (MainThread): On master: Close
2020-04-27 23:06:23.070126 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:06:23.070290 (MainThread): On model.order_history.customers: Close
2020-04-27 23:06:23.080377 (MainThread): 
2020-04-27 23:06:23.080582 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 23:06:23.080714 (MainThread): 
2020-04-27 23:06:23.080836 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 23:06:23.080945 (MainThread):   syntax error at or near "from"
2020-04-27 23:06:23.081073 (MainThread):   LINE 17:     from order_tickets
2020-04-27 23:06:23.081185 (MainThread):                ^
2020-04-27 23:06:23.081285 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:06:23.081397 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-27 23:06:23.081589 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd96d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dfdf310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dfd1350>]}
2020-04-27 23:06:23.081790 (MainThread): Flushing usage events
2020-04-27 23:09:06.879499 (MainThread): Running with dbt=0.16.1
2020-04-27 23:09:06.956548 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:09:06.957371 (MainThread): Tracking: tracking
2020-04-27 23:09:06.965932 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba6f090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d011d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d002410>]}
2020-04-27 23:09:06.989267 (MainThread): Partial parsing not enabled
2020-04-27 23:09:06.991461 (MainThread): Parsing macros/core.sql
2020-04-27 23:09:06.996967 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:09:07.007154 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:09:07.009524 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:09:07.030303 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:09:07.065232 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:09:07.088228 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:09:07.090402 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:09:07.096904 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:09:07.110443 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:09:07.117519 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:09:07.124169 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:09:07.129309 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:09:07.130299 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:09:07.131412 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:09:07.133139 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:09:07.135280 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:09:07.145637 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:09:07.147881 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:09:07.149012 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:09:07.201311 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:09:07.202856 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:09:07.204042 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:09:07.205470 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:09:07.208091 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:09:07.210660 (MainThread): Parsing macros/relations.sql
2020-04-27 23:09:07.212123 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:09:07.230366 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:09:07.249557 (MainThread): Partial parsing not enabled
2020-04-27 23:09:07.278566 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:09:07.278720 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:07.295503 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:07.295629 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:07.299757 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:07.299847 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:07.438090 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:09:07.439973 (MainThread): 
2020-04-27 23:09:07.440363 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:09:07.440463 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:07.451333 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:09:07.451540 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:09:07.556582 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:09:07.556725 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:09:08.076358 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.52 seconds
2020-04-27 23:09:08.096619 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:08.096863 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:09:08.098732 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:08.098860 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:09:08.140715 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:08.141125 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:08.141276 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:09:08.250267 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-27 23:09:08.254155 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:09:08.318637 (MainThread): Using postgres connection "master".
2020-04-27 23:09:08.318819 (MainThread): On master: BEGIN
2020-04-27 23:09:08.695110 (MainThread): SQL status: BEGIN in 0.38 seconds
2020-04-27 23:09:08.695420 (MainThread): Using postgres connection "master".
2020-04-27 23:09:08.695593 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:09:09.006209 (MainThread): SQL status: SELECT in 0.31 seconds
2020-04-27 23:09:09.080901 (MainThread): On master: ROLLBACK
2020-04-27 23:09:09.121465 (MainThread): Using postgres connection "master".
2020-04-27 23:09:09.121771 (MainThread): On master: BEGIN
2020-04-27 23:09:09.204487 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:09:09.204799 (MainThread): On master: COMMIT
2020-04-27 23:09:09.204997 (MainThread): Using postgres connection "master".
2020-04-27 23:09:09.205175 (MainThread): On master: COMMIT
2020-04-27 23:09:09.245351 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:09:09.245880 (MainThread): 16:09:09 | Concurrency: 1 threads (target='dev')
2020-04-27 23:09:09.246079 (MainThread): 16:09:09 | 
2020-04-27 23:09:09.248187 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 23:09:09.248456 (Thread-1): 16:09:09 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 23:09:09.248967 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.249127 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:09:09.249262 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 23:09:09.264025 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 23:09:09.264446 (Thread-1): finished collecting timing info
2020-04-27 23:09:09.302789 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.302955 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 23:09:09.389796 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-27 23:09:09.394162 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.394313 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:09:09.437904 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:09:09.440941 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 23:09:09.441651 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.441835 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 23:09:09.484313 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:09.484621 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.484790 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 23:09:09.547634 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:09:09.554607 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.554832 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 23:09:09.599105 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:09.603492 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.603650 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 23:09:09.657937 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-27 23:09:09.659908 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:09:09.660099 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.660255 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:09:09.925847 (Thread-1): SQL status: COMMIT in 0.27 seconds
2020-04-27 23:09:09.928127 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:09.928272 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:09:10.335914 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-04-27 23:09:10.339474 (Thread-1): finished collecting timing info
2020-04-27 23:09:10.340480 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8515ba72-0963-4752-bb00-685c24949c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6b9090>]}
2020-04-27 23:09:10.340870 (Thread-1): 16:09:10 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.09s]
2020-04-27 23:09:10.341085 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 23:09:10.341400 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 23:09:10.341734 (Thread-1): 16:09:10 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 23:09:10.342166 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:10.342491 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 23:09:10.342772 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 23:09:10.350367 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:09:10.350901 (Thread-1): finished collecting timing info
2020-04-27 23:09:10.360058 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:10.360240 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 23:09:10.644119 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2020-04-27 23:09:10.648156 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:10.648314 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:09:11.052775 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-27 23:09:11.056063 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:09:11.056799 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.057013 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 23:09:11.101425 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:11.101704 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.101839 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 23:09:11.159809 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:09:11.165474 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.165633 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 23:09:11.208140 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:11.212107 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.212256 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 23:09:11.256946 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:11.258348 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:09:11.258502 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.258608 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:09:11.474971 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-27 23:09:11.478444 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:11.478591 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:09:11.973872 (Thread-1): SQL status: DROP VIEW in 0.50 seconds
2020-04-27 23:09:11.976552 (Thread-1): finished collecting timing info
2020-04-27 23:09:11.977204 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8515ba72-0963-4752-bb00-685c24949c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6a5a90>]}
2020-04-27 23:09:11.977436 (Thread-1): 16:09:11 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.64s]
2020-04-27 23:09:11.977568 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 23:09:11.977956 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:09:11.978121 (Thread-1): 16:09:11 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 23:09:11.978431 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:09:11.978532 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 23:09:11.978631 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:09:11.986748 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:09:11.987190 (Thread-1): finished collecting timing info
2020-04-27 23:09:11.994348 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:11.994517 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:09:12.281096 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-04-27 23:09:12.283947 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:12.284075 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:09:12.588860 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2020-04-27 23:09:12.591676 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:09:12.592337 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:12.592509 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:09:12.635390 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:12.635691 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:12.635895 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(order_unique_id) as number_of_orders,
        COUNT(order_ticket_unique_id) AS tickets_purchased,
        SUM(amount_gross) AS total_revenue
    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.tickets_sold, 0) as ticket_purchased,
        cusomter_orders.total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:09:12.683813 (Thread-1): Postgres error: column customer_orders.tickets_sold does not exist

2020-04-27 23:09:12.684230 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 23:09:12.726773 (Thread-1): finished collecting timing info
2020-04-27 23:09:12.727482 (Thread-1): Database Error in model customers (models/customers.sql)
  column customer_orders.tickets_sold does not exist
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column customer_orders.tickets_sold does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  column customer_orders.tickets_sold does not exist
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:09:12.730273 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8515ba72-0963-4752-bb00-685c24949c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8d2710>]}
2020-04-27 23:09:12.730524 (Thread-1): 16:09:12 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.75s]
2020-04-27 23:09:12.730677 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:09:12.832358 (MainThread): Using postgres connection "master".
2020-04-27 23:09:12.832649 (MainThread): On master: BEGIN
2020-04-27 23:09:12.873039 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:12.873493 (MainThread): On master: COMMIT
2020-04-27 23:09:12.873688 (MainThread): Using postgres connection "master".
2020-04-27 23:09:12.873836 (MainThread): On master: COMMIT
2020-04-27 23:09:12.913275 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:09:12.914172 (MainThread): 16:09:12 | 
2020-04-27 23:09:12.914449 (MainThread): 16:09:12 | Finished running 3 view models in 5.47s.
2020-04-27 23:09:12.914740 (MainThread): Connection 'master' was left open.
2020-04-27 23:09:12.914909 (MainThread): On master: Close
2020-04-27 23:09:12.915311 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:09:12.915472 (MainThread): On model.order_history.customers: Close
2020-04-27 23:09:12.926303 (MainThread): 
2020-04-27 23:09:12.926532 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 23:09:12.926679 (MainThread): 
2020-04-27 23:09:12.926810 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 23:09:12.926928 (MainThread):   column customer_orders.tickets_sold does not exist
2020-04-27 23:09:12.927037 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:09:12.927155 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-27 23:09:12.927354 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d421710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8e69d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d69e750>]}
2020-04-27 23:09:12.927565 (MainThread): Flushing usage events
2020-04-27 23:09:42.207134 (MainThread): Running with dbt=0.16.1
2020-04-27 23:09:42.282142 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:09:42.283263 (MainThread): Tracking: tracking
2020-04-27 23:09:42.289321 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104614f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104863fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104887a10>]}
2020-04-27 23:09:42.311706 (MainThread): Partial parsing not enabled
2020-04-27 23:09:42.313872 (MainThread): Parsing macros/core.sql
2020-04-27 23:09:42.319702 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:09:42.329499 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:09:42.331664 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:09:42.351392 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:09:42.387795 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:09:42.409536 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:09:42.411504 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:09:42.418019 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:09:42.431165 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:09:42.438567 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:09:42.445150 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:09:42.450481 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:09:42.451885 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:09:42.453298 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:09:42.455076 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:09:42.457207 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:09:42.466507 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:09:42.468983 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:09:42.470129 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:09:42.520410 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:09:42.521709 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:09:42.522691 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:09:42.523848 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:09:42.526235 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:09:42.528654 (MainThread): Parsing macros/relations.sql
2020-04-27 23:09:42.530054 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:09:42.547551 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:09:42.567186 (MainThread): Partial parsing not enabled
2020-04-27 23:09:42.597261 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:09:42.597395 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:42.613925 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:42.614056 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:42.618719 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:42.618918 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:42.752295 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:09:42.754037 (MainThread): 
2020-04-27 23:09:42.754337 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:09:42.754428 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:09:42.765860 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:09:42.765998 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:09:42.872139 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:09:42.872287 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:09:43.268706 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.40 seconds
2020-04-27 23:09:43.289385 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:43.289628 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:09:43.291473 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:43.291598 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:09:43.329759 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:43.330185 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:09:43.330448 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:09:43.531532 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.20 seconds
2020-04-27 23:09:43.534936 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:09:43.618549 (MainThread): Using postgres connection "master".
2020-04-27 23:09:43.618735 (MainThread): On master: BEGIN
2020-04-27 23:09:43.979019 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-27 23:09:43.979486 (MainThread): Using postgres connection "master".
2020-04-27 23:09:43.979670 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:09:44.160670 (MainThread): SQL status: SELECT in 0.18 seconds
2020-04-27 23:09:44.238564 (MainThread): On master: ROLLBACK
2020-04-27 23:09:44.297287 (MainThread): Using postgres connection "master".
2020-04-27 23:09:44.297697 (MainThread): On master: BEGIN
2020-04-27 23:09:44.381968 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:09:44.382412 (MainThread): On master: COMMIT
2020-04-27 23:09:44.382701 (MainThread): Using postgres connection "master".
2020-04-27 23:09:44.382880 (MainThread): On master: COMMIT
2020-04-27 23:09:44.428274 (MainThread): SQL status: COMMIT in 0.05 seconds
2020-04-27 23:09:44.429168 (MainThread): 16:09:44 | Concurrency: 1 threads (target='dev')
2020-04-27 23:09:44.429415 (MainThread): 16:09:44 | 
2020-04-27 23:09:44.431578 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 23:09:44.431835 (Thread-1): 16:09:44 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 23:09:44.432297 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:44.432464 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:09:44.432627 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 23:09:44.449117 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 23:09:44.449656 (Thread-1): finished collecting timing info
2020-04-27 23:09:44.494536 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:44.494711 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 23:09:44.571646 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 23:09:44.574497 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:44.574624 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:09:44.614155 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:09:44.617551 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 23:09:44.618254 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:44.618422 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 23:09:45.204362 (Thread-1): SQL status: BEGIN in 0.59 seconds
2020-04-27 23:09:45.204573 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.204691 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 23:09:45.261651 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:09:45.268285 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.268554 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 23:09:45.311713 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:45.316149 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.316308 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 23:09:45.356312 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:45.358041 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:09:45.358248 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.358404 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:09:45.573680 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-27 23:09:45.576525 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:09:45.576700 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:09:46.073401 (Thread-1): SQL status: DROP VIEW in 0.50 seconds
2020-04-27 23:09:46.077091 (Thread-1): finished collecting timing info
2020-04-27 23:09:46.077844 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88e9649f-3c60-464d-b114-46296ebe098f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ced710>]}
2020-04-27 23:09:46.078116 (Thread-1): 16:09:46 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.65s]
2020-04-27 23:09:46.078278 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 23:09:46.078458 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 23:09:46.078814 (Thread-1): 16:09:46 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 23:09:46.079365 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:46.079559 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 23:09:46.079694 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 23:09:46.086148 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:09:46.086640 (Thread-1): finished collecting timing info
2020-04-27 23:09:46.095097 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:46.095230 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 23:09:46.380672 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-04-27 23:09:46.384931 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:46.385106 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:09:46.995344 (Thread-1): SQL status: DROP VIEW in 0.61 seconds
2020-04-27 23:09:46.998399 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:09:46.999076 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:46.999230 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 23:09:47.038929 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:47.039253 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.039430 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 23:09:47.102511 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-27 23:09:47.107207 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.107366 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 23:09:47.149047 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:09:47.152266 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.152435 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 23:09:47.197663 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-27 23:09:47.199623 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:09:47.199821 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.199980 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:09:47.507775 (Thread-1): SQL status: COMMIT in 0.31 seconds
2020-04-27 23:09:47.510539 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:09:47.510697 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:09:47.916513 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-04-27 23:09:47.920652 (Thread-1): finished collecting timing info
2020-04-27 23:09:47.921541 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88e9649f-3c60-464d-b114-46296ebe098f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ea8a50>]}
2020-04-27 23:09:47.921847 (Thread-1): 16:09:47 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.84s]
2020-04-27 23:09:47.922025 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 23:09:47.922437 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:09:47.922791 (Thread-1): 16:09:47 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 23:09:47.923169 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:09:47.923342 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 23:09:47.923485 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:09:47.932760 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:09:47.933220 (Thread-1): finished collecting timing info
2020-04-27 23:09:47.941667 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:47.941838 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:09:48.214764 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2020-04-27 23:09:48.219080 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:48.219273 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:09:48.775277 (Thread-1): SQL status: DROP VIEW in 0.56 seconds
2020-04-27 23:09:48.778352 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:09:48.778934 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:48.779088 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:09:48.817283 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:48.817599 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:09:48.817770 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(order_unique_id) as number_of_orders,
        COUNT(order_ticket_unique_id) AS tickets_purchased,
        SUM(amount_gross) AS total_revenue
    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.tickets_purchased, 0) as ticket_purchased,
        cusomter_orders.total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:09:48.864480 (Thread-1): Postgres error: relation "cusomter_orders" does not exist

2020-04-27 23:09:48.864964 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 23:09:48.903461 (Thread-1): finished collecting timing info
2020-04-27 23:09:48.904212 (Thread-1): Database Error in model customers (models/customers.sql)
  relation "cusomter_orders" does not exist
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "cusomter_orders" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  relation "cusomter_orders" does not exist
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:09:48.907006 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88e9649f-3c60-464d-b114-46296ebe098f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a24050>]}
2020-04-27 23:09:48.907303 (Thread-1): 16:09:48 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.98s]
2020-04-27 23:09:48.907484 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:09:49.008978 (MainThread): Using postgres connection "master".
2020-04-27 23:09:49.009202 (MainThread): On master: BEGIN
2020-04-27 23:09:49.047682 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:09:49.048145 (MainThread): On master: COMMIT
2020-04-27 23:09:49.048438 (MainThread): Using postgres connection "master".
2020-04-27 23:09:49.048594 (MainThread): On master: COMMIT
2020-04-27 23:09:49.086416 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:09:49.087084 (MainThread): 16:09:49 | 
2020-04-27 23:09:49.087332 (MainThread): 16:09:49 | Finished running 3 view models in 6.33s.
2020-04-27 23:09:49.087597 (MainThread): Connection 'master' was left open.
2020-04-27 23:09:49.087822 (MainThread): On master: Close
2020-04-27 23:09:49.088222 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:09:49.088370 (MainThread): On model.order_history.customers: Close
2020-04-27 23:09:49.098153 (MainThread): 
2020-04-27 23:09:49.098330 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 23:09:49.098472 (MainThread): 
2020-04-27 23:09:49.098608 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 23:09:49.098730 (MainThread):   relation "cusomter_orders" does not exist
2020-04-27 23:09:49.098844 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:09:49.098966 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-27 23:09:49.099166 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f00d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104efce90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104efcd90>]}
2020-04-27 23:09:49.099384 (MainThread): Flushing usage events
2020-04-27 23:10:04.454654 (MainThread): Running with dbt=0.16.1
2020-04-27 23:10:04.534255 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:10:04.535408 (MainThread): Tracking: tracking
2020-04-27 23:10:04.541660 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110331a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100d7a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110331510>]}
2020-04-27 23:10:04.570989 (MainThread): Partial parsing not enabled
2020-04-27 23:10:04.573229 (MainThread): Parsing macros/core.sql
2020-04-27 23:10:04.578794 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:10:04.588503 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:10:04.590508 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:10:04.609603 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:10:04.644667 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:10:04.666832 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:10:04.669166 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:10:04.675908 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:10:04.689562 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:10:04.696631 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:10:04.703607 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:10:04.708861 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:10:04.709862 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:10:04.710963 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:10:04.712673 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:10:04.714796 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:10:04.724264 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:10:04.726339 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:10:04.727438 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:10:04.770606 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:10:04.771861 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:10:04.772829 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:10:04.773959 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:10:04.776279 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:10:04.778693 (MainThread): Parsing macros/relations.sql
2020-04-27 23:10:04.780141 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:10:04.798161 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:10:04.819649 (MainThread): Partial parsing not enabled
2020-04-27 23:10:04.855058 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:10:04.855200 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:10:04.872792 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:04.872932 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:10:04.877308 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:04.877407 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:10:05.008215 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:10:05.009687 (MainThread): 
2020-04-27 23:10:05.009970 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:10:05.010056 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:10:05.020538 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:10:05.020676 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:10:05.123984 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:10:05.124155 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:10:05.563543 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.44 seconds
2020-04-27 23:10:05.584186 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:10:05.584431 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:10:05.586289 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:10:05.586416 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:10:05.941047 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.35 seconds
2020-04-27 23:10:05.941339 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:10:05.941512 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:10:06.116554 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.17 seconds
2020-04-27 23:10:06.120345 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:10:06.185541 (MainThread): Using postgres connection "master".
2020-04-27 23:10:06.185799 (MainThread): On master: BEGIN
2020-04-27 23:10:06.555341 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-27 23:10:06.555527 (MainThread): Using postgres connection "master".
2020-04-27 23:10:06.555631 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:10:06.770820 (MainThread): SQL status: SELECT in 0.22 seconds
2020-04-27 23:10:06.844559 (MainThread): On master: ROLLBACK
2020-04-27 23:10:06.885358 (MainThread): Using postgres connection "master".
2020-04-27 23:10:06.885523 (MainThread): On master: BEGIN
2020-04-27 23:10:06.967367 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:10:06.967676 (MainThread): On master: COMMIT
2020-04-27 23:10:06.967864 (MainThread): Using postgres connection "master".
2020-04-27 23:10:06.968022 (MainThread): On master: COMMIT
2020-04-27 23:10:07.010363 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:10:07.011251 (MainThread): 16:10:07 | Concurrency: 1 threads (target='dev')
2020-04-27 23:10:07.011490 (MainThread): 16:10:07 | 
2020-04-27 23:10:07.013745 (Thread-1): Began running node model.order_history.stg_customers
2020-04-27 23:10:07.014017 (Thread-1): 16:10:07 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-27 23:10:07.014407 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.014548 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:10:07.014694 (Thread-1): Compiling model.order_history.stg_customers
2020-04-27 23:10:07.031554 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-27 23:10:07.032092 (Thread-1): finished collecting timing info
2020-04-27 23:10:07.074137 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.074319 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-27 23:10:07.156614 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 23:10:07.160791 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.160950 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:10:07.202036 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:10:07.205166 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-27 23:10:07.205874 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.206070 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-27 23:10:07.246734 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:10:07.247172 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.247442 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-27 23:10:07.312937 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-27 23:10:07.319239 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.319487 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-27 23:10:07.363314 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:07.367591 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.367766 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-27 23:10:07.408954 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:07.410373 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:10:07.410516 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.410625 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-27 23:10:07.612393 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-27 23:10:07.616011 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-27 23:10:07.616186 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-27 23:10:07.861344 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-27 23:10:07.864546 (Thread-1): finished collecting timing info
2020-04-27 23:10:07.865319 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '994319a0-36a7-47f6-8bd3-c9a30f1584be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107ab290>]}
2020-04-27 23:10:07.865607 (Thread-1): 16:10:07 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.85s]
2020-04-27 23:10:07.865784 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-27 23:10:07.865953 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-27 23:10:07.866215 (Thread-1): 16:10:07 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-27 23:10:07.866576 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:07.866699 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-27 23:10:07.866817 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-27 23:10:07.873452 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:10:07.873951 (Thread-1): finished collecting timing info
2020-04-27 23:10:07.882598 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:07.882782 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-27 23:10:08.093566 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-27 23:10:08.097351 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.097513 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:10:08.499106 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-27 23:10:08.502284 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-27 23:10:08.503051 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.503325 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-27 23:10:08.544324 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:10:08.544530 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.544645 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    price_code_unique_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
  );

2020-04-27 23:10:08.616789 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-27 23:10:08.622592 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.622735 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-27 23:10:08.664383 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:08.668979 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.669224 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-27 23:10:08.713596 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:08.715553 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:10:08.715751 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:08.715910 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-27 23:10:09.013014 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-27 23:10:09.016559 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:10:09.016718 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-27 23:10:09.273850 (Thread-1): SQL status: DROP VIEW in 0.26 seconds
2020-04-27 23:10:09.276469 (Thread-1): finished collecting timing info
2020-04-27 23:10:09.277117 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '994319a0-36a7-47f6-8bd3-c9a30f1584be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107ad4d0>]}
2020-04-27 23:10:09.277348 (Thread-1): 16:10:09 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.41s]
2020-04-27 23:10:09.277483 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-27 23:10:09.277864 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:10:09.278033 (Thread-1): 16:10:09 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-27 23:10:09.278335 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:10:09.278445 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-27 23:10:09.278543 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:10:09.286805 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:10:09.287348 (Thread-1): finished collecting timing info
2020-04-27 23:10:09.294298 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.294418 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:10:09.493289 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-27 23:10:09.497087 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.497239 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:10:09.718770 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-27 23:10:09.721609 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:10:09.722240 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.722412 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:10:09.765922 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:10:09.766346 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.766511 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(order_unique_id) as number_of_orders,
        COUNT(order_ticket_unique_id) AS tickets_purchased,
        SUM(amount_gross) AS total_revenue
    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.tickets_purchased, 0) as ticket_purchased,
        customer_orders.total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:10:09.866668 (Thread-1): SQL status: CREATE VIEW in 0.10 seconds
2020-04-27 23:10:09.871544 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.871710 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-27 23:10:09.914597 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-27 23:10:09.916551 (Thread-1): On model.order_history.customers: COMMIT
2020-04-27 23:10:09.916790 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:09.916969 (Thread-1): On model.order_history.customers: COMMIT
2020-04-27 23:10:10.119759 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-27 23:10:10.156449 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:10:10.156664 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:10:10.444576 (Thread-1): SQL status: DROP VIEW in 0.29 seconds
2020-04-27 23:10:10.448117 (Thread-1): finished collecting timing info
2020-04-27 23:10:10.448944 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '994319a0-36a7-47f6-8bd3-c9a30f1584be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110321fd0>]}
2020-04-27 23:10:10.449204 (Thread-1): 16:10:10 | 3 of 3 OK created view model data_science.customers.................. [CREATE VIEW in 1.17s]
2020-04-27 23:10:10.449353 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:10:10.490968 (MainThread): Using postgres connection "master".
2020-04-27 23:10:10.491287 (MainThread): On master: BEGIN
2020-04-27 23:10:10.532872 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:10:10.533167 (MainThread): On master: COMMIT
2020-04-27 23:10:10.533344 (MainThread): Using postgres connection "master".
2020-04-27 23:10:10.533511 (MainThread): On master: COMMIT
2020-04-27 23:10:10.574035 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:10:10.574682 (MainThread): 16:10:10 | 
2020-04-27 23:10:10.574907 (MainThread): 16:10:10 | Finished running 3 view models in 5.56s.
2020-04-27 23:10:10.575091 (MainThread): Connection 'master' was left open.
2020-04-27 23:10:10.575237 (MainThread): On master: Close
2020-04-27 23:10:10.575613 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:10:10.575896 (MainThread): On model.order_history.customers: Close
2020-04-27 23:10:10.586826 (MainThread): 
2020-04-27 23:10:10.587160 (MainThread): Completed successfully
2020-04-27 23:10:10.587391 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2020-04-27 23:10:10.587667 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110980bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110979490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107a23d0>]}
2020-04-27 23:10:10.587943 (MainThread): Flushing usage events
2020-04-27 23:50:16.645439 (MainThread): Running with dbt=0.16.1
2020-04-27 23:50:16.761355 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-27 23:50:16.762589 (MainThread): Tracking: tracking
2020-04-27 23:50:16.770382 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9db790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc3d450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9e4990>]}
2020-04-27 23:50:16.796341 (MainThread): Partial parsing not enabled
2020-04-27 23:50:16.799832 (MainThread): Parsing macros/core.sql
2020-04-27 23:50:16.806840 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-27 23:50:16.816113 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-27 23:50:16.819195 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-27 23:50:16.839946 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-27 23:50:16.874872 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-27 23:50:16.897658 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-27 23:50:16.900358 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-27 23:50:16.907750 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-27 23:50:16.921624 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-27 23:50:16.929607 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-27 23:50:16.937097 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-27 23:50:16.943151 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-27 23:50:16.945182 (MainThread): Parsing macros/etc/query.sql
2020-04-27 23:50:16.947211 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-27 23:50:16.950118 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-27 23:50:16.953175 (MainThread): Parsing macros/etc/datetime.sql
2020-04-27 23:50:16.967394 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-27 23:50:16.970648 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-27 23:50:16.972854 (MainThread): Parsing macros/adapters/common.sql
2020-04-27 23:50:17.017670 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-27 23:50:17.019975 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-27 23:50:17.021635 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-27 23:50:17.023475 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-27 23:50:17.026597 (MainThread): Parsing macros/catalog.sql
2020-04-27 23:50:17.029827 (MainThread): Parsing macros/relations.sql
2020-04-27 23:50:17.032054 (MainThread): Parsing macros/adapters.sql
2020-04-27 23:50:17.050487 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-27 23:50:17.068912 (MainThread): Partial parsing not enabled
2020-04-27 23:50:17.095879 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:50:17.095982 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:50:17.112692 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-27 23:50:17.112796 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:50:17.116830 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-27 23:50:17.116921 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:50:17.245787 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-27 23:50:17.247756 (MainThread): 
2020-04-27 23:50:17.248058 (MainThread): Acquiring new postgres connection "master".
2020-04-27 23:50:17.248148 (MainThread): Opening a new connection, currently in state init
2020-04-27 23:50:17.254038 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-27 23:50:17.254404 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-27 23:50:17.350389 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-27 23:50:17.350529 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-27 23:50:17.882978 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-27 23:50:17.908102 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:50:17.908336 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-27 23:50:17.910136 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:50:17.910260 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-27 23:50:17.952016 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:50:17.952210 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-27 23:50:17.952315 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-27 23:50:18.062644 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-27 23:50:18.067610 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-27 23:50:18.127837 (MainThread): Using postgres connection "master".
2020-04-27 23:50:18.128004 (MainThread): On master: BEGIN
2020-04-27 23:50:18.508744 (MainThread): SQL status: BEGIN in 0.38 seconds
2020-04-27 23:50:18.509174 (MainThread): Using postgres connection "master".
2020-04-27 23:50:18.509451 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-27 23:50:18.677203 (MainThread): SQL status: SELECT in 0.17 seconds
2020-04-27 23:50:18.745960 (MainThread): On master: ROLLBACK
2020-04-27 23:50:18.785998 (MainThread): Using postgres connection "master".
2020-04-27 23:50:18.786415 (MainThread): On master: BEGIN
2020-04-27 23:50:18.867943 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-27 23:50:18.868406 (MainThread): On master: COMMIT
2020-04-27 23:50:18.868713 (MainThread): Using postgres connection "master".
2020-04-27 23:50:18.868873 (MainThread): On master: COMMIT
2020-04-27 23:50:18.908567 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:50:18.909464 (MainThread): 16:50:18 | Concurrency: 1 threads (target='dev')
2020-04-27 23:50:18.909718 (MainThread): 16:50:18 | 
2020-04-27 23:50:18.914476 (Thread-1): Began running node model.order_history.customers
2020-04-27 23:50:18.914731 (Thread-1): 16:50:18 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-27 23:50:18.915097 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-27 23:50:18.915220 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-27 23:50:18.915347 (Thread-1): Compiling model.order_history.customers
2020-04-27 23:50:18.934859 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-27 23:50:18.935581 (Thread-1): finished collecting timing info
2020-04-27 23:50:18.982429 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:50:18.982595 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-27 23:50:19.066528 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-27 23:50:19.070881 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:50:19.071036 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-27 23:50:19.111142 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-27 23:50:19.114240 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-27 23:50:19.114880 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:50:19.115035 (Thread-1): On model.order_history.customers: BEGIN
2020-04-27 23:50:19.154506 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:50:19.154943 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-27 23:50:19.155218 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),
customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id  ELSE NULL END) AS number_of_orders,

        COALESCE(COALESCE(CAST( ( SUM(DISTINCT (CAST(FLOOR(COALESCE(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN amount_gross ELSE NULL END,0)*(1000000*1.0)) AS DECIMAL(38,0))) 
        + CAST(STRTOL(LEFT(MD5(CAST(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN order_ticket_unique_id  ELSE NULL END AS VARCHAR)),15),16) AS DECIMAL(38,0))* 1.0e8 
        + CAST(STRTOL(RIGHT(MD5(CAST(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN  order_ticket_unique_id  ELSE NULL END AS VARCHAR)),15),16) AS DECIMAL(38,0)) ) 
        - SUM(DISTINCT CAST(STRTOL(LEFT(MD5(CAST(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN order_ticket_unique_id  ELSE NULL END AS VARCHAR)),15),16) AS DECIMAL(38,0))* 1.0e8 
        + CAST(STRTOL(RIGHT(MD5(CAST(CASE WHEN NOT COALESCE(is_canceled , FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),15),16) AS DECIMAL(38,0))) ) AS DOUBLE PRECISION) / 
        CAST((1000000*1.0) AS DOUBLE PRECISION), 0), 0) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-27 23:50:19.202997 (Thread-1): Postgres error: syntax error at or near "VARCHAR"
LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
                                                              ^

2020-04-27 23:50:19.203309 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-27 23:50:19.245724 (Thread-1): finished collecting timing info
2020-04-27 23:50:19.246668 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "VARCHAR"
  LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
                                                                ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "VARCHAR"
LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
                                                              ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "VARCHAR"
  LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
                                                                ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:50:19.268858 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9928aeb-e3b1-4d17-9c7d-65c785bbf9d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e281a50>]}
2020-04-27 23:50:19.269160 (Thread-1): 16:50:19 | 1 of 1 ERROR creating view model data_science.customers.............. [ERROR in 0.35s]
2020-04-27 23:50:19.269325 (Thread-1): Finished running node model.order_history.customers
2020-04-27 23:50:19.322116 (MainThread): Using postgres connection "master".
2020-04-27 23:50:19.322285 (MainThread): On master: BEGIN
2020-04-27 23:50:19.361981 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-27 23:50:19.362446 (MainThread): On master: COMMIT
2020-04-27 23:50:19.362743 (MainThread): Using postgres connection "master".
2020-04-27 23:50:19.362899 (MainThread): On master: COMMIT
2020-04-27 23:50:19.402884 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-27 23:50:19.403787 (MainThread): 16:50:19 | 
2020-04-27 23:50:19.404027 (MainThread): 16:50:19 | Finished running 1 view model in 2.16s.
2020-04-27 23:50:19.404231 (MainThread): Connection 'master' was left open.
2020-04-27 23:50:19.404389 (MainThread): On master: Close
2020-04-27 23:50:19.404791 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-27 23:50:19.404955 (MainThread): On model.order_history.customers: Close
2020-04-27 23:50:19.409503 (MainThread): 
2020-04-27 23:50:19.409729 (MainThread): Completed with 1 error and 0 warnings:
2020-04-27 23:50:19.409876 (MainThread): 
2020-04-27 23:50:19.410014 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-27 23:50:19.410139 (MainThread):   syntax error at or near "VARCHAR"
2020-04-27 23:50:19.410255 (MainThread):   LINE 25: ...FALSE) THEN order_ticket_unique_id  ELSE NULL END VARCHAR)),...
2020-04-27 23:50:19.410371 (MainThread):                                                                 ^
2020-04-27 23:50:19.410485 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-27 23:50:19.410605 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2020-04-27 23:50:19.410801 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e055990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2b3ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e07abd0>]}
2020-04-27 23:50:19.411072 (MainThread): Flushing usage events
2020-04-28 00:23:21.500161 (MainThread): Running with dbt=0.16.1
2020-04-28 00:23:21.592426 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:23:21.593586 (MainThread): Tracking: tracking
2020-04-28 00:23:21.599826 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114efad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111774e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111774ed0>]}
2020-04-28 00:23:21.622401 (MainThread): Partial parsing not enabled
2020-04-28 00:23:21.624667 (MainThread): Parsing macros/core.sql
2020-04-28 00:23:21.630223 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:23:21.639092 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:23:21.641272 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:23:21.659622 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:23:21.693847 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:23:21.715718 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:23:21.718407 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:23:21.725038 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:23:21.738745 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:23:21.746952 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:23:21.758101 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:23:21.764151 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:23:21.765481 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:23:21.767467 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:23:21.769707 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:23:21.772342 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:23:21.782586 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:23:21.784906 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:23:21.786373 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:23:21.828975 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:23:21.831099 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:23:21.832585 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:23:21.834301 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:23:21.836796 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:23:21.839784 (MainThread): Parsing macros/relations.sql
2020-04-28 00:23:21.841586 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:23:21.861297 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:23:21.886238 (MainThread): Partial parsing not enabled
2020-04-28 00:23:21.915161 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:23:21.915302 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:23:21.932628 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:21.932780 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:23:21.937833 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:21.937937 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:23:22.067670 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:23:22.070095 (MainThread): 
2020-04-28 00:23:22.070520 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:23:22.070608 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:23:22.081506 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:23:22.081708 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:23:22.166743 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:23:22.166876 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:23:22.761456 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.59 seconds
2020-04-28 00:23:22.783675 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:23:22.783899 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:23:22.785747 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:23:22.785876 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:23:22.824651 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:22.825082 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:23:22.825355 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:23:22.989705 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.16 seconds
2020-04-28 00:23:22.994735 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:23:23.053766 (MainThread): Using postgres connection "master".
2020-04-28 00:23:23.053933 (MainThread): On master: BEGIN
2020-04-28 00:23:23.440599 (MainThread): SQL status: BEGIN in 0.39 seconds
2020-04-28 00:23:23.441027 (MainThread): Using postgres connection "master".
2020-04-28 00:23:23.441306 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:23:23.837975 (MainThread): SQL status: SELECT in 0.40 seconds
2020-04-28 00:23:23.919064 (MainThread): On master: ROLLBACK
2020-04-28 00:23:23.960495 (MainThread): Using postgres connection "master".
2020-04-28 00:23:23.960904 (MainThread): On master: BEGIN
2020-04-28 00:23:24.041246 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:23:24.041708 (MainThread): On master: COMMIT
2020-04-28 00:23:24.042012 (MainThread): Using postgres connection "master".
2020-04-28 00:23:24.042167 (MainThread): On master: COMMIT
2020-04-28 00:23:24.081652 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:23:24.082459 (MainThread): 17:23:24 | Concurrency: 1 threads (target='dev')
2020-04-28 00:23:24.082706 (MainThread): 17:23:24 | 
2020-04-28 00:23:24.085528 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 00:23:24.085793 (Thread-1): 17:23:24 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-28 00:23:24.086354 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.086494 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:23:24.086637 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 00:23:24.102633 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 00:23:24.103106 (Thread-1): finished collecting timing info
2020-04-28 00:23:24.143828 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.143992 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 00:23:24.222940 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:23:24.226085 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.226221 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:23:24.265779 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:23:24.267740 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 00:23:24.268724 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.268849 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 00:23:24.308065 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:24.308515 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.308827 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 00:23:24.380136 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:23:24.386397 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.386556 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 00:23:24.438716 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 00:23:24.442850 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.443007 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 00:23:24.482493 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:23:24.484527 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:23:24.484730 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:24.484890 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:23:25.068114 (Thread-1): SQL status: COMMIT in 0.58 seconds
2020-04-28 00:23:25.071638 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:23:25.071803 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:23:25.303794 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 00:23:25.306633 (Thread-1): finished collecting timing info
2020-04-28 00:23:25.307311 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03f8f154-c088-4874-91c2-b123601dd746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11208e1d0>]}
2020-04-28 00:23:25.307558 (Thread-1): 17:23:25 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.22s]
2020-04-28 00:23:25.307704 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 00:23:25.307947 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-28 00:23:25.308197 (Thread-1): 17:23:25 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-28 00:23:25.308477 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:25.308575 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 00:23:25.308671 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-28 00:23:25.313863 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:23:25.314302 (Thread-1): finished collecting timing info
2020-04-28 00:23:25.322215 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:25.322358 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-28 00:23:25.577077 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-28 00:23:25.581178 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:25.581343 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:23:25.987130 (Thread-1): SQL status: DROP VIEW in 0.41 seconds
2020-04-28 00:23:25.988857 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:23:25.989286 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:25.989400 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-28 00:23:26.028291 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:26.028724 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.028993 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
  );

2020-04-28 00:23:26.091875 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:23:26.097915 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.098071 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-28 00:23:26.141864 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:23:26.146223 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.146381 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-28 00:23:26.186476 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:23:26.188449 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:23:26.188655 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.188818 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:23:26.384059 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 00:23:26.387641 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:23:26.387808 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:23:26.908240 (Thread-1): SQL status: DROP VIEW in 0.52 seconds
2020-04-28 00:23:26.912499 (Thread-1): finished collecting timing info
2020-04-28 00:23:26.913343 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03f8f154-c088-4874-91c2-b123601dd746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11192dfd0>]}
2020-04-28 00:23:26.913651 (Thread-1): 17:23:26 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.60s]
2020-04-28 00:23:26.913830 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-28 00:23:26.914363 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:23:26.914659 (Thread-1): 17:23:26 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-28 00:23:26.915093 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:23:26.915259 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-28 00:23:26.915387 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:23:26.924988 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:23:26.925471 (Thread-1): finished collecting timing info
2020-04-28 00:23:26.933096 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:26.933252 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:23:27.122121 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 00:23:27.126326 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.126490 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:23:27.522649 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-28 00:23:27.525714 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:23:27.526336 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.526494 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:23:27.565958 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:27.566400 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.566677 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS number_of_orders,

        SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:23:27.633011 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:23:27.636880 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.637043 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:23:27.676721 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:23:27.677762 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:23:27.677876 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.677969 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:23:27.866039 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-28 00:23:27.903014 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:23:27.903232 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:23:28.115203 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 00:23:28.119506 (Thread-1): finished collecting timing info
2020-04-28 00:23:28.120351 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03f8f154-c088-4874-91c2-b123601dd746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111740410>]}
2020-04-28 00:23:28.120658 (Thread-1): 17:23:28 | 3 of 3 OK created view model data_science.customers.................. [CREATE VIEW in 1.21s]
2020-04-28 00:23:28.120838 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:23:28.169358 (MainThread): Using postgres connection "master".
2020-04-28 00:23:28.169690 (MainThread): On master: BEGIN
2020-04-28 00:23:28.211491 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:23:28.211957 (MainThread): On master: COMMIT
2020-04-28 00:23:28.212262 (MainThread): Using postgres connection "master".
2020-04-28 00:23:28.212419 (MainThread): On master: COMMIT
2020-04-28 00:23:28.251522 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:23:28.252253 (MainThread): 17:23:28 | 
2020-04-28 00:23:28.252491 (MainThread): 17:23:28 | Finished running 3 view models in 6.18s.
2020-04-28 00:23:28.252778 (MainThread): Connection 'master' was left open.
2020-04-28 00:23:28.252985 (MainThread): On master: Close
2020-04-28 00:23:28.254800 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:23:28.255088 (MainThread): On model.order_history.customers: Close
2020-04-28 00:23:28.265606 (MainThread): 
2020-04-28 00:23:28.265804 (MainThread): Completed successfully
2020-04-28 00:23:28.265951 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2020-04-28 00:23:28.266159 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111740610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111da3a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b76890>]}
2020-04-28 00:23:28.266377 (MainThread): Flushing usage events
2020-04-28 00:25:33.579994 (MainThread): Running with dbt=0.16.1
2020-04-28 00:25:33.667882 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:25:33.669030 (MainThread): Tracking: tracking
2020-04-28 00:25:33.676008 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102d8350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11054bdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d386ad0>]}
2020-04-28 00:25:33.700270 (MainThread): Partial parsing not enabled
2020-04-28 00:25:33.702820 (MainThread): Parsing macros/core.sql
2020-04-28 00:25:33.708552 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:25:33.717346 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:25:33.719831 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:25:33.739915 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:25:33.780122 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:25:33.802183 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:25:33.804662 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:25:33.814320 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:25:33.829588 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:25:33.837377 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:25:33.844016 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:25:33.849461 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:25:33.850772 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:25:33.852256 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:25:33.854488 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:25:33.857168 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:25:33.870641 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:25:33.873496 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:25:33.875369 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:25:33.919565 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:25:33.921077 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:25:33.922290 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:25:33.923598 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:25:33.926164 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:25:33.929072 (MainThread): Parsing macros/relations.sql
2020-04-28 00:25:33.930885 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:25:33.950010 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:25:33.968197 (MainThread): Partial parsing not enabled
2020-04-28 00:25:33.996115 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:25:33.996246 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:25:34.013739 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:25:34.013894 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:25:34.019501 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:25:34.019659 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:25:34.155430 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:25:34.157318 (MainThread): 
2020-04-28 00:25:34.157611 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:25:34.157701 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:25:34.162472 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:25:34.162626 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:25:34.247451 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:25:34.247592 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:25:34.736747 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.49 seconds
2020-04-28 00:25:34.756731 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:25:34.757053 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:25:34.759758 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:25:34.759936 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:25:34.803025 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:25:34.803450 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:25:34.803713 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:25:34.915546 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 00:25:34.920485 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:25:34.980118 (MainThread): Using postgres connection "master".
2020-04-28 00:25:34.980292 (MainThread): On master: BEGIN
2020-04-28 00:25:35.341395 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 00:25:35.341826 (MainThread): Using postgres connection "master".
2020-04-28 00:25:35.342086 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:25:35.626798 (MainThread): SQL status: SELECT in 0.28 seconds
2020-04-28 00:25:35.700557 (MainThread): On master: ROLLBACK
2020-04-28 00:25:35.741971 (MainThread): Using postgres connection "master".
2020-04-28 00:25:35.742412 (MainThread): On master: BEGIN
2020-04-28 00:25:35.821385 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:25:35.821829 (MainThread): On master: COMMIT
2020-04-28 00:25:35.822115 (MainThread): Using postgres connection "master".
2020-04-28 00:25:35.822288 (MainThread): On master: COMMIT
2020-04-28 00:25:35.862403 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:25:35.863288 (MainThread): 17:25:35 | Concurrency: 1 threads (target='dev')
2020-04-28 00:25:35.863528 (MainThread): 17:25:35 | 
2020-04-28 00:25:35.867015 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:25:35.867264 (Thread-1): 17:25:35 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 00:25:35.867594 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:25:35.867709 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:25:35.867828 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:25:35.886463 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:25:35.886973 (Thread-1): finished collecting timing info
2020-04-28 00:25:35.927141 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:35.927303 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:25:36.006787 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:25:36.011157 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.011304 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:25:36.050940 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:25:36.053893 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:25:36.054460 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.054609 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:25:36.093259 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:25:36.093459 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.093572 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS number_of_orders,
        SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:25:36.164080 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:25:36.170431 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.170586 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 00:25:36.211053 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:25:36.215467 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.215617 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:25:36.255717 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:25:36.257662 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:25:36.257854 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.258006 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:25:36.455124 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 00:25:36.457368 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:25:36.457507 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:25:36.855418 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-28 00:25:36.859932 (Thread-1): finished collecting timing info
2020-04-28 00:25:36.860804 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd09fafe6-4cd8-4f3a-880e-91f67d8f5cec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b98d10>]}
2020-04-28 00:25:36.861119 (Thread-1): 17:25:36 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 0.99s]
2020-04-28 00:25:36.861304 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:25:36.881786 (MainThread): Using postgres connection "master".
2020-04-28 00:25:36.882020 (MainThread): On master: BEGIN
2020-04-28 00:25:36.928658 (MainThread): SQL status: BEGIN in 0.05 seconds
2020-04-28 00:25:36.929088 (MainThread): On master: COMMIT
2020-04-28 00:25:36.929290 (MainThread): Using postgres connection "master".
2020-04-28 00:25:36.929437 (MainThread): On master: COMMIT
2020-04-28 00:25:36.969268 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:25:36.970151 (MainThread): 17:25:36 | 
2020-04-28 00:25:36.970386 (MainThread): 17:25:36 | Finished running 1 view model in 2.81s.
2020-04-28 00:25:36.970585 (MainThread): Connection 'master' was left open.
2020-04-28 00:25:36.970775 (MainThread): On master: Close
2020-04-28 00:25:36.971246 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:25:36.971415 (MainThread): On model.order_history.customers: Close
2020-04-28 00:25:36.975994 (MainThread): 
2020-04-28 00:25:36.976198 (MainThread): Completed successfully
2020-04-28 00:25:36.976357 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 00:25:36.976584 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bcd0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11080b390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de15690>]}
2020-04-28 00:25:36.976814 (MainThread): Flushing usage events
2020-04-28 00:32:55.034258 (MainThread): Running with dbt=0.16.1
2020-04-28 00:32:55.125906 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:32:55.127102 (MainThread): Tracking: tracking
2020-04-28 00:32:55.134601 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c2d210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e83f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c2d190>]}
2020-04-28 00:32:55.159796 (MainThread): Partial parsing not enabled
2020-04-28 00:32:55.163843 (MainThread): Parsing macros/core.sql
2020-04-28 00:32:55.170651 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:32:55.179775 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:32:55.181900 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:32:55.200598 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:32:55.241325 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:32:55.264671 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:32:55.268945 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:32:55.277920 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:32:55.293325 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:32:55.300981 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:32:55.308390 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:32:55.313634 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:32:55.314798 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:32:55.316106 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:32:55.318113 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:32:55.320392 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:32:55.330313 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:32:55.332589 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:32:55.334264 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:32:55.384158 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:32:55.385738 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:32:55.386939 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:32:55.388400 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:32:55.390893 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:32:55.393544 (MainThread): Parsing macros/relations.sql
2020-04-28 00:32:55.395124 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:32:55.413050 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:32:55.436680 (MainThread): Partial parsing not enabled
2020-04-28 00:32:55.465081 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:32:55.465207 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:32:55.482877 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:32:55.483029 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:32:55.487935 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:32:55.488035 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:32:55.618850 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:32:55.621888 (MainThread): 
2020-04-28 00:32:55.622212 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:32:55.622357 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:32:55.627982 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:32:55.628157 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:32:55.723974 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:32:55.724118 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:32:56.251550 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-28 00:32:56.274163 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:32:56.274390 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:32:56.276273 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:32:56.276404 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:32:56.316648 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:32:56.317082 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:32:56.317357 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:32:56.444614 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-28 00:32:56.449656 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:32:56.508908 (MainThread): Using postgres connection "master".
2020-04-28 00:32:56.509074 (MainThread): On master: BEGIN
2020-04-28 00:32:56.852469 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-04-28 00:32:56.852925 (MainThread): Using postgres connection "master".
2020-04-28 00:32:56.853142 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:32:57.133418 (MainThread): SQL status: SELECT in 0.28 seconds
2020-04-28 00:32:57.207289 (MainThread): On master: ROLLBACK
2020-04-28 00:32:57.246580 (MainThread): Using postgres connection "master".
2020-04-28 00:32:57.246991 (MainThread): On master: BEGIN
2020-04-28 00:32:57.322134 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 00:32:57.322444 (MainThread): On master: COMMIT
2020-04-28 00:32:57.322625 (MainThread): Using postgres connection "master".
2020-04-28 00:32:57.322783 (MainThread): On master: COMMIT
2020-04-28 00:32:57.359864 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:32:57.360509 (MainThread): 17:32:57 | Concurrency: 1 threads (target='dev')
2020-04-28 00:32:57.360755 (MainThread): 17:32:57 | 
2020-04-28 00:32:57.364243 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:32:57.364506 (Thread-1): 17:32:57 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 00:32:57.364887 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:32:57.365043 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:32:57.365196 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:32:57.385319 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:32:57.385951 (Thread-1): finished collecting timing info
2020-04-28 00:32:57.433238 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:32:57.433403 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:32:57.515762 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:32:57.520178 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:32:57.520351 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:32:57.561299 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:32:57.564432 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:32:57.565046 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:32:57.565200 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:32:57.604587 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:32:57.604873 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:32:57.605040 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS number_of_orders,

        SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:32:57.665309 (Thread-1): Postgres error: column customer_orders.tickets_sold_no_comps does not exist

2020-04-28 00:32:57.665747 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 00:32:57.705977 (Thread-1): finished collecting timing info
2020-04-28 00:32:57.707029 (Thread-1): Database Error in model customers (models/customers.sql)
  column customer_orders.tickets_sold_no_comps does not exist
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column customer_orders.tickets_sold_no_comps does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  column customer_orders.tickets_sold_no_comps does not exist
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 00:32:57.720260 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9fa13076-6cc3-4e6a-87e1-791b5c9dece2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4b1710>]}
2020-04-28 00:32:57.720599 (Thread-1): 17:32:57 | 1 of 1 ERROR creating view model data_science.customers.............. [ERROR in 0.36s]
2020-04-28 00:32:57.720798 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:32:57.773506 (MainThread): Using postgres connection "master".
2020-04-28 00:32:57.773705 (MainThread): On master: BEGIN
2020-04-28 00:32:57.812718 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:32:57.813184 (MainThread): On master: COMMIT
2020-04-28 00:32:57.813484 (MainThread): Using postgres connection "master".
2020-04-28 00:32:57.813643 (MainThread): On master: COMMIT
2020-04-28 00:32:57.851289 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:32:57.852179 (MainThread): 17:32:57 | 
2020-04-28 00:32:57.852415 (MainThread): 17:32:57 | Finished running 1 view model in 2.23s.
2020-04-28 00:32:57.852635 (MainThread): Connection 'master' was left open.
2020-04-28 00:32:57.852797 (MainThread): On master: Close
2020-04-28 00:32:57.853192 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:32:57.853356 (MainThread): On model.order_history.customers: Close
2020-04-28 00:32:57.858224 (MainThread): 
2020-04-28 00:32:57.858441 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 00:32:57.858591 (MainThread): 
2020-04-28 00:32:57.858731 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 00:32:57.858858 (MainThread):   column customer_orders.tickets_sold_no_comps does not exist
2020-04-28 00:32:57.858976 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 00:32:57.859099 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2020-04-28 00:32:57.859308 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2c6390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4b9d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a293450>]}
2020-04-28 00:32:57.859528 (MainThread): Flushing usage events
2020-04-28 00:33:06.959211 (MainThread): Running with dbt=0.16.1
2020-04-28 00:33:07.036671 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:33:07.037581 (MainThread): Tracking: tracking
2020-04-28 00:33:07.043441 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e49f550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e49f1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e49f310>]}
2020-04-28 00:33:07.066345 (MainThread): Partial parsing not enabled
2020-04-28 00:33:07.068540 (MainThread): Parsing macros/core.sql
2020-04-28 00:33:07.074096 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:33:07.083929 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:33:07.086161 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:33:07.105904 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:33:07.140783 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:33:07.163512 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:33:07.165696 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:33:07.172290 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:33:07.185625 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:33:07.192821 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:33:07.199408 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:33:07.204630 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:33:07.205634 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:33:07.206767 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:33:07.208524 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:33:07.210710 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:33:07.220616 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:33:07.222807 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:33:07.223962 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:33:07.273459 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:33:07.274773 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:33:07.275777 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:33:07.276961 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:33:07.279402 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:33:07.281965 (MainThread): Parsing macros/relations.sql
2020-04-28 00:33:07.283436 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:33:07.301040 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:33:07.319598 (MainThread): Partial parsing not enabled
2020-04-28 00:33:07.347099 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:33:07.347218 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:33:07.363665 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:33:07.363763 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:33:07.367804 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:33:07.367894 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:33:07.493834 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:33:07.495312 (MainThread): 
2020-04-28 00:33:07.495584 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:33:07.495671 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:33:07.499855 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:33:07.499959 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:33:07.603326 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:33:07.603463 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:33:07.976586 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.37 seconds
2020-04-28 00:33:07.995695 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:33:07.995900 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:33:07.997592 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:33:07.997711 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:33:08.038138 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:33:08.038571 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:33:08.038774 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:33:08.158188 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-28 00:33:08.162510 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:33:08.225881 (MainThread): Using postgres connection "master".
2020-04-28 00:33:08.226052 (MainThread): On master: BEGIN
2020-04-28 00:33:08.586492 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 00:33:08.586925 (MainThread): Using postgres connection "master".
2020-04-28 00:33:08.587109 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:33:08.743813 (MainThread): SQL status: SELECT in 0.16 seconds
2020-04-28 00:33:08.818151 (MainThread): On master: ROLLBACK
2020-04-28 00:33:08.858377 (MainThread): Using postgres connection "master".
2020-04-28 00:33:08.858641 (MainThread): On master: BEGIN
2020-04-28 00:33:08.940143 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:33:08.940391 (MainThread): On master: COMMIT
2020-04-28 00:33:08.940532 (MainThread): Using postgres connection "master".
2020-04-28 00:33:08.940657 (MainThread): On master: COMMIT
2020-04-28 00:33:08.980917 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:33:08.981509 (MainThread): 17:33:08 | Concurrency: 1 threads (target='dev')
2020-04-28 00:33:08.981683 (MainThread): 17:33:08 | 
2020-04-28 00:33:08.984102 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:33:08.984339 (Thread-1): 17:33:08 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 00:33:08.984665 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:33:08.984773 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:33:08.984891 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:33:09.002901 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:33:09.003797 (Thread-1): finished collecting timing info
2020-04-28 00:33:09.044609 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.044776 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:33:09.126592 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:33:09.130953 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.131109 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:33:09.171596 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:33:09.174393 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:33:09.174967 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.175123 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:33:09.216365 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:33:09.216596 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.216732 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS number_of_orders,

        SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        THEN order_unique_id ELSE NULL END) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        -- coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:33:09.287891 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:33:09.293206 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:33:09.293353 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 00:34:20.193670 (Thread-1): SQL status: ALTER TABLE in 70.90 seconds
2020-04-28 00:34:20.197272 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:34:20.197455 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:34:20.489068 (Thread-1): SQL status: ALTER TABLE in 0.29 seconds
2020-04-28 00:34:20.490266 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:34:20.490401 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:34:20.490506 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:34:20.815324 (Thread-1): SQL status: COMMIT in 0.32 seconds
2020-04-28 00:34:20.817415 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:34:20.817552 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:34:21.040850 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 00:34:21.045040 (Thread-1): finished collecting timing info
2020-04-28 00:34:21.045947 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd843c344-48ef-4584-abf4-972615d7ec9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed31850>]}
2020-04-28 00:34:21.046275 (Thread-1): 17:34:21 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 72.06s]
2020-04-28 00:34:21.046469 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:34:21.119779 (MainThread): Using postgres connection "master".
2020-04-28 00:34:21.119964 (MainThread): On master: BEGIN
2020-04-28 00:34:21.173254 (MainThread): SQL status: BEGIN in 0.05 seconds
2020-04-28 00:34:21.173462 (MainThread): On master: COMMIT
2020-04-28 00:34:21.173568 (MainThread): Using postgres connection "master".
2020-04-28 00:34:21.173665 (MainThread): On master: COMMIT
2020-04-28 00:34:21.214486 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:34:21.215007 (MainThread): 17:34:21 | 
2020-04-28 00:34:21.215196 (MainThread): 17:34:21 | Finished running 1 view model in 73.72s.
2020-04-28 00:34:21.215357 (MainThread): Connection 'master' was left open.
2020-04-28 00:34:21.215481 (MainThread): On master: Close
2020-04-28 00:34:21.215875 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:34:21.216037 (MainThread): On model.order_history.customers: Close
2020-04-28 00:34:21.220413 (MainThread): 
2020-04-28 00:34:21.220610 (MainThread): Completed successfully
2020-04-28 00:34:21.220757 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 00:34:21.220963 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb07d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb07e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed63c10>]}
2020-04-28 00:34:21.221173 (MainThread): Flushing usage events
2020-04-28 00:42:53.258082 (MainThread): Running with dbt=0.16.1
2020-04-28 00:42:53.354125 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:42:53.355316 (MainThread): Tracking: tracking
2020-04-28 00:42:53.363048 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051c0310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051df410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105422190>]}
2020-04-28 00:42:53.383061 (MainThread): Partial parsing not enabled
2020-04-28 00:42:53.385242 (MainThread): Parsing macros/core.sql
2020-04-28 00:42:53.390614 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:42:53.399622 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:42:53.401813 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:42:53.420258 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:42:53.454389 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:42:53.476231 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:42:53.478376 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:42:53.485045 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:42:53.498161 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:42:53.505204 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:42:53.511789 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:42:53.517038 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:42:53.518188 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:42:53.520023 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:42:53.521914 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:42:53.524249 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:42:53.533556 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:42:53.535767 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:42:53.537493 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:42:53.580171 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:42:53.582017 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:42:53.583221 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:42:53.584534 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:42:53.587090 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:42:53.589941 (MainThread): Parsing macros/relations.sql
2020-04-28 00:42:53.591543 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:42:53.608232 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:42:53.625646 (MainThread): Partial parsing not enabled
2020-04-28 00:42:53.656466 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:42:53.656645 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:42:53.678821 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:53.678948 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:42:53.683072 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:53.683165 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:42:53.814281 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:42:53.817268 (MainThread): 
2020-04-28 00:42:53.817597 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:42:53.817685 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:42:53.828383 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:42:53.828532 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:42:53.920225 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:42:53.920375 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:42:54.466529 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.55 seconds
2020-04-28 00:42:54.488582 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:42:54.488815 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:42:54.490622 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:42:54.490749 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:42:54.563060 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.07 seconds
2020-04-28 00:42:54.563487 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:42:54.563721 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:42:54.674945 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 00:42:54.678593 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:42:54.733157 (MainThread): Using postgres connection "master".
2020-04-28 00:42:54.733302 (MainThread): On master: BEGIN
2020-04-28 00:42:55.093951 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 00:42:55.094377 (MainThread): Using postgres connection "master".
2020-04-28 00:42:55.094638 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:42:55.271753 (MainThread): SQL status: SELECT in 0.18 seconds
2020-04-28 00:42:55.343179 (MainThread): On master: ROLLBACK
2020-04-28 00:42:55.381593 (MainThread): Using postgres connection "master".
2020-04-28 00:42:55.381842 (MainThread): On master: BEGIN
2020-04-28 00:42:55.459673 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:42:55.459976 (MainThread): On master: COMMIT
2020-04-28 00:42:55.460153 (MainThread): Using postgres connection "master".
2020-04-28 00:42:55.460308 (MainThread): On master: COMMIT
2020-04-28 00:42:55.499484 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:42:55.500400 (MainThread): 17:42:55 | Concurrency: 1 threads (target='dev')
2020-04-28 00:42:55.500639 (MainThread): 17:42:55 | 
2020-04-28 00:42:55.503624 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 00:42:55.503892 (Thread-1): 17:42:55 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-28 00:42:55.504294 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.504424 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:42:55.504551 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 00:42:55.520503 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 00:42:55.520981 (Thread-1): finished collecting timing info
2020-04-28 00:42:55.562152 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.562319 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 00:42:55.640509 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:42:55.644862 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.645015 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:42:55.683835 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:42:55.686438 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 00:42:55.687064 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.687223 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 00:42:55.725122 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:42:55.725385 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.725553 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 00:42:55.787176 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:42:55.793501 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.793675 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 00:42:55.836405 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:42:55.840785 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.840945 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 00:42:55.904239 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 00:42:55.906171 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:42:55.906360 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:55.906517 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:42:56.127299 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 00:42:56.130767 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:42:56.130925 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:42:56.453980 (Thread-1): SQL status: DROP VIEW in 0.32 seconds
2020-04-28 00:42:56.458458 (Thread-1): finished collecting timing info
2020-04-28 00:42:56.459320 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0d27138a-a2dc-4591-874e-0703917ecc8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10582c110>]}
2020-04-28 00:42:56.459634 (Thread-1): 17:42:56 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.95s]
2020-04-28 00:42:56.459827 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 00:42:56.460016 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-28 00:42:56.460299 (Thread-1): 17:42:56 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-28 00:42:56.460670 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.460802 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 00:42:56.460932 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-28 00:42:56.466644 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:42:56.467059 (Thread-1): finished collecting timing info
2020-04-28 00:42:56.474399 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.474511 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-28 00:42:56.683814 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 00:42:56.687984 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.688132 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:42:56.885999 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 00:42:56.889047 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:42:56.889653 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.889807 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-28 00:42:56.928882 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:42:56.929313 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:56.929582 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 00:42:56.996029 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:42:57.002300 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:57.002450 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-28 00:42:57.045596 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:42:57.049975 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:57.050192 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-28 00:42:57.089017 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:42:57.090755 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:42:57.090928 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:57.091073 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:42:57.310237 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 00:42:57.313780 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:42:57.313941 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:42:57.507229 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 00:42:57.511498 (Thread-1): finished collecting timing info
2020-04-28 00:42:57.512424 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0d27138a-a2dc-4591-874e-0703917ecc8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a91f90>]}
2020-04-28 00:42:57.512735 (Thread-1): 17:42:57 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 1.05s]
2020-04-28 00:42:57.512913 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-28 00:42:57.513441 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:42:57.513766 (Thread-1): 17:42:57 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-28 00:42:57.514216 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:42:57.514357 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-28 00:42:57.514469 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:42:57.523366 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:42:57.523813 (Thread-1): finished collecting timing info
2020-04-28 00:42:57.531195 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:57.531377 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:42:57.728795 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 00:42:57.732944 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:57.733095 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:42:57.918307 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 00:42:57.921448 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:42:57.922055 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:57.922209 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:42:57.961280 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:42:57.961705 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:57.961980 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,

        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS total_revenue

        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT order_unique_id) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        -- coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:42:58.027093 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:42:58.032387 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:58.032535 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:42:58.072843 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:42:58.074778 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:42:58.074968 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:58.075121 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:42:58.373113 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-28 00:42:58.410242 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:42:58.410453 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:42:58.613414 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 00:42:58.617728 (Thread-1): finished collecting timing info
2020-04-28 00:42:58.618574 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0d27138a-a2dc-4591-874e-0703917ecc8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10584a3d0>]}
2020-04-28 00:42:58.618897 (Thread-1): 17:42:58 | 3 of 3 OK created view model data_science.customers.................. [CREATE VIEW in 1.10s]
2020-04-28 00:42:58.619079 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:42:58.725177 (MainThread): Using postgres connection "master".
2020-04-28 00:42:58.725505 (MainThread): On master: BEGIN
2020-04-28 00:42:58.764233 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:42:58.764683 (MainThread): On master: COMMIT
2020-04-28 00:42:58.764957 (MainThread): Using postgres connection "master".
2020-04-28 00:42:58.765137 (MainThread): On master: COMMIT
2020-04-28 00:42:58.804005 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:42:58.804467 (MainThread): 17:42:58 | 
2020-04-28 00:42:58.804625 (MainThread): 17:42:58 | Finished running 3 view models in 4.99s.
2020-04-28 00:42:58.804754 (MainThread): Connection 'master' was left open.
2020-04-28 00:42:58.804855 (MainThread): On master: Close
2020-04-28 00:42:58.805130 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:42:58.805233 (MainThread): On model.order_history.customers: Close
2020-04-28 00:42:58.816886 (MainThread): 
2020-04-28 00:42:58.817203 (MainThread): Completed successfully
2020-04-28 00:42:58.817413 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2020-04-28 00:42:58.817734 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a73250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a73fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105477690>]}
2020-04-28 00:42:58.818072 (MainThread): Flushing usage events
2020-04-28 00:48:52.912809 (MainThread): Running with dbt=0.16.1
2020-04-28 00:48:53.004099 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:48:53.005534 (MainThread): Tracking: tracking
2020-04-28 00:48:53.012282 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112958650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bdfed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bcbe50>]}
2020-04-28 00:48:53.037605 (MainThread): Partial parsing not enabled
2020-04-28 00:48:53.041446 (MainThread): Parsing macros/core.sql
2020-04-28 00:48:53.048037 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:48:53.056771 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:48:53.059019 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:48:53.079524 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:48:53.119397 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:48:53.141249 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:48:53.143701 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:48:53.151420 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:48:53.165235 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:48:53.172548 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:48:53.179312 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:48:53.184786 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:48:53.186151 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:48:53.187710 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:48:53.189873 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:48:53.192503 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:48:53.202588 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:48:53.204862 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:48:53.206745 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:48:53.251516 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:48:53.254366 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:48:53.256286 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:48:53.258401 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:48:53.261748 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:48:53.264970 (MainThread): Parsing macros/relations.sql
2020-04-28 00:48:53.266857 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:48:53.286063 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:48:53.306139 (MainThread): Partial parsing not enabled
2020-04-28 00:48:53.334946 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:48:53.335091 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:48:53.352922 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:53.353062 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:48:53.357989 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:53.358124 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:48:53.496191 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:48:53.498415 (MainThread): 
2020-04-28 00:48:53.498751 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:48:53.498842 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:48:53.510437 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:48:53.510560 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:48:53.599081 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:48:53.599224 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:48:54.132937 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.53 seconds
2020-04-28 00:48:54.150572 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:48:54.150709 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:48:54.152230 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:48:54.152338 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:48:54.192769 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:48:54.193197 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:48:54.193479 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:48:54.325026 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-28 00:48:54.329688 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:48:54.389426 (MainThread): Using postgres connection "master".
2020-04-28 00:48:54.389595 (MainThread): On master: BEGIN
2020-04-28 00:48:54.772593 (MainThread): SQL status: BEGIN in 0.38 seconds
2020-04-28 00:48:54.772850 (MainThread): Using postgres connection "master".
2020-04-28 00:48:54.772983 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:48:54.950677 (MainThread): SQL status: SELECT in 0.18 seconds
2020-04-28 00:48:55.025328 (MainThread): On master: ROLLBACK
2020-04-28 00:48:55.068219 (MainThread): Using postgres connection "master".
2020-04-28 00:48:55.068634 (MainThread): On master: BEGIN
2020-04-28 00:48:55.163260 (MainThread): SQL status: BEGIN in 0.09 seconds
2020-04-28 00:48:55.163467 (MainThread): On master: COMMIT
2020-04-28 00:48:55.163581 (MainThread): Using postgres connection "master".
2020-04-28 00:48:55.163676 (MainThread): On master: COMMIT
2020-04-28 00:48:55.206261 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:48:55.207152 (MainThread): 17:48:55 | Concurrency: 1 threads (target='dev')
2020-04-28 00:48:55.207408 (MainThread): 17:48:55 | 
2020-04-28 00:48:55.210590 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 00:48:55.210851 (Thread-1): 17:48:55 | 1 of 3 START view model data_science.stg_customers................... [RUN]
2020-04-28 00:48:55.211223 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.211345 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:48:55.211473 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 00:48:55.227335 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 00:48:55.227791 (Thread-1): finished collecting timing info
2020-04-28 00:48:55.268384 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.268556 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 00:48:55.350507 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:48:55.354765 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.354922 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:48:55.394551 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:48:55.397230 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 00:48:55.399040 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.399333 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 00:48:55.438586 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:48:55.439011 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.439277 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 00:48:55.498661 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:48:55.502966 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.503108 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 00:48:55.543018 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:48:55.546681 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.546852 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 00:48:55.602628 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 00:48:55.604594 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:48:55.604791 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.604949 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 00:48:55.874692 (Thread-1): SQL status: COMMIT in 0.27 seconds
2020-04-28 00:48:55.877985 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 00:48:55.878168 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 00:48:56.259690 (Thread-1): SQL status: DROP VIEW in 0.38 seconds
2020-04-28 00:48:56.264175 (Thread-1): finished collecting timing info
2020-04-28 00:48:56.265048 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '78a106d7-f913-459a-889b-1104fcd36168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1130263d0>]}
2020-04-28 00:48:56.265375 (Thread-1): 17:48:56 | 1 of 3 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.05s]
2020-04-28 00:48:56.265568 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 00:48:56.265779 (Thread-1): Began running node model.order_history.stg_orders_aggregate
2020-04-28 00:48:56.266066 (Thread-1): 17:48:56 | 2 of 3 START view model data_science.stg_orders_aggregate............ [RUN]
2020-04-28 00:48:56.266776 (Thread-1): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:56.266944 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 00:48:56.267095 (Thread-1): Compiling model.order_history.stg_orders_aggregate
2020-04-28 00:48:56.273237 (Thread-1): Writing injected SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:48:56.273695 (Thread-1): finished collecting timing info
2020-04-28 00:48:56.282252 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:56.282393 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" cascade
2020-04-28 00:48:56.684762 (Thread-1): SQL status: DROP VIEW in 0.40 seconds
2020-04-28 00:48:56.688301 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:56.688453 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:48:57.078966 (Thread-1): SQL status: DROP VIEW in 0.39 seconds
2020-04-28 00:48:57.082007 (Thread-1): Writing runtime SQL for node "model.order_history.stg_orders_aggregate"
2020-04-28 00:48:57.082636 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:57.082796 (Thread-1): On model.order_history.stg_orders_aggregate: BEGIN
2020-04-28 00:48:57.122926 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:48:57.123369 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:57.123692 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */

  create view "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 00:48:57.183861 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 00:48:57.188481 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:48:57.188627 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate" rename to "stg_orders_aggregate__dbt_backup"
2020-04-28 00:49:19.463331 (Thread-1): SQL status: ALTER TABLE in 22.27 seconds
2020-04-28 00:49:19.467414 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:49:19.467600 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
alter table "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_tmp" rename to "stg_orders_aggregate"
2020-04-28 00:49:19.508388 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:49:19.509646 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:49:19.509796 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:49:19.509906 (Thread-1): On model.order_history.stg_orders_aggregate: COMMIT
2020-04-28 00:49:19.990916 (Thread-1): SQL status: COMMIT in 0.48 seconds
2020-04-28 00:49:19.993733 (Thread-1): Using postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:49:19.993890 (Thread-1): On model.order_history.stg_orders_aggregate: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_orders_aggregate"} */
drop view if exists "data_platform_prod"."data_science"."stg_orders_aggregate__dbt_backup" cascade
2020-04-28 00:49:20.214872 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 00:49:20.218448 (Thread-1): finished collecting timing info
2020-04-28 00:49:20.219298 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '78a106d7-f913-459a-889b-1104fcd36168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113209450>]}
2020-04-28 00:49:20.219553 (Thread-1): 17:49:20 | 2 of 3 OK created view model data_science.stg_orders_aggregate....... [CREATE VIEW in 23.95s]
2020-04-28 00:49:20.219705 (Thread-1): Finished running node model.order_history.stg_orders_aggregate
2020-04-28 00:49:20.220121 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:49:20.220322 (Thread-1): 17:49:20 | 3 of 3 START view model data_science.customers....................... [RUN]
2020-04-28 00:49:20.220876 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:49:20.221249 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_orders_aggregate).
2020-04-28 00:49:20.221403 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:49:20.230842 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:49:20.231304 (Thread-1): finished collecting timing info
2020-04-28 00:49:20.238385 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:49:20.238520 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:49:20.428620 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 00:49:20.432831 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:49:20.432984 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:49:20.664557 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 00:49:20.667800 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:49:20.668380 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:49:20.668550 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:49:20.707995 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:49:20.708324 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:49:20.708553 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,

        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN total_revenue ELSE NULL END) AS total_revenue

        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT total_revenue) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        -- coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:49:20.754271 (Thread-1): Postgres error: column "total_revenue" does not exist in order_tickets

2020-04-28 00:49:20.754479 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 00:49:20.793579 (Thread-1): finished collecting timing info
2020-04-28 00:49:20.794421 (Thread-1): Database Error in model customers (models/customers.sql)
  column "total_revenue" does not exist in order_tickets
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column "total_revenue" does not exist in order_tickets


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  column "total_revenue" does not exist in order_tickets
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 00:49:20.802581 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '78a106d7-f913-459a-889b-1104fcd36168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113273850>]}
2020-04-28 00:49:20.802932 (Thread-1): 17:49:20 | 3 of 3 ERROR creating view model data_science.customers.............. [ERROR in 0.58s]
2020-04-28 00:49:20.803120 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:49:20.828893 (MainThread): Using postgres connection "master".
2020-04-28 00:49:20.829089 (MainThread): On master: BEGIN
2020-04-28 00:49:20.871821 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:49:20.872135 (MainThread): On master: COMMIT
2020-04-28 00:49:20.872316 (MainThread): Using postgres connection "master".
2020-04-28 00:49:20.872484 (MainThread): On master: COMMIT
2020-04-28 00:49:20.914843 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:49:20.915423 (MainThread): 17:49:20 | 
2020-04-28 00:49:20.915634 (MainThread): 17:49:20 | Finished running 3 view models in 27.42s.
2020-04-28 00:49:20.916031 (MainThread): Connection 'master' was left open.
2020-04-28 00:49:20.916208 (MainThread): On master: Close
2020-04-28 00:49:20.916851 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:49:20.917002 (MainThread): On model.order_history.customers: Close
2020-04-28 00:49:20.929505 (MainThread): 
2020-04-28 00:49:20.929754 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 00:49:20.929914 (MainThread): 
2020-04-28 00:49:20.930057 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 00:49:20.930189 (MainThread):   column "total_revenue" does not exist in order_tickets
2020-04-28 00:49:20.930310 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 00:49:20.930456 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2020-04-28 00:49:20.930675 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113013f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112fddb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113022290>]}
2020-04-28 00:49:20.930909 (MainThread): Flushing usage events
2020-04-28 00:50:05.046587 (MainThread): Running with dbt=0.16.1
2020-04-28 00:50:05.131261 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 00:50:05.132720 (MainThread): Tracking: tracking
2020-04-28 00:50:05.139296 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4cac50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d723e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d723f10>]}
2020-04-28 00:50:05.162630 (MainThread): Partial parsing not enabled
2020-04-28 00:50:05.165274 (MainThread): Parsing macros/core.sql
2020-04-28 00:50:05.171301 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 00:50:05.180167 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 00:50:05.182330 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 00:50:05.201050 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 00:50:05.236336 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 00:50:05.263860 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 00:50:05.266038 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 00:50:05.272845 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 00:50:05.285888 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 00:50:05.293482 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 00:50:05.300053 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 00:50:05.305213 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 00:50:05.306389 (MainThread): Parsing macros/etc/query.sql
2020-04-28 00:50:05.307737 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 00:50:05.309768 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 00:50:05.312468 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 00:50:05.322724 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 00:50:05.325575 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 00:50:05.327335 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 00:50:05.377137 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 00:50:05.379534 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 00:50:05.381503 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 00:50:05.383305 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 00:50:05.387397 (MainThread): Parsing macros/catalog.sql
2020-04-28 00:50:05.391087 (MainThread): Parsing macros/relations.sql
2020-04-28 00:50:05.393105 (MainThread): Parsing macros/adapters.sql
2020-04-28 00:50:05.413892 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 00:50:05.434119 (MainThread): Partial parsing not enabled
2020-04-28 00:50:05.461424 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:50:05.461526 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:50:05.478791 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 00:50:05.478926 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:50:05.483843 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 00:50:05.483946 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:50:05.624837 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 00:50:05.626331 (MainThread): 
2020-04-28 00:50:05.626624 (MainThread): Acquiring new postgres connection "master".
2020-04-28 00:50:05.626714 (MainThread): Opening a new connection, currently in state init
2020-04-28 00:50:05.631725 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 00:50:05.631953 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 00:50:05.717969 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 00:50:05.718112 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 00:50:06.186179 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.47 seconds
2020-04-28 00:50:06.208109 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:50:06.208334 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 00:50:06.210151 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:50:06.210276 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 00:50:06.283132 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.07 seconds
2020-04-28 00:50:06.283345 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 00:50:06.283463 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 00:50:06.413881 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.13 seconds
2020-04-28 00:50:06.417825 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 00:50:06.479987 (MainThread): Using postgres connection "master".
2020-04-28 00:50:06.480162 (MainThread): On master: BEGIN
2020-04-28 00:50:06.826552 (MainThread): SQL status: BEGIN in 0.35 seconds
2020-04-28 00:50:06.826977 (MainThread): Using postgres connection "master".
2020-04-28 00:50:06.827247 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 00:50:07.121780 (MainThread): SQL status: SELECT in 0.29 seconds
2020-04-28 00:50:07.192126 (MainThread): On master: ROLLBACK
2020-04-28 00:50:07.231048 (MainThread): Using postgres connection "master".
2020-04-28 00:50:07.231454 (MainThread): On master: BEGIN
2020-04-28 00:50:07.308625 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 00:50:07.308840 (MainThread): On master: COMMIT
2020-04-28 00:50:07.308958 (MainThread): Using postgres connection "master".
2020-04-28 00:50:07.309060 (MainThread): On master: COMMIT
2020-04-28 00:50:07.347266 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 00:50:07.347686 (MainThread): 17:50:07 | Concurrency: 1 threads (target='dev')
2020-04-28 00:50:07.347832 (MainThread): 17:50:07 | 
2020-04-28 00:50:07.350038 (Thread-1): Began running node model.order_history.customers
2020-04-28 00:50:07.350231 (Thread-1): 17:50:07 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 00:50:07.350524 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 00:50:07.350626 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 00:50:07.350730 (Thread-1): Compiling model.order_history.customers
2020-04-28 00:50:07.367595 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 00:50:07.368024 (Thread-1): finished collecting timing info
2020-04-28 00:50:07.405612 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.405773 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 00:50:07.484704 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 00:50:07.488713 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.488882 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:50:07.528789 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 00:50:07.531879 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 00:50:07.532510 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.532667 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 00:50:07.571388 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:50:07.571819 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.572089 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,

        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,

        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN amount_gross ELSE NULL END) AS total_revenue

        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        -- coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 00:50:07.642892 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2020-04-28 00:50:07.645953 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.646094 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 00:50:07.686487 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 00:50:07.687573 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:50:07.687691 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.687786 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 00:50:07.932601 (Thread-1): SQL status: COMMIT in 0.24 seconds
2020-04-28 00:50:07.935999 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 00:50:07.936173 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 00:50:08.555104 (Thread-1): SQL status: DROP VIEW in 0.62 seconds
2020-04-28 00:50:08.559431 (Thread-1): finished collecting timing info
2020-04-28 00:50:08.560294 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9553c5cc-1be7-4cc9-8cf9-ce4fb95c6579', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db36850>]}
2020-04-28 00:50:08.560614 (Thread-1): 17:50:08 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 1.21s]
2020-04-28 00:50:08.560805 (Thread-1): Finished running node model.order_history.customers
2020-04-28 00:50:08.586457 (MainThread): Using postgres connection "master".
2020-04-28 00:50:08.586756 (MainThread): On master: BEGIN
2020-04-28 00:50:08.625044 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 00:50:08.625376 (MainThread): On master: COMMIT
2020-04-28 00:50:08.625547 (MainThread): Using postgres connection "master".
2020-04-28 00:50:08.625701 (MainThread): On master: COMMIT
2020-04-28 00:50:08.686292 (MainThread): SQL status: COMMIT in 0.06 seconds
2020-04-28 00:50:08.686740 (MainThread): 17:50:08 | 
2020-04-28 00:50:08.686898 (MainThread): 17:50:08 | Finished running 1 view model in 3.06s.
2020-04-28 00:50:08.687033 (MainThread): Connection 'master' was left open.
2020-04-28 00:50:08.687139 (MainThread): On master: Close
2020-04-28 00:50:08.687417 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 00:50:08.687524 (MainThread): On model.order_history.customers: Close
2020-04-28 00:50:08.691515 (MainThread): 
2020-04-28 00:50:08.691695 (MainThread): Completed successfully
2020-04-28 00:50:08.691838 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 00:50:08.692045 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd6c590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8b7e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd2ef50>]}
2020-04-28 00:50:08.692255 (MainThread): Flushing usage events
2020-04-28 01:00:32.034299 (MainThread): Running with dbt=0.16.1
2020-04-28 01:00:32.109420 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=['staging'], full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 01:00:32.110171 (MainThread): Tracking: tracking
2020-04-28 01:00:32.115645 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a076d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c77fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a07d90>]}
2020-04-28 01:00:32.138033 (MainThread): Partial parsing not enabled
2020-04-28 01:00:32.140255 (MainThread): Parsing macros/core.sql
2020-04-28 01:00:32.145726 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 01:00:32.154657 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 01:00:32.156525 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 01:00:32.175184 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 01:00:32.209856 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 01:00:32.231709 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 01:00:32.233684 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 01:00:32.240502 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 01:00:32.253344 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 01:00:32.260355 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 01:00:32.266797 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 01:00:32.272026 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 01:00:32.273015 (MainThread): Parsing macros/etc/query.sql
2020-04-28 01:00:32.274575 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 01:00:32.276434 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 01:00:32.278832 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 01:00:32.289766 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 01:00:32.292209 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 01:00:32.293473 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 01:00:32.341269 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 01:00:32.342523 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 01:00:32.343529 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 01:00:32.344662 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 01:00:32.346988 (MainThread): Parsing macros/catalog.sql
2020-04-28 01:00:32.349394 (MainThread): Parsing macros/relations.sql
2020-04-28 01:00:32.350795 (MainThread): Parsing macros/adapters.sql
2020-04-28 01:00:32.368042 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 01:00:32.386363 (MainThread): Partial parsing not enabled
2020-04-28 01:00:32.416368 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 01:00:32.416506 (MainThread): Opening a new connection, currently in state init
2020-04-28 01:00:32.432999 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 01:00:32.433112 (MainThread): Opening a new connection, currently in state init
2020-04-28 01:00:32.437381 (MainThread): Acquiring new postgres connection "model.order_history.stg_orders_aggregate".
2020-04-28 01:00:32.437478 (MainThread): Opening a new connection, currently in state init
2020-04-28 01:00:32.563944 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 01:00:32.565400 (MainThread): 
2020-04-28 01:00:32.565672 (MainThread): Acquiring new postgres connection "master".
2020-04-28 01:00:32.565756 (MainThread): Opening a new connection, currently in state init
2020-04-28 01:00:32.570091 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 01:00:32.570196 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 01:00:32.666109 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 01:00:32.666242 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 01:00:33.206200 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.54 seconds
2020-04-28 01:00:33.226835 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 01:00:33.226971 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 01:00:33.228766 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 01:00:33.228893 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 01:00:33.271414 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 01:00:33.271710 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 01:00:33.271885 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 01:00:33.412061 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.14 seconds
2020-04-28 01:00:33.417105 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 01:00:33.477924 (MainThread): Using postgres connection "master".
2020-04-28 01:00:33.478093 (MainThread): On master: BEGIN
2020-04-28 01:00:33.836369 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 01:00:33.836648 (MainThread): Using postgres connection "master".
2020-04-28 01:00:33.836806 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 01:00:34.113307 (MainThread): SQL status: SELECT in 0.28 seconds
2020-04-28 01:00:34.188440 (MainThread): On master: ROLLBACK
2020-04-28 01:00:34.227860 (MainThread): Using postgres connection "master".
2020-04-28 01:00:34.228265 (MainThread): On master: BEGIN
2020-04-28 01:00:34.306848 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 01:00:34.307141 (MainThread): On master: COMMIT
2020-04-28 01:00:34.307314 (MainThread): Using postgres connection "master".
2020-04-28 01:00:34.307466 (MainThread): On master: COMMIT
2020-04-28 01:00:34.346464 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 01:00:34.346996 (MainThread): 18:00:34 | Concurrency: 1 threads (target='dev')
2020-04-28 01:00:34.347246 (MainThread): 18:00:34 | 
2020-04-28 01:00:34.350713 (Thread-1): Began running node model.order_history.customers
2020-04-28 01:00:34.351037 (Thread-1): 18:00:34 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 01:00:34.351522 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 01:00:34.351711 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 01:00:34.351923 (Thread-1): Compiling model.order_history.customers
2020-04-28 01:00:34.376686 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 01:00:34.377217 (Thread-1): finished collecting timing info
2020-04-28 01:00:34.415090 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.415254 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 01:00:34.499992 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 01:00:34.504419 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.504581 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 01:00:34.549048 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 01:00:34.551929 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 01:00:34.552577 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.552738 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 01:00:34.593895 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 01:00:34.594188 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.594361 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_tickets as (
    select * from "data_platform_prod"."data_science"."stg_orders_aggregate"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,
        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN amount_gross ELSE NULL END) AS total_revenue
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue

    from order_tickets
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 01:00:34.677728 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2020-04-28 01:00:34.682529 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.682675 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 01:00:34.729575 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 01:00:34.732240 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.732359 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 01:00:34.774758 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 01:00:34.776740 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 01:00:34.776930 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:34.777085 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 01:00:35.108684 (Thread-1): SQL status: COMMIT in 0.33 seconds
2020-04-28 01:00:35.111560 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 01:00:35.111723 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 01:00:35.502020 (Thread-1): SQL status: DROP VIEW in 0.39 seconds
2020-04-28 01:00:35.505852 (Thread-1): finished collecting timing info
2020-04-28 01:00:35.506655 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66ae1a8d-89e0-4f95-a1c1-016f3cb0c386', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120cb490>]}
2020-04-28 01:00:35.506919 (Thread-1): 18:00:35 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 1.16s]
2020-04-28 01:00:35.507076 (Thread-1): Finished running node model.order_history.customers
2020-04-28 01:00:35.582340 (MainThread): Using postgres connection "master".
2020-04-28 01:00:35.582705 (MainThread): On master: BEGIN
2020-04-28 01:00:35.630398 (MainThread): SQL status: BEGIN in 0.05 seconds
2020-04-28 01:00:35.630867 (MainThread): On master: COMMIT
2020-04-28 01:00:35.631081 (MainThread): Using postgres connection "master".
2020-04-28 01:00:35.631233 (MainThread): On master: COMMIT
2020-04-28 01:00:35.678686 (MainThread): SQL status: COMMIT in 0.05 seconds
2020-04-28 01:00:35.679570 (MainThread): 18:00:35 | 
2020-04-28 01:00:35.679803 (MainThread): 18:00:35 | Finished running 1 view model in 3.11s.
2020-04-28 01:00:35.680000 (MainThread): Connection 'master' was left open.
2020-04-28 01:00:35.680150 (MainThread): On master: Close
2020-04-28 01:00:35.680538 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 01:00:35.680696 (MainThread): On model.order_history.customers: Close
2020-04-28 01:00:35.685565 (MainThread): 
2020-04-28 01:00:35.685764 (MainThread): Completed successfully
2020-04-28 01:00:35.685919 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 01:00:35.686137 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122cb9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111fe8410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120cbf90>]}
2020-04-28 01:00:35.686369 (MainThread): Flushing usage events
2020-04-28 04:49:26.571429 (MainThread): Running with dbt=0.16.1
2020-04-28 04:49:26.691625 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 04:49:26.692821 (MainThread): Tracking: tracking
2020-04-28 04:49:26.702314 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e73810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060cced0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e732d0>]}
2020-04-28 04:49:26.727110 (MainThread): Partial parsing not enabled
2020-04-28 04:49:26.731236 (MainThread): Parsing macros/core.sql
2020-04-28 04:49:26.738747 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 04:49:26.748262 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 04:49:26.751257 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 04:49:26.770335 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 04:49:26.804482 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 04:49:26.826231 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 04:49:26.828835 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 04:49:26.835938 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 04:49:26.849168 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 04:49:26.856639 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 04:49:26.863664 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 04:49:26.869363 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 04:49:26.871040 (MainThread): Parsing macros/etc/query.sql
2020-04-28 04:49:26.872787 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 04:49:26.875169 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 04:49:26.878005 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 04:49:26.888106 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 04:49:26.890807 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 04:49:26.893472 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 04:49:26.936782 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 04:49:26.938868 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 04:49:26.940671 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 04:49:26.942462 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 04:49:26.945467 (MainThread): Parsing macros/catalog.sql
2020-04-28 04:49:26.948671 (MainThread): Parsing macros/relations.sql
2020-04-28 04:49:26.951496 (MainThread): Parsing macros/adapters.sql
2020-04-28 04:49:26.974407 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 04:49:26.992574 (MainThread): Partial parsing not enabled
2020-04-28 04:49:27.020031 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:49:27.020159 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.038351 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:27.038496 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.043207 (MainThread): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:27.043310 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.051463 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:27.051588 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.055740 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:49:27.055827 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.189308 (MainThread): Found 5 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 04:49:27.192632 (MainThread): 
2020-04-28 04:49:27.193017 (MainThread): Acquiring new postgres connection "master".
2020-04-28 04:49:27.193133 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:49:27.210017 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 04:49:27.210147 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 04:49:27.309317 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 04:49:27.309459 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 04:49:27.917107 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.61 seconds
2020-04-28 04:49:27.948972 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:49:27.949207 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 04:49:27.951030 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:49:27.951161 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 04:49:27.989881 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:27.990061 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:49:27.990161 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 04:49:28.094416 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-28 04:49:28.098795 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 04:49:28.164517 (MainThread): Using postgres connection "master".
2020-04-28 04:49:28.164691 (MainThread): On master: BEGIN
2020-04-28 04:49:28.806596 (MainThread): SQL status: BEGIN in 0.64 seconds
2020-04-28 04:49:28.807026 (MainThread): Using postgres connection "master".
2020-04-28 04:49:28.807288 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 04:49:28.965581 (MainThread): SQL status: SELECT in 0.16 seconds
2020-04-28 04:49:29.047561 (MainThread): On master: ROLLBACK
2020-04-28 04:49:29.117631 (MainThread): Using postgres connection "master".
2020-04-28 04:49:29.118037 (MainThread): On master: BEGIN
2020-04-28 04:49:29.256621 (MainThread): SQL status: BEGIN in 0.14 seconds
2020-04-28 04:49:29.256851 (MainThread): On master: COMMIT
2020-04-28 04:49:29.256974 (MainThread): Using postgres connection "master".
2020-04-28 04:49:29.257085 (MainThread): On master: COMMIT
2020-04-28 04:49:29.329743 (MainThread): SQL status: COMMIT in 0.07 seconds
2020-04-28 04:49:29.330188 (MainThread): 21:49:29 | Concurrency: 1 threads (target='dev')
2020-04-28 04:49:29.330359 (MainThread): 21:49:29 | 
2020-04-28 04:49:29.333866 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 04:49:29.334096 (Thread-1): 21:49:29 | 1 of 5 START view model data_science.stg_flash....................... [RUN]
2020-04-28 04:49:29.334452 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.334565 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 04:49:29.334683 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 04:49:29.353015 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 04:49:29.353684 (Thread-1): finished collecting timing info
2020-04-28 04:49:29.394834 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.395013 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 04:49:29.474584 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 04:49:29.479053 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.479208 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:49:29.518361 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 04:49:29.521474 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 04:49:29.522084 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.522244 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 04:49:29.561543 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:29.561976 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.562235 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 04:49:29.624400 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:49:29.628595 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.628750 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 04:49:29.667872 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:49:29.669199 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:49:29.669335 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.669442 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:49:29.838252 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:49:29.842638 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:49:29.842856 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:49:30.009056 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:49:30.012362 (Thread-1): finished collecting timing info
2020-04-28 04:49:30.013133 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ff02455-6753-4c91-b6d6-a0c39d1bec07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066fbf90>]}
2020-04-28 04:49:30.013403 (Thread-1): 21:49:30 | 1 of 5 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.68s]
2020-04-28 04:49:30.013560 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 04:49:30.013723 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 04:49:30.013882 (Thread-1): 21:49:30 | 2 of 5 START view model data_science.stg_order....................... [RUN]
2020-04-28 04:49:30.014280 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.014405 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 04:49:30.014518 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 04:49:30.020700 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 04:49:30.021135 (Thread-1): finished collecting timing info
2020-04-28 04:49:30.028840 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.028993 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 04:49:30.197334 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:49:30.201539 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.201692 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:49:30.420960 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:49:30.423993 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 04:49:30.424611 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.424779 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 04:49:30.466099 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:30.466538 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.466812 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 04:49:30.523037 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:49:30.527320 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.527478 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 04:49:30.567057 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:49:30.569005 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:49:30.569204 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.569363 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:49:30.746224 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 04:49:30.748262 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:49:30.748382 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:49:30.952286 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 04:49:30.956554 (Thread-1): finished collecting timing info
2020-04-28 04:49:30.957398 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ff02455-6753-4c91-b6d6-a0c39d1bec07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106377290>]}
2020-04-28 04:49:30.957703 (Thread-1): 21:49:30 | 2 of 5 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.94s]
2020-04-28 04:49:30.957882 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 04:49:30.958105 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 04:49:30.958342 (Thread-1): 21:49:30 | 3 of 5 START view model data_science.stg_customers................... [RUN]
2020-04-28 04:49:30.959003 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:30.959333 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 04:49:30.959540 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 04:49:30.965977 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 04:49:30.966473 (Thread-1): finished collecting timing info
2020-04-28 04:49:30.974142 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:30.974277 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 04:49:31.163869 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 04:49:31.168150 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.168310 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:49:31.348503 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:49:31.353008 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 04:49:31.353691 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.353852 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 04:49:31.392772 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:31.392978 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.393094 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 04:49:31.445963 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:49:31.480465 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.480647 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 04:49:31.523840 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:49:31.528008 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.528162 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 04:49:31.570769 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:49:31.572701 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:49:31.572898 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.573056 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:49:31.746090 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:49:31.749003 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:49:31.749190 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:49:31.931014 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:49:31.935308 (Thread-1): finished collecting timing info
2020-04-28 04:49:31.936308 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ff02455-6753-4c91-b6d6-a0c39d1bec07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106744f10>]}
2020-04-28 04:49:31.936614 (Thread-1): 21:49:31 | 3 of 5 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.98s]
2020-04-28 04:49:31.936795 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 04:49:31.937043 (Thread-1): Began running node model.order_history.stg_order_flash
2020-04-28 04:49:31.937396 (Thread-1): 21:49:31 | 4 of 5 START view model data_science.stg_order_flash................. [RUN]
2020-04-28 04:49:31.937782 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:31.937922 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 04:49:31.938062 (Thread-1): Compiling model.order_history.stg_order_flash
2020-04-28 04:49:31.947178 (Thread-1): Writing injected SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:49:31.947632 (Thread-1): finished collecting timing info
2020-04-28 04:49:31.955059 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:31.955188 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" cascade
2020-04-28 04:49:32.127441 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:49:32.130142 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:32.130268 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:49:32.299935 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:49:32.302944 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:49:32.303582 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:32.303749 (Thread-1): On model.order_history.stg_order_flash: BEGIN
2020-04-28 04:49:32.343906 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:49:32.344345 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:49:32.344620 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */

  create view "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final;
  );

2020-04-28 04:49:32.384451 (Thread-1): Postgres error: syntax error at or near "ON"
LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
                                   ^

2020-04-28 04:49:32.384866 (Thread-1): On model.order_history.stg_order_flash: ROLLBACK
2020-04-28 04:49:32.424333 (Thread-1): finished collecting timing info
2020-04-28 04:49:32.424926 (Thread-1): Database Error in model stg_order_flash (models/staging/stg_order_flash.sql)
  syntax error at or near "ON"
  LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
                                     ^
  compiled SQL at target/run/order_history/staging/stg_order_flash.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "ON"
LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
                                   ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model stg_order_flash (models/staging/stg_order_flash.sql)
  syntax error at or near "ON"
  LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
                                     ^
  compiled SQL at target/run/order_history/staging/stg_order_flash.sql
2020-04-28 04:49:32.444265 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ff02455-6753-4c91-b6d6-a0c39d1bec07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106250910>]}
2020-04-28 04:49:32.444529 (Thread-1): 21:49:32 | 4 of 5 ERROR creating view model data_science.stg_order_flash........ [ERROR in 0.51s]
2020-04-28 04:49:32.444671 (Thread-1): Finished running node model.order_history.stg_order_flash
2020-04-28 04:49:32.445064 (Thread-1): Began running node model.order_history.customers
2020-04-28 04:49:32.445239 (Thread-1): 21:49:32 | 5 of 5 SKIP relation data_science.customers.......................... [SKIP]
2020-04-28 04:49:32.445374 (Thread-1): Finished running node model.order_history.customers
2020-04-28 04:49:32.519535 (MainThread): Using postgres connection "master".
2020-04-28 04:49:32.519860 (MainThread): On master: BEGIN
2020-04-28 04:49:32.590234 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 04:49:32.590638 (MainThread): On master: COMMIT
2020-04-28 04:49:32.590934 (MainThread): Using postgres connection "master".
2020-04-28 04:49:32.591104 (MainThread): On master: COMMIT
2020-04-28 04:49:32.658528 (MainThread): SQL status: COMMIT in 0.07 seconds
2020-04-28 04:49:32.659350 (MainThread): 21:49:32 | 
2020-04-28 04:49:32.659592 (MainThread): 21:49:32 | Finished running 5 view models in 5.47s.
2020-04-28 04:49:32.659786 (MainThread): Connection 'master' was left open.
2020-04-28 04:49:32.659940 (MainThread): On master: Close
2020-04-28 04:49:32.660331 (MainThread): Connection 'model.order_history.stg_order_flash' was left open.
2020-04-28 04:49:32.660496 (MainThread): On model.order_history.stg_order_flash: Close
2020-04-28 04:49:32.678453 (MainThread): 
2020-04-28 04:49:32.678762 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 04:49:32.678916 (MainThread): 
2020-04-28 04:49:32.679055 (MainThread): Database Error in model stg_order_flash (models/staging/stg_order_flash.sql)
2020-04-28 04:49:32.679180 (MainThread):   syntax error at or near "ON"
2020-04-28 04:49:32.679295 (MainThread):   LINE 21:     from orders LEFT JOIN ON flash.fk_order_unique_id=orders...
2020-04-28 04:49:32.679405 (MainThread):                                      ^
2020-04-28 04:49:32.679513 (MainThread):   compiled SQL at target/run/order_history/staging/stg_order_flash.sql
2020-04-28 04:49:32.679634 (MainThread): 
Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
2020-04-28 04:49:32.679834 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064cd950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10616c590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10616cf50>]}
2020-04-28 04:49:32.680055 (MainThread): Flushing usage events
2020-04-28 04:52:33.911322 (MainThread): Running with dbt=0.16.1
2020-04-28 04:52:34.000782 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 04:52:34.002196 (MainThread): Tracking: tracking
2020-04-28 04:52:34.009742 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8e21d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb05e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8e2150>]}
2020-04-28 04:52:34.034923 (MainThread): Partial parsing not enabled
2020-04-28 04:52:34.037486 (MainThread): Parsing macros/core.sql
2020-04-28 04:52:34.043730 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 04:52:34.053036 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 04:52:34.055345 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 04:52:34.074149 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 04:52:34.108221 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 04:52:34.130247 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 04:52:34.132381 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 04:52:34.138852 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 04:52:34.151649 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 04:52:34.158650 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 04:52:34.167724 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 04:52:34.176709 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 04:52:34.178136 (MainThread): Parsing macros/etc/query.sql
2020-04-28 04:52:34.180128 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 04:52:34.182224 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 04:52:34.184723 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 04:52:34.194554 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 04:52:34.197513 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 04:52:34.200090 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 04:52:34.253156 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 04:52:34.255162 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 04:52:34.256379 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 04:52:34.257815 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 04:52:34.260453 (MainThread): Parsing macros/catalog.sql
2020-04-28 04:52:34.263034 (MainThread): Parsing macros/relations.sql
2020-04-28 04:52:34.264572 (MainThread): Parsing macros/adapters.sql
2020-04-28 04:52:34.282232 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 04:52:34.300581 (MainThread): Partial parsing not enabled
2020-04-28 04:52:34.328867 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:52:34.328980 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.346331 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:34.346477 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.351543 (MainThread): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:34.351679 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.359088 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:34.359227 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.364247 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:52:34.364385 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.505443 (MainThread): Found 5 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 04:52:34.507901 (MainThread): 
2020-04-28 04:52:34.508239 (MainThread): Acquiring new postgres connection "master".
2020-04-28 04:52:34.508334 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:34.525831 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 04:52:34.525949 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 04:52:34.630947 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 04:52:34.631144 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 04:52:35.191965 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.56 seconds
2020-04-28 04:52:35.217844 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:52:35.217968 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 04:52:35.219545 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:52:35.219661 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 04:52:35.257785 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:35.258213 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:52:35.258487 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 04:52:35.357299 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-28 04:52:35.361248 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 04:52:35.423695 (MainThread): Using postgres connection "master".
2020-04-28 04:52:35.423841 (MainThread): On master: BEGIN
2020-04-28 04:52:35.827992 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-28 04:52:35.828422 (MainThread): Using postgres connection "master".
2020-04-28 04:52:35.828692 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 04:52:35.958204 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 04:52:36.041678 (MainThread): On master: ROLLBACK
2020-04-28 04:52:36.140868 (MainThread): Using postgres connection "master".
2020-04-28 04:52:36.141287 (MainThread): On master: BEGIN
2020-04-28 04:52:36.226232 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 04:52:36.226695 (MainThread): On master: COMMIT
2020-04-28 04:52:36.227011 (MainThread): Using postgres connection "master".
2020-04-28 04:52:36.227171 (MainThread): On master: COMMIT
2020-04-28 04:52:36.269409 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 04:52:36.270298 (MainThread): 21:52:36 | Concurrency: 1 threads (target='dev')
2020-04-28 04:52:36.270546 (MainThread): 21:52:36 | 
2020-04-28 04:52:36.273399 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 04:52:36.273671 (Thread-1): 21:52:36 | 1 of 5 START view model data_science.stg_flash....................... [RUN]
2020-04-28 04:52:36.274086 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.274222 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 04:52:36.274366 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 04:52:36.291125 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 04:52:36.291663 (Thread-1): finished collecting timing info
2020-04-28 04:52:36.333866 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.334351 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 04:52:36.412753 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 04:52:36.415507 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.415624 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:52:36.454170 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 04:52:36.457277 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 04:52:36.458923 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.459160 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 04:52:36.498427 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:36.498866 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.499131 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 04:52:36.554636 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:52:36.560723 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.560892 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 04:52:36.604214 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:36.609453 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.609606 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 04:52:36.648848 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:36.650810 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:52:36.651011 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:36.651172 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:52:37.669009 (Thread-1): SQL status: COMMIT in 1.02 seconds
2020-04-28 04:52:37.672116 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:37.672266 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:52:37.894104 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:52:37.898396 (Thread-1): finished collecting timing info
2020-04-28 04:52:37.899273 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef39810>]}
2020-04-28 04:52:37.899597 (Thread-1): 21:52:37 | 1 of 5 OK created view model data_science.stg_flash.................. [CREATE VIEW in 1.63s]
2020-04-28 04:52:37.899825 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 04:52:37.900056 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 04:52:37.900492 (Thread-1): 21:52:37 | 2 of 5 START view model data_science.stg_order....................... [RUN]
2020-04-28 04:52:37.901015 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:52:37.901178 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 04:52:37.901339 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 04:52:37.908065 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 04:52:37.908556 (Thread-1): finished collecting timing info
2020-04-28 04:52:37.916154 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:37.916288 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 04:52:38.096589 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:52:38.099148 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.099264 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:52:38.267985 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:38.271050 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 04:52:38.271726 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.271882 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 04:52:38.310640 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:38.311057 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.311316 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 04:52:38.366245 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:52:38.372476 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.372626 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 04:52:38.413450 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:38.417318 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.417505 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 04:52:38.456298 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:38.458255 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:52:38.458443 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.458596 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:52:38.627041 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:52:38.630504 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:52:38.630656 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:52:38.804075 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:38.808364 (Thread-1): finished collecting timing info
2020-04-28 04:52:38.809204 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f10fa90>]}
2020-04-28 04:52:38.809504 (Thread-1): 21:52:38 | 2 of 5 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.91s]
2020-04-28 04:52:38.809678 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 04:52:38.809904 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 04:52:38.810296 (Thread-1): 21:52:38 | 3 of 5 START view model data_science.stg_customers................... [RUN]
2020-04-28 04:52:38.810857 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:38.811056 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 04:52:38.811225 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 04:52:38.817202 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 04:52:38.818547 (Thread-1): finished collecting timing info
2020-04-28 04:52:38.826504 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:38.826622 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 04:52:38.991728 (Thread-1): SQL status: DROP VIEW in 0.16 seconds
2020-04-28 04:52:39.026027 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.026214 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:52:39.192097 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:39.194919 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 04:52:39.195510 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.195670 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 04:52:39.237375 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:39.237789 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.238074 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 04:52:39.289058 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:52:39.295264 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.295414 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 04:52:39.337137 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:39.341466 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.341622 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 04:52:39.380880 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:39.382809 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:52:39.383003 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.383157 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:52:39.552710 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:52:39.556246 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:39.556397 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:52:39.729653 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:39.733854 (Thread-1): finished collecting timing info
2020-04-28 04:52:39.734679 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef39b10>]}
2020-04-28 04:52:39.734979 (Thread-1): 21:52:39 | 3 of 5 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.92s]
2020-04-28 04:52:39.735155 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 04:52:39.735388 (Thread-1): Began running node model.order_history.stg_order_flash
2020-04-28 04:52:39.735755 (Thread-1): 21:52:39 | 4 of 5 START view model data_science.stg_order_flash................. [RUN]
2020-04-28 04:52:39.736259 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:39.736450 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 04:52:39.736605 (Thread-1): Compiling model.order_history.stg_order_flash
2020-04-28 04:52:39.746452 (Thread-1): Writing injected SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:52:39.746935 (Thread-1): finished collecting timing info
2020-04-28 04:52:39.754560 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:39.754729 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" cascade
2020-04-28 04:52:39.923617 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:39.927737 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:39.927889 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:52:40.104898 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:52:40.109137 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:52:40.109710 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.109867 (Thread-1): On model.order_history.stg_order_flash: BEGIN
2020-04-28 04:52:40.148331 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:40.148751 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.149013 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */

  create view "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 04:52:40.203315 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:52:40.207314 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.207473 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
alter table "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" rename to "stg_order_flash"
2020-04-28 04:52:40.246314 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:52:40.248271 (Thread-1): On model.order_history.stg_order_flash: COMMIT
2020-04-28 04:52:40.248465 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.248620 (Thread-1): On model.order_history.stg_order_flash: COMMIT
2020-04-28 04:52:40.420719 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 04:52:40.424220 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:40.424365 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:52:40.638822 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 04:52:40.643007 (Thread-1): finished collecting timing info
2020-04-28 04:52:40.643833 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecf5310>]}
2020-04-28 04:52:40.644135 (Thread-1): 21:52:40 | 4 of 5 OK created view model data_science.stg_order_flash............ [CREATE VIEW in 0.91s]
2020-04-28 04:52:40.644311 (Thread-1): Finished running node model.order_history.stg_order_flash
2020-04-28 04:52:40.644783 (Thread-1): Began running node model.order_history.customers
2020-04-28 04:52:40.645044 (Thread-1): 21:52:40 | 5 of 5 START view model data_science.customers....................... [RUN]
2020-04-28 04:52:40.645395 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:52:40.645522 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order_flash).
2020-04-28 04:52:40.645648 (Thread-1): Compiling model.order_history.customers
2020-04-28 04:52:40.654764 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 04:52:40.656260 (Thread-1): finished collecting timing info
2020-04-28 04:52:40.663575 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:52:40.663709 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 04:52:40.839769 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:52:40.843876 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:52:40.844025 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 04:52:41.011896 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 04:52:41.014256 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 04:52:41.016935 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:52:41.017212 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 04:52:41.055787 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:41.056207 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:52:41.056470 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."stg_order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,
        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN amount_gross ELSE NULL END) AS total_revenue
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 04:52:41.098903 (Thread-1): Postgres error: syntax error at or near "COUNT"
LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
                 ^

2020-04-28 04:52:41.099319 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 04:52:41.138134 (Thread-1): finished collecting timing info
2020-04-28 04:52:41.139171 (Thread-1): Database Error in model customers (models/customers.sql)
  syntax error at or near "COUNT"
  LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
                   ^
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "COUNT"
LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
                 ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  syntax error at or near "COUNT"
  LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
                   ^
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 04:52:41.157095 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90c1e3ee-b9e7-4fe1-aacf-60c23c0ebe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecf5310>]}
2020-04-28 04:52:41.157470 (Thread-1): 21:52:41 | 5 of 5 ERROR creating view model data_science.customers.............. [ERROR in 0.51s]
2020-04-28 04:52:41.157660 (Thread-1): Finished running node model.order_history.customers
2020-04-28 04:52:41.198759 (MainThread): Using postgres connection "master".
2020-04-28 04:52:41.198942 (MainThread): On master: BEGIN
2020-04-28 04:52:41.242139 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:52:41.242465 (MainThread): On master: COMMIT
2020-04-28 04:52:41.242590 (MainThread): Using postgres connection "master".
2020-04-28 04:52:41.242694 (MainThread): On master: COMMIT
2020-04-28 04:52:41.284665 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 04:52:41.285185 (MainThread): 21:52:41 | 
2020-04-28 04:52:41.285383 (MainThread): 21:52:41 | Finished running 5 view models in 6.78s.
2020-04-28 04:52:41.285586 (MainThread): Connection 'master' was left open.
2020-04-28 04:52:41.285748 (MainThread): On master: Close
2020-04-28 04:52:41.286213 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 04:52:41.286414 (MainThread): On model.order_history.customers: Close
2020-04-28 04:52:41.309701 (MainThread): 
2020-04-28 04:52:41.309906 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 04:52:41.310042 (MainThread): 
2020-04-28 04:52:41.310170 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 04:52:41.310322 (MainThread):   syntax error at or near "COUNT"
2020-04-28 04:52:41.310437 (MainThread):   LINE 31:         COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRE...
2020-04-28 04:52:41.310534 (MainThread):                    ^
2020-04-28 04:52:41.310626 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 04:52:41.310728 (MainThread): 
Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
2020-04-28 04:52:41.310901 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef32990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1b0090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee42210>]}
2020-04-28 04:52:41.311097 (MainThread): Flushing usage events
2020-04-28 04:52:58.972352 (MainThread): Running with dbt=0.16.1
2020-04-28 04:52:59.065419 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 04:52:59.066837 (MainThread): Tracking: tracking
2020-04-28 04:52:59.075322 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd2c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd2e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd2750>]}
2020-04-28 04:52:59.101546 (MainThread): Partial parsing not enabled
2020-04-28 04:52:59.105361 (MainThread): Parsing macros/core.sql
2020-04-28 04:52:59.111831 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 04:52:59.121711 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 04:52:59.124463 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 04:52:59.144980 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 04:52:59.178629 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 04:52:59.199937 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 04:52:59.202259 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 04:52:59.211352 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 04:52:59.225802 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 04:52:59.232997 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 04:52:59.240156 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 04:52:59.245558 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 04:52:59.246704 (MainThread): Parsing macros/etc/query.sql
2020-04-28 04:52:59.247992 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 04:52:59.249814 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 04:52:59.252329 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 04:52:59.261845 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 04:52:59.264038 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 04:52:59.265628 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 04:52:59.313512 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 04:52:59.316781 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 04:52:59.318934 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 04:52:59.321042 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 04:52:59.324360 (MainThread): Parsing macros/catalog.sql
2020-04-28 04:52:59.327605 (MainThread): Parsing macros/relations.sql
2020-04-28 04:52:59.329471 (MainThread): Parsing macros/adapters.sql
2020-04-28 04:52:59.348583 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 04:52:59.366782 (MainThread): Partial parsing not enabled
2020-04-28 04:52:59.394340 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:52:59.394449 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.411688 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:52:59.411841 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.416957 (MainThread): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:52:59.417094 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.424432 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:52:59.424570 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.429501 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:52:59.429638 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.560226 (MainThread): Found 5 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 04:52:59.563130 (MainThread): 
2020-04-28 04:52:59.563622 (MainThread): Acquiring new postgres connection "master".
2020-04-28 04:52:59.563710 (MainThread): Opening a new connection, currently in state init
2020-04-28 04:52:59.580505 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 04:52:59.580643 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 04:52:59.686870 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 04:52:59.687044 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 04:53:00.139681 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.45 seconds
2020-04-28 04:53:00.169506 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:53:00.169731 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 04:53:00.171554 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:53:00.171680 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 04:53:00.209236 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:00.209658 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 04:53:00.209920 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 04:53:00.312639 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.10 seconds
2020-04-28 04:53:00.318110 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 04:53:00.381476 (MainThread): Using postgres connection "master".
2020-04-28 04:53:00.381628 (MainThread): On master: BEGIN
2020-04-28 04:53:00.751125 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-28 04:53:00.751578 (MainThread): Using postgres connection "master".
2020-04-28 04:53:00.751843 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 04:53:00.904200 (MainThread): SQL status: SELECT in 0.15 seconds
2020-04-28 04:53:00.988116 (MainThread): On master: ROLLBACK
2020-04-28 04:53:01.028650 (MainThread): Using postgres connection "master".
2020-04-28 04:53:01.028884 (MainThread): On master: BEGIN
2020-04-28 04:53:01.116355 (MainThread): SQL status: BEGIN in 0.09 seconds
2020-04-28 04:53:01.116816 (MainThread): On master: COMMIT
2020-04-28 04:53:01.117116 (MainThread): Using postgres connection "master".
2020-04-28 04:53:01.117275 (MainThread): On master: COMMIT
2020-04-28 04:53:01.157906 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 04:53:01.158769 (MainThread): 21:53:01 | Concurrency: 1 threads (target='dev')
2020-04-28 04:53:01.159036 (MainThread): 21:53:01 | 
2020-04-28 04:53:01.162095 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 04:53:01.162359 (Thread-1): 21:53:01 | 1 of 5 START view model data_science.stg_flash....................... [RUN]
2020-04-28 04:53:01.162779 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.162922 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 04:53:01.163066 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 04:53:01.179860 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 04:53:01.180414 (Thread-1): finished collecting timing info
2020-04-28 04:53:01.222066 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.222248 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 04:53:01.297992 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 04:53:01.301373 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.301520 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:53:01.339376 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 04:53:01.341172 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 04:53:01.341637 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.341746 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 04:53:01.379517 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:01.379947 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.380218 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 04:53:01.436083 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:53:01.442385 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.442542 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 04:53:01.506304 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 04:53:01.511493 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.511659 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 04:53:01.553503 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:01.555445 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:53:01.555644 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.555808 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 04:53:01.766861 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-28 04:53:01.770347 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 04:53:01.770515 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 04:53:02.079153 (Thread-1): SQL status: DROP VIEW in 0.31 seconds
2020-04-28 04:53:02.083469 (Thread-1): finished collecting timing info
2020-04-28 04:53:02.084334 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c291550>]}
2020-04-28 04:53:02.084661 (Thread-1): 21:53:02 | 1 of 5 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.92s]
2020-04-28 04:53:02.084850 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 04:53:02.085039 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 04:53:02.085220 (Thread-1): 21:53:02 | 2 of 5 START view model data_science.stg_order....................... [RUN]
2020-04-28 04:53:02.085558 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.085689 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 04:53:02.085817 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 04:53:02.091868 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 04:53:02.092370 (Thread-1): finished collecting timing info
2020-04-28 04:53:02.099725 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.099852 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 04:53:02.345858 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2020-04-28 04:53:02.348759 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.348903 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:53:02.571075 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:53:02.574123 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 04:53:02.574711 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.574868 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 04:53:02.612961 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:02.613250 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.613442 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is False
  );

2020-04-28 04:53:02.668116 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:53:02.673038 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.673180 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 04:53:02.713860 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:02.716563 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.716680 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 04:53:02.755724 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:02.757702 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:53:02.757898 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.758057 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 04:53:02.989558 (Thread-1): SQL status: COMMIT in 0.23 seconds
2020-04-28 04:53:02.993078 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 04:53:02.993238 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 04:53:03.216661 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:53:03.220967 (Thread-1): finished collecting timing info
2020-04-28 04:53:03.221817 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c638d90>]}
2020-04-28 04:53:03.222124 (Thread-1): 21:53:03 | 2 of 5 OK created view model data_science.stg_order.................. [CREATE VIEW in 1.14s]
2020-04-28 04:53:03.222306 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 04:53:03.222490 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 04:53:03.222670 (Thread-1): 21:53:03 | 3 of 5 START view model data_science.stg_customers................... [RUN]
2020-04-28 04:53:03.223336 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.223590 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 04:53:03.223925 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 04:53:03.230346 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 04:53:03.230775 (Thread-1): finished collecting timing info
2020-04-28 04:53:03.239462 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.239592 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 04:53:03.421144 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:53:03.452266 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.452451 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:53:03.630486 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 04:53:03.633313 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 04:53:03.633870 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.634030 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 04:53:03.676491 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:03.676952 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.677131 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 04:53:03.728750 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:53:03.734650 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.734844 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 04:53:03.773809 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:03.778080 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.778235 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 04:53:03.819047 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:03.820976 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:53:03.821184 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:03.821346 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 04:53:04.040723 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 04:53:04.043607 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 04:53:04.043763 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 04:53:04.248911 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 04:53:04.253201 (Thread-1): finished collecting timing info
2020-04-28 04:53:04.254072 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdb4310>]}
2020-04-28 04:53:04.254393 (Thread-1): 21:53:04 | 3 of 5 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.03s]
2020-04-28 04:53:04.254585 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 04:53:04.254800 (Thread-1): Began running node model.order_history.stg_order_flash
2020-04-28 04:53:04.255198 (Thread-1): 21:53:04 | 4 of 5 START view model data_science.stg_order_flash................. [RUN]
2020-04-28 04:53:04.255662 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.255848 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 04:53:04.255998 (Thread-1): Compiling model.order_history.stg_order_flash
2020-04-28 04:53:04.264605 (Thread-1): Writing injected SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:53:04.265037 (Thread-1): finished collecting timing info
2020-04-28 04:53:04.271710 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.271823 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" cascade
2020-04-28 04:53:04.472203 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 04:53:04.476331 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.476489 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:53:04.668235 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 04:53:04.672443 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order_flash"
2020-04-28 04:53:04.672988 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.673143 (Thread-1): On model.order_history.stg_order_flash: BEGIN
2020-04-28 04:53:04.711434 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:04.711871 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.712155 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */

  create view "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 04:53:04.767286 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 04:53:04.770653 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.770829 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
alter table "data_platform_prod"."data_science"."stg_order_flash__dbt_tmp" rename to "stg_order_flash"
2020-04-28 04:53:04.809386 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:04.810558 (Thread-1): On model.order_history.stg_order_flash: COMMIT
2020-04-28 04:53:04.810674 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:04.810769 (Thread-1): On model.order_history.stg_order_flash: COMMIT
2020-04-28 04:53:05.009004 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 04:53:05.012613 (Thread-1): Using postgres connection "model.order_history.stg_order_flash".
2020-04-28 04:53:05.012768 (Thread-1): On model.order_history.stg_order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_flash__dbt_backup" cascade
2020-04-28 04:53:06.344925 (Thread-1): SQL status: DROP VIEW in 1.33 seconds
2020-04-28 04:53:06.348854 (Thread-1): finished collecting timing info
2020-04-28 04:53:06.349723 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2fa490>]}
2020-04-28 04:53:06.350033 (Thread-1): 21:53:06 | 4 of 5 OK created view model data_science.stg_order_flash............ [CREATE VIEW in 2.09s]
2020-04-28 04:53:06.350215 (Thread-1): Finished running node model.order_history.stg_order_flash
2020-04-28 04:53:06.350790 (Thread-1): Began running node model.order_history.customers
2020-04-28 04:53:06.351026 (Thread-1): 21:53:06 | 5 of 5 START view model data_science.customers....................... [RUN]
2020-04-28 04:53:06.351514 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 04:53:06.351678 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order_flash).
2020-04-28 04:53:06.351802 (Thread-1): Compiling model.order_history.customers
2020-04-28 04:53:06.360950 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 04:53:06.361375 (Thread-1): finished collecting timing info
2020-04-28 04:53:06.368754 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.368886 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 04:53:06.597098 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 04:53:06.601213 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.601366 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 04:53:06.821377 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 04:53:06.823883 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 04:53:06.824446 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.824609 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 04:53:06.887804 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-04-28 04:53:06.888259 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.888445 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."stg_order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        -- COUNT(DISTINCT CASE WHEN (NOT COALESCE(is_canceled , FALSE)) AND 
        -- (NOT COALESCE(pricing_mode_id = 1 , FALSE)) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_ticket_unique_id ELSE NULL END) AS number_of_tickets_sold,
        -- COUNT(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN order_unique_id ELSE NULL END) AS number_of_orders,
        -- SUM(DISTINCT CASE WHEN NOT COALESCE(is_canceled , FALSE) 
        -- THEN amount_gross ELSE NULL END) AS total_revenue
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 04:53:06.945467 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 04:53:06.949683 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.949836 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 04:53:06.988784 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 04:53:06.990719 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 04:53:06.990915 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:06.991075 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 04:53:07.166872 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 04:53:07.169780 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 04:53:07.169934 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 04:53:07.374796 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 04:53:07.379084 (Thread-1): finished collecting timing info
2020-04-28 04:53:07.379938 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3165da58-9284-479f-8510-f1a484a382ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c303fd0>]}
2020-04-28 04:53:07.380248 (Thread-1): 21:53:07 | 5 of 5 OK created view model data_science.customers.................. [CREATE VIEW in 1.03s]
2020-04-28 04:53:07.380429 (Thread-1): Finished running node model.order_history.customers
2020-04-28 04:53:07.410613 (MainThread): Using postgres connection "master".
2020-04-28 04:53:07.410806 (MainThread): On master: BEGIN
2020-04-28 04:53:07.451821 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 04:53:07.452274 (MainThread): On master: COMMIT
2020-04-28 04:53:07.452451 (MainThread): Using postgres connection "master".
2020-04-28 04:53:07.452609 (MainThread): On master: COMMIT
2020-04-28 04:53:07.492253 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 04:53:07.492745 (MainThread): 21:53:07 | 
2020-04-28 04:53:07.492901 (MainThread): 21:53:07 | Finished running 5 view models in 7.93s.
2020-04-28 04:53:07.493033 (MainThread): Connection 'master' was left open.
2020-04-28 04:53:07.493172 (MainThread): On master: Close
2020-04-28 04:53:07.493443 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 04:53:07.493584 (MainThread): On model.order_history.customers: Close
2020-04-28 04:53:07.516331 (MainThread): 
2020-04-28 04:53:07.516604 (MainThread): Completed successfully
2020-04-28 04:53:07.516818 (MainThread): 
Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
2020-04-28 04:53:07.517109 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c21d590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c464c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2df150>]}
2020-04-28 04:53:07.517421 (MainThread): Flushing usage events
2020-04-28 19:37:21.417876 (MainThread): Running with dbt=0.16.1
2020-04-28 19:37:21.544262 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:37:21.545574 (MainThread): Tracking: tracking
2020-04-28 19:37:21.555735 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ef9090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111253f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ef9bd0>]}
2020-04-28 19:37:21.582147 (MainThread): Partial parsing not enabled
2020-04-28 19:37:21.586210 (MainThread): Parsing macros/core.sql
2020-04-28 19:37:21.594076 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:37:21.608354 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:37:21.611817 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:37:21.631541 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:37:21.667295 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:37:21.689838 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:37:21.692636 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:37:21.700004 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:37:21.715602 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:37:21.723931 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:37:21.731296 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:37:21.736848 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:37:21.738643 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:37:21.740473 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:37:21.742857 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:37:21.745748 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:37:21.755349 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:37:21.758054 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:37:21.760994 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:37:21.805341 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:37:21.807574 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:37:21.809181 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:37:21.810964 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:37:21.813913 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:37:21.816993 (MainThread): Parsing macros/relations.sql
2020-04-28 19:37:21.819280 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:37:21.836374 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:37:21.853867 (MainThread): Partial parsing not enabled
2020-04-28 19:37:21.883536 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:37:21.883667 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.901322 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:21.901455 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.907118 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:21.907218 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.913219 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:37:21.913354 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.918227 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:37:21.918347 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.923765 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:21.923892 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.930937 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:37:21.931076 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:21.980811 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113eb250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114bd950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114bdb90>]}
2020-04-28 19:37:21.981030 (MainThread): Flushing usage events
2020-04-28 19:37:22.314756 (MainThread): Connection 'model.order_history.order_flash' was properly closed.
2020-04-28 19:37:22.315005 (MainThread): Encountered an error:
2020-04-28 19:37:22.315203 (MainThread): Compilation Error in model customers (models/customers.sql)
  Model 'model.order_history.customers' depends on a node named 'stg_order_flash' which was not found or is disabled
2020-04-28 19:37:22.327604 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 302, in process_manifest
    process_refs(manifest, project_name)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 553, in process_refs
    _process_refs_for_node(manifest, current_project, node)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 534, in _process_refs_for_node
    disabled=(isinstance(target_model, Disabled))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/utils.py", line 338, in invalid_ref_fail_unless_test
    target_model_package)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 488, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model customers (models/customers.sql)
  Model 'model.order_history.customers' depends on a node named 'stg_order_flash' which was not found or is disabled

2020-04-28 19:37:48.064891 (MainThread): Running with dbt=0.16.1
2020-04-28 19:37:48.131069 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:37:48.132060 (MainThread): Tracking: tracking
2020-04-28 19:37:48.139422 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cbbf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f2e9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f12a90>]}
2020-04-28 19:37:48.170468 (MainThread): Partial parsing not enabled
2020-04-28 19:37:48.172663 (MainThread): Parsing macros/core.sql
2020-04-28 19:37:48.178258 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:37:48.187927 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:37:48.189905 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:37:48.208092 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:37:48.242097 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:37:48.263975 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:37:48.265963 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:37:48.272503 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:37:48.285730 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:37:48.293001 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:37:48.299538 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:37:48.304785 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:37:48.305785 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:37:48.306908 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:37:48.308643 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:37:48.310871 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:37:48.320186 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:37:48.322272 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:37:48.323383 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:37:48.364673 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:37:48.365885 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:37:48.366809 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:37:48.367901 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:37:48.370572 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:37:48.373590 (MainThread): Parsing macros/relations.sql
2020-04-28 19:37:48.375091 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:37:48.392954 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:37:48.411335 (MainThread): Partial parsing not enabled
2020-04-28 19:37:48.445772 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:37:48.445901 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.462443 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:48.462549 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.466572 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:48.466659 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.471037 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:37:48.471124 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.475052 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:37:48.475139 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.479748 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:48.479841 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.484779 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:37:48.484866 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.625248 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:37:48.628916 (MainThread): 
2020-04-28 19:37:48.629278 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:37:48.629374 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:37:48.652326 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:37:48.652477 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:37:48.755029 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:37:48.755247 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:37:49.300749 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.55 seconds
2020-04-28 19:37:49.339804 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:37:49.340026 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:37:49.341715 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:37:49.341845 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:37:49.379145 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:49.379566 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:37:49.379827 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:37:49.488748 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 19:37:49.496496 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:37:49.572624 (MainThread): Using postgres connection "master".
2020-04-28 19:37:49.572780 (MainThread): On master: BEGIN
2020-04-28 19:37:49.915104 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-04-28 19:37:49.915525 (MainThread): Using postgres connection "master".
2020-04-28 19:37:49.915791 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:37:50.055429 (MainThread): SQL status: SELECT in 0.14 seconds
2020-04-28 19:37:50.131172 (MainThread): On master: ROLLBACK
2020-04-28 19:37:50.168890 (MainThread): Using postgres connection "master".
2020-04-28 19:37:50.169066 (MainThread): On master: BEGIN
2020-04-28 19:37:50.243867 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 19:37:50.244313 (MainThread): On master: COMMIT
2020-04-28 19:37:50.244596 (MainThread): Using postgres connection "master".
2020-04-28 19:37:50.244772 (MainThread): On master: COMMIT
2020-04-28 19:37:50.282101 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:37:50.282981 (MainThread): 12:37:50 | Concurrency: 1 threads (target='dev')
2020-04-28 19:37:50.283232 (MainThread): 12:37:50 | 
2020-04-28 19:37:50.287180 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:37:50.287404 (Thread-1): 12:37:50 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:37:50.287742 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.287860 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:37:50.287982 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:37:50.303902 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:37:50.304427 (Thread-1): finished collecting timing info
2020-04-28 19:37:50.343726 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.343927 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:37:50.419723 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:37:50.424707 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.424861 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:37:50.462416 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:37:50.464639 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:37:50.465314 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.465473 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:37:50.501734 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:50.501962 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.502090 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:37:50.549080 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:37:50.555507 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.555663 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:37:50.603720 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:37:50.607947 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.608113 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:37:50.646565 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:50.648518 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:37:50.648715 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:50.648876 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:37:52.730335 (Thread-1): SQL status: COMMIT in 2.08 seconds
2020-04-28 19:37:52.733776 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:37:52.733937 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:37:53.750218 (Thread-1): SQL status: DROP VIEW in 1.02 seconds
2020-04-28 19:37:53.754486 (Thread-1): finished collecting timing info
2020-04-28 19:37:53.755343 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091c5e10>]}
2020-04-28 19:37:53.755663 (Thread-1): 12:37:53 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 3.47s]
2020-04-28 19:37:53.755854 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:37:53.756038 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:37:53.756227 (Thread-1): 12:37:53 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:37:53.756965 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:53.757213 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:37:53.757383 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:37:53.763804 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:37:53.765344 (Thread-1): finished collecting timing info
2020-04-28 19:37:53.772903 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:53.773037 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:37:53.943414 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:53.947617 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:53.947769 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:37:54.143988 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:37:54.146392 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:37:54.147727 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.147891 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:37:54.184810 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:54.185092 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.185266 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:37:54.246775 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:37:54.253046 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.253200 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:37:54.294728 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:54.298495 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.298645 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:37:54.336588 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:54.338605 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:37:54.338810 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.338984 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:37:54.517687 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:37:54.521068 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:37:54.521218 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:37:54.715033 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:37:54.719253 (Thread-1): finished collecting timing info
2020-04-28 19:37:54.720090 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090bf090>]}
2020-04-28 19:37:54.720392 (Thread-1): 12:37:54 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.96s]
2020-04-28 19:37:54.720571 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:37:54.720754 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:37:54.720934 (Thread-1): 12:37:54 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:37:54.721271 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:37:54.721401 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:37:54.721531 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:37:54.758789 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:37:54.759313 (Thread-1): finished collecting timing info
2020-04-28 19:37:54.767219 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:54.767373 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:37:54.937783 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:54.941961 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:54.942113 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:37:55.109742 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:55.112712 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:37:55.113392 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.113547 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:37:55.180278 (Thread-1): SQL status: BEGIN in 0.07 seconds
2020-04-28 19:37:55.180479 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.180591 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:37:55.229302 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:37:55.233292 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.233407 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:37:55.273043 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:55.277312 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.277467 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:37:55.314890 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:55.316892 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:37:55.317086 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.317243 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:37:55.488586 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:37:55.491943 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:37:55.492100 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:37:55.679063 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:37:55.683254 (Thread-1): finished collecting timing info
2020-04-28 19:37:55.684100 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f7f510>]}
2020-04-28 19:37:55.684405 (Thread-1): 12:37:55 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.96s]
2020-04-28 19:37:55.684585 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:37:55.684772 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:37:55.685128 (Thread-1): 12:37:55 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:37:55.686012 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:37:55.686180 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:37:55.686316 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:37:55.693641 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:37:55.694081 (Thread-1): finished collecting timing info
2020-04-28 19:37:55.701298 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:55.701424 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:37:55.895729 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:37:55.898251 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:55.898367 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:37:56.939860 (Thread-1): SQL status: DROP VIEW in 1.04 seconds
2020-04-28 19:37:56.942940 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:37:56.943572 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:56.943726 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:37:56.983548 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:56.983976 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:56.984241 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:37:57.039341 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:37:57.043552 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:57.043704 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:37:57.082373 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:37:57.084302 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:37:57.084499 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:57.084658 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:37:57.272887 (Thread-1): SQL status: COMMIT in 0.19 seconds
2020-04-28 19:37:57.274956 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:37:57.275080 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:37:57.449524 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:57.453924 (Thread-1): finished collecting timing info
2020-04-28 19:37:57.454759 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090fad10>]}
2020-04-28 19:37:57.455064 (Thread-1): 12:37:57 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 1.77s]
2020-04-28 19:37:57.455243 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:37:57.455427 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:37:57.455828 (Thread-1): 12:37:57 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:37:57.456361 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.456533 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:37:57.456655 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:37:57.464558 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:37:57.465128 (Thread-1): finished collecting timing info
2020-04-28 19:37:57.472685 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.472815 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:37:57.707795 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 19:37:57.711952 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.712107 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:37:57.880885 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:37:57.884225 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:37:57.884879 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.885042 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:37:57.922378 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:57.922660 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:37:57.922842 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
)

brokers as (
    SELECT email
    FROM analytics.yield_manager_partners
)

final as (
    SELECT *
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.email
)
  );

2020-04-28 19:37:57.961444 (Thread-1): Postgres error: syntax error at or near "brokers"
LINE 8: brokers as (
        ^

2020-04-28 19:37:57.961872 (Thread-1): On model.order_history.customer_broker: ROLLBACK
2020-04-28 19:37:57.999453 (Thread-1): finished collecting timing info
2020-04-28 19:37:58.000240 (Thread-1): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  syntax error at or near "brokers"
  LINE 8: brokers as (
          ^
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "brokers"
LINE 8: brokers as (
        ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  syntax error at or near "brokers"
  LINE 8: brokers as (
          ^
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:37:58.022988 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f67f10>]}
2020-04-28 19:37:58.023262 (Thread-1): 12:37:58 | 5 of 7 ERROR creating view model data_science.customer_broker........ [ERROR in 0.57s]
2020-04-28 19:37:58.023419 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:37:58.023579 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:37:58.023733 (Thread-1): 12:37:58 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:37:58.024180 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.024294 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:37:58.024404 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:37:58.033232 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:37:58.033613 (Thread-1): finished collecting timing info
2020-04-28 19:37:58.040468 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.040581 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:37:58.138471 (Thread-1): SQL status: DROP VIEW in 0.10 seconds
2020-04-28 19:37:58.143583 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.143770 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:37:58.181895 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:37:58.184941 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:37:58.185560 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.185716 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:37:58.223480 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:37:58.223719 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.223840 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:37:58.270615 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:37:58.273963 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.274099 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:37:58.323273 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:37:58.325186 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:37:58.325382 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.325541 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:37:58.603608 (Thread-1): SQL status: COMMIT in 0.28 seconds
2020-04-28 19:37:58.606357 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:37:58.606514 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:37:58.794952 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:37:58.799248 (Thread-1): finished collecting timing info
2020-04-28 19:37:58.800085 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f58610>]}
2020-04-28 19:37:58.800390 (Thread-1): 12:37:58 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.78s]
2020-04-28 19:37:58.800569 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:37:58.800997 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:37:58.801198 (Thread-1): 12:37:58 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:37:58.801653 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:37:58.801849 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:37:58.801984 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:37:58.811451 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:37:58.811875 (Thread-1): finished collecting timing info
2020-04-28 19:37:58.819460 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:37:58.819598 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:37:59.001854 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:37:59.007472 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:37:59.007629 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:38:00.019858 (Thread-1): SQL status: DROP VIEW in 1.01 seconds
2020-04-28 19:38:00.022449 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:38:00.023416 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.023647 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:38:00.205313 (Thread-1): SQL status: BEGIN in 0.18 seconds
2020-04-28 19:38:00.205753 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.205949 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:38:00.298640 (Thread-1): SQL status: CREATE VIEW in 0.09 seconds
2020-04-28 19:38:00.302358 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.302502 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:38:00.500005 (Thread-1): SQL status: ALTER TABLE in 0.20 seconds
2020-04-28 19:38:00.501832 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:38:00.502035 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.502196 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:38:00.798681 (Thread-1): SQL status: COMMIT in 0.30 seconds
2020-04-28 19:38:00.801691 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:00.801857 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:38:01.339641 (Thread-1): SQL status: DROP VIEW in 0.54 seconds
2020-04-28 19:38:01.343869 (Thread-1): finished collecting timing info
2020-04-28 19:38:01.344714 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32aeeb74-6420-4f71-b469-b5b9c0c20415', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f62fd0>]}
2020-04-28 19:38:01.345016 (Thread-1): 12:38:01 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 2.54s]
2020-04-28 19:38:01.345196 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:38:01.364462 (MainThread): Using postgres connection "master".
2020-04-28 19:38:01.364732 (MainThread): On master: BEGIN
2020-04-28 19:38:01.499170 (MainThread): SQL status: BEGIN in 0.13 seconds
2020-04-28 19:38:01.499625 (MainThread): On master: COMMIT
2020-04-28 19:38:01.499912 (MainThread): Using postgres connection "master".
2020-04-28 19:38:01.500065 (MainThread): On master: COMMIT
2020-04-28 19:38:01.536841 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:38:01.537760 (MainThread): 12:38:01 | 
2020-04-28 19:38:01.537998 (MainThread): 12:38:01 | Finished running 7 view models in 12.91s.
2020-04-28 19:38:01.538191 (MainThread): Connection 'master' was left open.
2020-04-28 19:38:01.538347 (MainThread): On master: Close
2020-04-28 19:38:01.538749 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:38:01.538914 (MainThread): On model.order_history.customers: Close
2020-04-28 19:38:01.560247 (MainThread): 
2020-04-28 19:38:01.560453 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 19:38:01.560593 (MainThread): 
2020-04-28 19:38:01.560727 (MainThread): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
2020-04-28 19:38:01.560849 (MainThread):   syntax error at or near "brokers"
2020-04-28 19:38:01.560960 (MainThread):   LINE 8: brokers as (
2020-04-28 19:38:01.561067 (MainThread):           ^
2020-04-28 19:38:01.561172 (MainThread):   compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:38:01.561291 (MainThread): 
Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
2020-04-28 19:38:01.561482 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095979d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109191910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090e5310>]}
2020-04-28 19:38:01.561694 (MainThread): Flushing usage events
2020-04-28 19:38:25.552664 (MainThread): Running with dbt=0.16.1
2020-04-28 19:38:25.626596 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:38:25.627674 (MainThread): Tracking: tracking
2020-04-28 19:38:25.635348 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036f0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10371d0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036f0d50>]}
2020-04-28 19:38:25.658152 (MainThread): Partial parsing not enabled
2020-04-28 19:38:25.660342 (MainThread): Parsing macros/core.sql
2020-04-28 19:38:25.665864 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:38:25.675480 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:38:25.677399 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:38:25.696493 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:38:25.730070 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:38:25.751118 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:38:25.753009 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:38:25.759336 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:38:25.772100 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:38:25.779571 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:38:25.786445 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:38:25.791824 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:38:25.792777 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:38:25.793855 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:38:25.795533 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:38:25.797605 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:38:25.806829 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:38:25.808826 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:38:25.809891 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:38:25.850976 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:38:25.852140 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:38:25.853073 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:38:25.854189 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:38:25.856613 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:38:25.858948 (MainThread): Parsing macros/relations.sql
2020-04-28 19:38:25.860307 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:38:25.878045 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:38:25.902342 (MainThread): Partial parsing not enabled
2020-04-28 19:38:25.930309 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:38:25.930427 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.946607 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:25.946702 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.950629 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:25.950718 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.955026 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:38:25.955130 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.959019 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:38:25.959105 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.963557 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:25.963652 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:25.968900 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:38:25.968988 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:26.103738 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:38:26.108010 (MainThread): 
2020-04-28 19:38:26.108307 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:38:26.108393 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:38:26.129488 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:38:26.129630 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:38:26.226808 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:38:26.226941 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:38:26.642594 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.42 seconds
2020-04-28 19:38:26.678739 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:38:26.678941 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:38:26.680559 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:38:26.680672 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:38:26.719193 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:26.719601 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:38:26.719853 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:38:26.813048 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-28 19:38:26.821480 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:38:26.896265 (MainThread): Using postgres connection "master".
2020-04-28 19:38:26.896477 (MainThread): On master: BEGIN
2020-04-28 19:38:27.242822 (MainThread): SQL status: BEGIN in 0.35 seconds
2020-04-28 19:38:27.243143 (MainThread): Using postgres connection "master".
2020-04-28 19:38:27.243244 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:38:27.371749 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 19:38:27.448396 (MainThread): On master: ROLLBACK
2020-04-28 19:38:27.486283 (MainThread): Using postgres connection "master".
2020-04-28 19:38:27.486673 (MainThread): On master: BEGIN
2020-04-28 19:38:27.561808 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 19:38:27.562096 (MainThread): On master: COMMIT
2020-04-28 19:38:27.562257 (MainThread): Using postgres connection "master".
2020-04-28 19:38:27.562401 (MainThread): On master: COMMIT
2020-04-28 19:38:27.600496 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:38:27.601374 (MainThread): 12:38:27 | Concurrency: 1 threads (target='dev')
2020-04-28 19:38:27.601621 (MainThread): 12:38:27 | 
2020-04-28 19:38:27.604254 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:38:27.604526 (Thread-1): 12:38:27 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:38:27.604910 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.605049 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:38:27.605187 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:38:27.620301 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:38:27.620712 (Thread-1): finished collecting timing info
2020-04-28 19:38:27.658655 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.658844 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:38:27.736305 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:38:27.741308 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.741465 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:38:27.782009 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:38:27.784910 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:38:27.785557 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.785715 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:38:27.824186 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:27.824601 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.824858 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:38:27.879076 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:38:27.883797 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.883944 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:38:27.922606 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:27.925249 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.925364 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:38:27.967178 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:27.969102 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:38:27.969292 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:27.969445 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:38:28.892788 (Thread-1): SQL status: COMMIT in 0.92 seconds
2020-04-28 19:38:28.895094 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:38:28.895239 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:38:29.120029 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 19:38:29.124306 (Thread-1): finished collecting timing info
2020-04-28 19:38:29.125154 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d34e50>]}
2020-04-28 19:38:29.125463 (Thread-1): 12:38:29 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.52s]
2020-04-28 19:38:29.125644 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:38:29.125877 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:38:29.126151 (Thread-1): 12:38:29 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:38:29.126987 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.127114 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:38:29.127228 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:38:29.133481 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:38:29.133942 (Thread-1): finished collecting timing info
2020-04-28 19:38:29.141547 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.141674 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:38:29.327352 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:38:29.331544 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.331693 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:38:29.803068 (Thread-1): SQL status: DROP VIEW in 0.47 seconds
2020-04-28 19:38:29.806026 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:38:29.806602 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.806759 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:38:29.845721 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:29.846127 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.846376 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:38:29.900239 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:38:29.905615 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.905752 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:38:29.944597 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:29.947482 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.947619 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:38:29.986916 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:29.988873 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:38:29.989066 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:29.989221 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:38:30.161669 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:38:30.166353 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:38:30.166502 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:38:30.351274 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:38:30.355603 (Thread-1): finished collecting timing info
2020-04-28 19:38:30.356448 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa7350>]}
2020-04-28 19:38:30.356751 (Thread-1): 12:38:30 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 1.23s]
2020-04-28 19:38:30.356927 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:38:30.357152 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:38:30.357514 (Thread-1): 12:38:30 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:38:30.357959 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:38:30.358144 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:38:30.358329 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:38:30.396213 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:38:30.396732 (Thread-1): finished collecting timing info
2020-04-28 19:38:30.404287 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:30.404425 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:38:32.331892 (Thread-1): SQL status: DROP VIEW in 1.93 seconds
2020-04-28 19:38:32.334377 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:32.334489 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:38:33.266299 (Thread-1): SQL status: DROP VIEW in 0.93 seconds
2020-04-28 19:38:33.269271 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:38:33.269909 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.270067 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:38:33.309198 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:33.309402 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.309516 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:38:33.367132 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:38:33.373270 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.373428 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:38:33.435343 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 19:38:33.439534 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.439702 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:38:33.478284 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:33.479618 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:38:33.479747 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.479855 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:38:33.934716 (Thread-1): SQL status: COMMIT in 0.45 seconds
2020-04-28 19:38:33.938072 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:38:33.938219 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:38:37.698160 (Thread-1): SQL status: DROP VIEW in 3.76 seconds
2020-04-28 19:38:37.702507 (Thread-1): finished collecting timing info
2020-04-28 19:38:37.703359 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fc1510>]}
2020-04-28 19:38:37.703664 (Thread-1): 12:38:37 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 7.35s]
2020-04-28 19:38:37.703914 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:38:37.704125 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:38:37.704457 (Thread-1): 12:38:37 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:38:37.705157 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:38:37.705338 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:38:37.705486 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:38:37.712550 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:38:37.712968 (Thread-1): finished collecting timing info
2020-04-28 19:38:37.719672 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:37.719784 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:38:38.510889 (Thread-1): SQL status: DROP VIEW in 0.79 seconds
2020-04-28 19:38:38.514611 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:38.514763 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:38:38.950659 (Thread-1): SQL status: DROP VIEW in 0.44 seconds
2020-04-28 19:38:38.953703 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:38:38.954315 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:38.954462 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:38:38.993659 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:38.993849 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:38.993957 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:38:39.047913 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:38:39.054048 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:39.054198 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:38:39.093080 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:39.097361 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:39.097511 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:38:39.147992 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:38:39.149892 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:38:39.150085 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:39.150240 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:38:40.468642 (Thread-1): SQL status: COMMIT in 1.32 seconds
2020-04-28 19:38:40.472194 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:38:40.472346 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:38:40.713074 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-28 19:38:40.715642 (Thread-1): finished collecting timing info
2020-04-28 19:38:40.716266 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b2fd90>]}
2020-04-28 19:38:40.716490 (Thread-1): 12:38:40 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 3.01s]
2020-04-28 19:38:40.716621 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:38:40.716768 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:38:40.717022 (Thread-1): 12:38:40 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:38:40.717397 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:40.717514 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:38:40.717616 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:38:40.724341 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:38:40.724750 (Thread-1): finished collecting timing info
2020-04-28 19:38:40.731233 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:40.731343 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:38:40.956632 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 19:38:40.961885 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:40.962040 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:38:41.130462 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:38:41.133532 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:38:41.134162 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:41.134314 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:38:41.177678 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:41.177961 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:38:41.178125 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT *
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.email
)
  );

2020-04-28 19:38:41.217559 (Thread-1): Postgres error: syntax error at or near ")"
LINE 17:   );
           ^

2020-04-28 19:38:41.218009 (Thread-1): On model.order_history.customer_broker: ROLLBACK
2020-04-28 19:38:41.256941 (Thread-1): finished collecting timing info
2020-04-28 19:38:41.257996 (Thread-1): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  syntax error at or near ")"
  LINE 17:   );
             ^
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near ")"
LINE 17:   );
           ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  syntax error at or near ")"
  LINE 17:   );
             ^
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:38:41.260905 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103940bd0>]}
2020-04-28 19:38:41.261197 (Thread-1): 12:38:41 | 5 of 7 ERROR creating view model data_science.customer_broker........ [ERROR in 0.54s]
2020-04-28 19:38:41.261378 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:38:41.261564 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:38:41.261746 (Thread-1): 12:38:41 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:38:41.262089 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.262222 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:38:41.262351 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:38:41.271669 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:38:41.272089 (Thread-1): finished collecting timing info
2020-04-28 19:38:41.279547 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.279690 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:38:41.357267 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:38:41.361192 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.361354 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:38:41.400141 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:38:41.402906 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:38:41.403477 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.403658 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:38:41.441510 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:41.441793 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.441958 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:38:41.493722 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:38:41.497765 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.497929 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:38:41.536518 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:41.538236 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:38:41.538431 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.538589 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:38:41.722558 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:38:41.724818 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:38:41.724960 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:38:41.903852 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:38:41.908114 (Thread-1): finished collecting timing info
2020-04-28 19:38:41.908964 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1039ec7d0>]}
2020-04-28 19:38:41.909269 (Thread-1): 12:38:41 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.65s]
2020-04-28 19:38:41.909449 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:38:41.909952 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:38:41.910203 (Thread-1): 12:38:41 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:38:41.910637 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:38:41.910764 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:38:41.910885 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:38:41.920482 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:38:41.920924 (Thread-1): finished collecting timing info
2020-04-28 19:38:41.929638 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:41.929847 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:38:42.097725 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:38:42.100336 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.100457 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:38:42.268367 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:38:42.271191 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:38:42.271873 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.272024 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:38:42.310917 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:42.311333 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.311615 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:38:42.370407 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:38:42.374811 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.374964 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:38:42.414536 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:38:42.415636 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:38:42.415764 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.415863 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:38:42.611622 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 19:38:42.614871 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:38:42.615023 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:38:42.813331 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:38:42.817388 (Thread-1): finished collecting timing info
2020-04-28 19:38:42.818235 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92707416-c05f-43b7-be3e-133753102b3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b2c610>]}
2020-04-28 19:38:42.818536 (Thread-1): 12:38:42 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 0.91s]
2020-04-28 19:38:42.818717 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:38:42.856429 (MainThread): Using postgres connection "master".
2020-04-28 19:38:42.856787 (MainThread): On master: BEGIN
2020-04-28 19:38:42.894748 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:38:42.894978 (MainThread): On master: COMMIT
2020-04-28 19:38:42.895092 (MainThread): Using postgres connection "master".
2020-04-28 19:38:42.895197 (MainThread): On master: COMMIT
2020-04-28 19:38:42.932281 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:38:42.933353 (MainThread): 12:38:42 | 
2020-04-28 19:38:42.933689 (MainThread): 12:38:42 | Finished running 7 view models in 16.83s.
2020-04-28 19:38:42.933983 (MainThread): Connection 'master' was left open.
2020-04-28 19:38:42.934196 (MainThread): On master: Close
2020-04-28 19:38:42.934692 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:38:42.934905 (MainThread): On model.order_history.customers: Close
2020-04-28 19:38:42.956208 (MainThread): 
2020-04-28 19:38:42.956442 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 19:38:42.956576 (MainThread): 
2020-04-28 19:38:42.956717 (MainThread): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
2020-04-28 19:38:42.956833 (MainThread):   syntax error at or near ")"
2020-04-28 19:38:42.956939 (MainThread):   LINE 17:   );
2020-04-28 19:38:42.957042 (MainThread):              ^
2020-04-28 19:38:42.957142 (MainThread):   compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:38:42.957254 (MainThread): 
Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
2020-04-28 19:38:42.957437 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b28950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa7e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c2a750>]}
2020-04-28 19:38:42.957643 (MainThread): Flushing usage events
2020-04-28 19:44:06.175445 (MainThread): Running with dbt=0.16.1
2020-04-28 19:44:06.252186 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:44:06.253405 (MainThread): Tracking: tracking
2020-04-28 19:44:06.259262 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d062ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2e7e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2e7f10>]}
2020-04-28 19:44:06.279470 (MainThread): Partial parsing not enabled
2020-04-28 19:44:06.281471 (MainThread): Parsing macros/core.sql
2020-04-28 19:44:06.286831 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:44:06.295475 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:44:06.297301 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:44:06.315610 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:44:06.349013 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:44:06.370492 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:44:06.372442 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:44:06.378956 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:44:06.392028 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:44:06.399060 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:44:06.405484 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:44:06.410344 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:44:06.411364 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:44:06.412436 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:44:06.414087 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:44:06.417490 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:44:06.427408 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:44:06.429432 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:44:06.430514 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:44:06.472704 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:44:06.474505 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:44:06.475577 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:44:06.477397 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:44:06.480113 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:44:06.482597 (MainThread): Parsing macros/relations.sql
2020-04-28 19:44:06.484622 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:44:06.507071 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:44:06.526089 (MainThread): Partial parsing not enabled
2020-04-28 19:44:06.556086 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:44:06.556211 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.573524 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:06.573641 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.577769 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:06.577864 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.582824 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:44:06.582920 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.587020 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:44:06.587117 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.592172 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:06.592267 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.597877 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:44:06.597991 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.745882 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:44:06.750072 (MainThread): 
2020-04-28 19:44:06.750477 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:44:06.750617 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:44:06.773215 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:44:06.773420 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:44:06.857130 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:44:06.857263 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:44:07.438743 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.58 seconds
2020-04-28 19:44:07.468295 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:44:07.468413 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:44:07.469705 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:44:07.469796 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:44:07.511966 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:07.512389 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:44:07.512650 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:44:07.634465 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-28 19:44:07.642760 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:44:07.714554 (MainThread): Using postgres connection "master".
2020-04-28 19:44:07.714707 (MainThread): On master: BEGIN
2020-04-28 19:44:08.075389 (MainThread): SQL status: BEGIN in 0.36 seconds
2020-04-28 19:44:08.075789 (MainThread): Using postgres connection "master".
2020-04-28 19:44:08.075922 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:44:08.206074 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 19:44:08.282103 (MainThread): On master: ROLLBACK
2020-04-28 19:44:08.322576 (MainThread): Using postgres connection "master".
2020-04-28 19:44:08.322986 (MainThread): On master: BEGIN
2020-04-28 19:44:08.402541 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 19:44:08.402995 (MainThread): On master: COMMIT
2020-04-28 19:44:08.403303 (MainThread): Using postgres connection "master".
2020-04-28 19:44:08.403468 (MainThread): On master: COMMIT
2020-04-28 19:44:08.442500 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:44:08.442942 (MainThread): 12:44:08 | Concurrency: 1 threads (target='dev')
2020-04-28 19:44:08.443110 (MainThread): 12:44:08 | 
2020-04-28 19:44:08.445687 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:44:08.445909 (Thread-1): 12:44:08 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:44:08.446256 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.446376 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:44:08.446509 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:44:08.469001 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:44:08.469514 (Thread-1): finished collecting timing info
2020-04-28 19:44:08.506200 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.506352 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:44:08.582223 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:44:08.586874 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.587033 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:44:08.625097 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:44:08.627971 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:44:08.628666 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.628813 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:44:08.666875 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:08.667299 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.667536 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:44:08.720658 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:08.727065 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.727218 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:44:08.768471 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:08.771486 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.771625 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:44:08.809906 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:08.810971 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:44:08.811096 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.811192 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:44:08.984972 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:08.988480 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:44:08.988637 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:44:09.216111 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 19:44:09.220424 (Thread-1): finished collecting timing info
2020-04-28 19:44:09.221274 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6bf590>]}
2020-04-28 19:44:09.221588 (Thread-1): 12:44:09 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.77s]
2020-04-28 19:44:09.221772 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:44:09.221954 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:44:09.222133 (Thread-1): 12:44:09 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:44:09.222466 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.222593 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:44:09.222721 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:44:09.228722 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:44:09.229325 (Thread-1): finished collecting timing info
2020-04-28 19:44:09.236774 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.236898 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:44:09.423631 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:44:09.427861 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.428011 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:44:09.596843 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:09.599638 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:44:09.600205 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.600376 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:44:09.639600 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:09.640013 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.640264 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:44:09.687724 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:09.694071 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.694227 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:44:09.732829 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:09.737221 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.737370 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:44:09.799411 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2020-04-28 19:44:09.800581 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:44:09.800704 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:09.800803 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:44:10.020135 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 19:44:10.024913 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:44:10.025059 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:44:10.203794 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:44:10.207216 (Thread-1): finished collecting timing info
2020-04-28 19:44:10.207908 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d616dd0>]}
2020-04-28 19:44:10.208153 (Thread-1): 12:44:10 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.99s]
2020-04-28 19:44:10.208294 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:44:10.208444 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:44:10.208652 (Thread-1): 12:44:10 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:44:10.209303 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.209705 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:44:10.209890 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:44:10.247965 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:44:10.248455 (Thread-1): finished collecting timing info
2020-04-28 19:44:10.255827 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.255957 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:44:10.422210 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:10.424767 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.424889 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:44:10.589661 (Thread-1): SQL status: DROP VIEW in 0.16 seconds
2020-04-28 19:44:10.592632 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:44:10.593265 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.593408 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:44:10.631691 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:10.632105 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.632356 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:44:10.680641 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:10.686788 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.686938 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:44:10.725272 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:10.729367 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.729515 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:44:10.768134 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:10.770270 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:44:10.770461 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.770614 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:44:10.941420 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:10.944802 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:44:10.944947 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:44:11.120438 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:44:11.124531 (Thread-1): finished collecting timing info
2020-04-28 19:44:11.125375 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d616dd0>]}
2020-04-28 19:44:11.125675 (Thread-1): 12:44:11 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.92s]
2020-04-28 19:44:11.125849 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:44:11.126082 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:44:11.126454 (Thread-1): 12:44:11 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:44:11.127030 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.127196 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:44:11.127325 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:44:11.134784 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:44:11.135223 (Thread-1): finished collecting timing info
2020-04-28 19:44:11.143095 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.143248 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:44:11.312859 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:11.316983 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.317129 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:44:11.484192 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:11.486158 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:44:11.486609 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.486733 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:44:11.525037 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:11.525215 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.525310 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:44:11.573355 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:11.579604 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.579757 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:44:11.620013 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:11.624314 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.624464 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:44:11.663292 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:11.665221 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:44:11.665411 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.665560 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:44:11.833405 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:11.835748 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:44:11.835887 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:44:12.053323 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 19:44:12.057565 (Thread-1): finished collecting timing info
2020-04-28 19:44:12.058404 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9249d0>]}
2020-04-28 19:44:12.058699 (Thread-1): 12:44:12 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 0.93s]
2020-04-28 19:44:12.058872 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:44:12.059108 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:44:12.059436 (Thread-1): 12:44:12 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:44:12.059913 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.060143 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:44:12.060327 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:44:12.068017 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:44:12.068444 (Thread-1): finished collecting timing info
2020-04-28 19:44:12.075857 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.075978 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:44:12.287148 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:44:12.292402 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.292547 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:44:12.537107 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-28 19:44:12.539426 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:44:12.539889 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.540007 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:44:12.578022 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:12.578435 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:44:12.578686 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT *
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.email
)
select * from final
  );

2020-04-28 19:44:12.620939 (Thread-1): Postgres error: column "email" duplicated

2020-04-28 19:44:12.621355 (Thread-1): On model.order_history.customer_broker: ROLLBACK
2020-04-28 19:44:12.659060 (Thread-1): finished collecting timing info
2020-04-28 19:44:12.660085 (Thread-1): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  column "email" duplicated
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.DuplicateColumn: column "email" duplicated


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  column "email" duplicated
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:44:12.663033 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2fda90>]}
2020-04-28 19:44:12.663320 (Thread-1): 12:44:12 | 5 of 7 ERROR creating view model data_science.customer_broker........ [ERROR in 0.60s]
2020-04-28 19:44:12.663493 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:44:12.663734 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:44:12.664093 (Thread-1): 12:44:12 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:44:12.664486 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.664632 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:44:12.664766 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:44:12.674093 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:44:12.675241 (Thread-1): finished collecting timing info
2020-04-28 19:44:12.682679 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.682810 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:44:12.759735 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:44:12.763915 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.764073 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:44:12.802663 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:44:12.805567 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:44:12.806116 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.806267 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:44:12.843722 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:12.843913 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.844022 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:44:12.901889 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:44:12.904438 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.904549 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:44:12.943359 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:12.945273 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:44:12.945458 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:12.945610 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:44:13.118134 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:13.121492 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:44:13.121662 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:44:13.288945 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:13.293202 (Thread-1): finished collecting timing info
2020-04-28 19:44:13.294038 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d344b50>]}
2020-04-28 19:44:13.294334 (Thread-1): 12:44:13 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.63s]
2020-04-28 19:44:13.294509 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:44:13.294919 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:44:13.295130 (Thread-1): 12:44:13 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:44:13.295484 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:44:13.295611 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:44:13.295736 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:44:13.304713 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:44:13.306195 (Thread-1): finished collecting timing info
2020-04-28 19:44:13.315128 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.315282 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:44:13.481452 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:13.485609 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.485756 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:44:13.656656 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:13.659483 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:44:13.660149 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.660303 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:44:13.698171 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:13.698607 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.698890 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:44:13.752581 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:44:13.757016 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.757164 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:44:13.797772 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:44:13.799708 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:44:13.799897 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.800052 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:44:13.969533 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:44:13.972898 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:44:13.973045 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:44:14.144194 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:44:14.148436 (Thread-1): finished collecting timing info
2020-04-28 19:44:14.149276 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cad230b-de96-45bb-aeb2-008d00f58499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d472d50>]}
2020-04-28 19:44:14.149573 (Thread-1): 12:44:14 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 0.85s]
2020-04-28 19:44:14.149745 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:44:14.194507 (MainThread): Using postgres connection "master".
2020-04-28 19:44:14.194857 (MainThread): On master: BEGIN
2020-04-28 19:44:14.234526 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:44:14.234784 (MainThread): On master: COMMIT
2020-04-28 19:44:14.234928 (MainThread): Using postgres connection "master".
2020-04-28 19:44:14.235061 (MainThread): On master: COMMIT
2020-04-28 19:44:14.274374 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:44:14.275051 (MainThread): 12:44:14 | 
2020-04-28 19:44:14.275283 (MainThread): 12:44:14 | Finished running 7 view models in 7.52s.
2020-04-28 19:44:14.275438 (MainThread): Connection 'master' was left open.
2020-04-28 19:44:14.275557 (MainThread): On master: Close
2020-04-28 19:44:14.275867 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:44:14.275990 (MainThread): On model.order_history.customers: Close
2020-04-28 19:44:14.295719 (MainThread): 
2020-04-28 19:44:14.295928 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 19:44:14.296055 (MainThread): 
2020-04-28 19:44:14.296164 (MainThread): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
2020-04-28 19:44:14.296264 (MainThread):   column "email" duplicated
2020-04-28 19:44:14.296356 (MainThread):   compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:44:14.296459 (MainThread): 
Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
2020-04-28 19:44:14.296628 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5c2a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4bae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d33a910>]}
2020-04-28 19:44:14.296811 (MainThread): Flushing usage events
2020-04-28 19:45:23.546959 (MainThread): Running with dbt=0.16.1
2020-04-28 19:45:23.610533 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:45:23.611285 (MainThread): Tracking: tracking
2020-04-28 19:45:23.617144 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbffed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c9b64d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c9b6a50>]}
2020-04-28 19:45:23.643166 (MainThread): Partial parsing not enabled
2020-04-28 19:45:23.645747 (MainThread): Parsing macros/core.sql
2020-04-28 19:45:23.651016 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:45:23.659371 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:45:23.661182 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:45:23.678898 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:45:23.711754 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:45:23.733152 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:45:23.735076 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:45:23.741440 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:45:23.754203 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:45:23.761209 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:45:23.767664 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:45:23.772694 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:45:23.773919 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:45:23.775038 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:45:23.776765 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:45:23.778900 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:45:23.788147 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:45:23.790193 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:45:23.791296 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:45:23.832351 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:45:23.833491 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:45:23.834406 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:45:23.835491 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:45:23.837705 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:45:23.841255 (MainThread): Parsing macros/relations.sql
2020-04-28 19:45:23.842717 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:45:23.860439 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:45:23.877832 (MainThread): Partial parsing not enabled
2020-04-28 19:45:23.904679 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:45:23.904779 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.921105 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:23.921219 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.925180 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:23.925270 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.930438 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:45:23.930543 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.935348 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:45:23.935453 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.940461 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:23.940585 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:23.948448 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:45:23.948645 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:24.089847 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:45:24.093385 (MainThread): 
2020-04-28 19:45:24.093825 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:45:24.093918 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:45:24.115320 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:45:24.115530 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:45:24.198831 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:45:24.198971 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:45:24.716833 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.52 seconds
2020-04-28 19:45:24.748914 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:45:24.749054 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:45:24.750525 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:45:24.750630 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:45:24.792547 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:24.792996 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:45:24.793296 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:45:24.911065 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2020-04-28 19:45:24.917601 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:45:24.984638 (MainThread): Using postgres connection "master".
2020-04-28 19:45:24.984764 (MainThread): On master: BEGIN
2020-04-28 19:45:25.378905 (MainThread): SQL status: BEGIN in 0.39 seconds
2020-04-28 19:45:25.379649 (MainThread): Using postgres connection "master".
2020-04-28 19:45:25.379826 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:45:25.563854 (MainThread): SQL status: SELECT in 0.18 seconds
2020-04-28 19:45:25.650117 (MainThread): On master: ROLLBACK
2020-04-28 19:45:25.691645 (MainThread): Using postgres connection "master".
2020-04-28 19:45:25.692058 (MainThread): On master: BEGIN
2020-04-28 19:45:25.776018 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 19:45:25.776486 (MainThread): On master: COMMIT
2020-04-28 19:45:25.776785 (MainThread): Using postgres connection "master".
2020-04-28 19:45:25.776945 (MainThread): On master: COMMIT
2020-04-28 19:45:25.820066 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:45:25.820952 (MainThread): 12:45:25 | Concurrency: 1 threads (target='dev')
2020-04-28 19:45:25.821199 (MainThread): 12:45:25 | 
2020-04-28 19:45:25.823527 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:45:25.823825 (Thread-1): 12:45:25 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:45:25.824233 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:25.824380 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:45:25.824534 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:45:25.841627 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:45:25.842114 (Thread-1): finished collecting timing info
2020-04-28 19:45:25.882758 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:25.882926 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:45:25.965824 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:45:25.969105 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:25.969235 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:45:26.011162 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:45:26.014037 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:45:26.014706 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.014863 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:45:26.055832 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:26.056128 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.056303 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:45:26.118262 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:26.124613 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.124774 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:45:26.170948 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:26.175137 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.175294 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:45:26.217869 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:26.219887 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:45:26.220090 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.220256 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:45:26.396125 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:45:26.399504 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:45:26.399673 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:45:26.603845 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:45:26.607390 (Thread-1): finished collecting timing info
2020-04-28 19:45:26.608218 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cff9990>]}
2020-04-28 19:45:26.608505 (Thread-1): 12:45:26 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.78s]
2020-04-28 19:45:26.608672 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:45:26.608842 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:45:26.609121 (Thread-1): 12:45:26 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:45:26.609562 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:26.609697 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:45:26.609816 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:45:26.615992 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:45:26.616426 (Thread-1): finished collecting timing info
2020-04-28 19:45:26.624062 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:26.624210 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:45:26.794650 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:45:26.798786 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:26.798947 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:45:26.975625 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:26.977600 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:45:26.978122 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:26.978258 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:45:27.020053 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:27.020450 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.020666 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:45:27.084131 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:27.089621 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.089752 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:45:27.132306 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:27.136768 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.136926 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:45:27.183322 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:27.185297 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:45:27.185502 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.185663 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:45:27.356961 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:45:27.359922 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:45:27.360041 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:45:27.543408 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:27.547610 (Thread-1): finished collecting timing info
2020-04-28 19:45:27.548452 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce2af10>]}
2020-04-28 19:45:27.548759 (Thread-1): 12:45:27 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.94s]
2020-04-28 19:45:27.548940 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:45:27.549237 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:45:27.549687 (Thread-1): 12:45:27 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:45:27.550215 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:45:27.550346 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:45:27.550468 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:45:27.588965 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:45:27.589513 (Thread-1): finished collecting timing info
2020-04-28 19:45:27.597260 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:27.597401 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:45:27.782895 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:45:27.786965 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:27.787122 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:45:29.035749 (Thread-1): SQL status: DROP VIEW in 1.25 seconds
2020-04-28 19:45:29.038805 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:45:29.039444 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:29.039601 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:45:29.081142 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:29.081420 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:29.081568 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:45:31.833280 (Thread-1): SQL status: CREATE VIEW in 2.75 seconds
2020-04-28 19:45:31.839474 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:31.839635 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:45:31.888574 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:31.892836 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:31.892993 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:45:31.940283 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:31.942421 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:45:31.942625 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:31.942790 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:45:32.119216 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:45:32.122585 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:45:32.122745 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:45:32.304277 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:32.307241 (Thread-1): finished collecting timing info
2020-04-28 19:45:32.307959 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce2af10>]}
2020-04-28 19:45:32.308222 (Thread-1): 12:45:32 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 4.76s]
2020-04-28 19:45:32.308378 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:45:32.308543 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:45:32.308963 (Thread-1): 12:45:32 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:45:32.309459 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.309597 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:45:32.309724 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:45:32.316924 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:45:32.317416 (Thread-1): finished collecting timing info
2020-04-28 19:45:32.324885 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.325071 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:45:32.563404 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2020-04-28 19:45:32.567382 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.567539 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:45:32.786954 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 19:45:32.789923 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:45:32.790531 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.790691 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:45:32.833330 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:32.833765 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.834043 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:45:32.891413 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:32.897396 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.897557 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:45:32.945071 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:45:32.949346 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.949502 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:45:32.992171 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:32.993429 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:45:32.993580 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:32.993694 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:45:33.167532 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:45:33.171069 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:45:33.171224 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:45:33.352340 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:33.356416 (Thread-1): finished collecting timing info
2020-04-28 19:45:33.357523 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cca4dd0>]}
2020-04-28 19:45:33.357887 (Thread-1): 12:45:33 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 1.05s]
2020-04-28 19:45:33.358118 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:45:33.358354 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:45:33.358769 (Thread-1): 12:45:33 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:45:33.359278 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.359472 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:45:33.359751 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:45:33.368159 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:45:33.368569 (Thread-1): finished collecting timing info
2020-04-28 19:45:33.375165 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.375284 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:45:33.549758 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:45:33.554994 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.555145 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:45:33.900629 (Thread-1): SQL status: DROP VIEW in 0.35 seconds
2020-04-28 19:45:33.903627 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:45:33.904221 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.904375 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:45:33.949087 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:33.949511 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:45:33.949772 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.email
)
select * from final
  );

2020-04-28 19:45:33.994719 (Thread-1): Postgres error: column reference "email" is ambiguous

2020-04-28 19:45:33.995140 (Thread-1): On model.order_history.customer_broker: ROLLBACK
2020-04-28 19:45:34.052386 (Thread-1): finished collecting timing info
2020-04-28 19:45:34.053066 (Thread-1): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  column reference "email" is ambiguous
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.AmbiguousColumn: column reference "email" is ambiguous


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customer_broker (models/intermediate/customer_broker.sql)
  column reference "email" is ambiguous
  compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:45:34.055619 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ccdbc90>]}
2020-04-28 19:45:34.055846 (Thread-1): 12:45:34 | 5 of 7 ERROR creating view model data_science.customer_broker........ [ERROR in 0.70s]
2020-04-28 19:45:34.055982 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:45:34.056120 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:45:34.056365 (Thread-1): 12:45:34 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:45:34.056662 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.056759 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:45:34.056854 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:45:34.064680 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:45:34.065057 (Thread-1): finished collecting timing info
2020-04-28 19:45:34.071575 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.071742 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:45:34.155110 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:45:34.159315 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.159465 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:45:34.201339 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:45:34.204374 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:45:34.205000 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.205154 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:45:34.246977 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:34.247396 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.247661 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:45:34.309855 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:34.313942 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.314110 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:45:34.355937 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:34.357111 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:45:34.357242 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.357345 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:45:34.534523 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:45:34.537891 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:45:34.538044 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:45:34.713230 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:34.715885 (Thread-1): finished collecting timing info
2020-04-28 19:45:34.716545 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cca1810>]}
2020-04-28 19:45:34.716790 (Thread-1): 12:45:34 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.66s]
2020-04-28 19:45:34.716922 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:45:34.717281 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:45:34.717449 (Thread-1): 12:45:34 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:45:34.717724 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:45:34.717820 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:45:34.717916 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:45:34.725884 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:45:34.726259 (Thread-1): finished collecting timing info
2020-04-28 19:45:34.733937 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:34.734084 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:45:34.912432 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:45:34.916554 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:34.916709 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:45:35.112354 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:45:35.115152 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:45:35.115802 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.115951 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:45:35.157029 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:35.157450 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.157714 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:45:35.221953 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:45:35.226370 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.226524 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:45:35.268792 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:45:35.270716 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:45:35.270910 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.271065 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:45:35.555459 (Thread-1): SQL status: COMMIT in 0.28 seconds
2020-04-28 19:45:35.558698 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:45:35.558853 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:45:37.819568 (Thread-1): SQL status: DROP VIEW in 2.26 seconds
2020-04-28 19:45:37.822526 (Thread-1): finished collecting timing info
2020-04-28 19:45:37.823267 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ee8a1d4-7807-4d26-a951-b1026aa83484', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc78bd0>]}
2020-04-28 19:45:37.823531 (Thread-1): 12:45:37 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 3.11s]
2020-04-28 19:45:37.823685 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:45:37.858665 (MainThread): Using postgres connection "master".
2020-04-28 19:45:37.858894 (MainThread): On master: BEGIN
2020-04-28 19:45:37.900985 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:45:37.901237 (MainThread): On master: COMMIT
2020-04-28 19:45:37.901347 (MainThread): Using postgres connection "master".
2020-04-28 19:45:37.901449 (MainThread): On master: COMMIT
2020-04-28 19:45:37.942834 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:45:37.943684 (MainThread): 12:45:37 | 
2020-04-28 19:45:37.943926 (MainThread): 12:45:37 | Finished running 7 view models in 13.85s.
2020-04-28 19:45:37.944120 (MainThread): Connection 'master' was left open.
2020-04-28 19:45:37.944274 (MainThread): On master: Close
2020-04-28 19:45:37.944668 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:45:37.944832 (MainThread): On model.order_history.customers: Close
2020-04-28 19:45:37.966809 (MainThread): 
2020-04-28 19:45:37.967033 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 19:45:37.967178 (MainThread): 
2020-04-28 19:45:37.967311 (MainThread): Database Error in model customer_broker (models/intermediate/customer_broker.sql)
2020-04-28 19:45:37.967433 (MainThread):   column reference "email" is ambiguous
2020-04-28 19:45:37.967547 (MainThread):   compiled SQL at target/run/order_history/intermediate/customer_broker.sql
2020-04-28 19:45:37.967670 (MainThread): 
Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
2020-04-28 19:45:37.967866 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce8fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce95310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce8ba90>]}
2020-04-28 19:45:37.968077 (MainThread): Flushing usage events
2020-04-28 19:46:02.694113 (MainThread): Running with dbt=0.16.1
2020-04-28 19:46:02.759550 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:46:02.760456 (MainThread): Tracking: tracking
2020-04-28 19:46:02.765588 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee5e390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee5ca50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ebd36d0>]}
2020-04-28 19:46:02.783630 (MainThread): Partial parsing not enabled
2020-04-28 19:46:02.785430 (MainThread): Parsing macros/core.sql
2020-04-28 19:46:02.789968 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:46:02.798226 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:46:02.800039 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:46:02.818196 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:46:02.851595 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:46:02.872914 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:46:02.874844 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:46:02.881216 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:46:02.893989 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:46:02.900974 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:46:02.907451 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:46:02.912278 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:46:02.913280 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:46:02.914543 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:46:02.916190 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:46:02.918225 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:46:02.927181 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:46:02.929139 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:46:02.930182 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:46:02.971579 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:46:02.972771 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:46:02.973924 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:46:02.975646 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:46:02.978376 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:46:02.981703 (MainThread): Parsing macros/relations.sql
2020-04-28 19:46:02.983228 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:46:03.005929 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:46:03.024389 (MainThread): Partial parsing not enabled
2020-04-28 19:46:03.051905 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:46:03.052010 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.067843 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:03.067939 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.071863 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:03.071953 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.076232 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:46:03.076320 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.080249 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:46:03.080335 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.084888 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:03.084993 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.090527 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:46:03.090625 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.223387 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:46:03.226978 (MainThread): 
2020-04-28 19:46:03.227414 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:46:03.227499 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:46:03.248389 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:46:03.248784 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:46:03.342101 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:46:03.342242 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:46:03.766262 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.42 seconds
2020-04-28 19:46:03.801255 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:46:03.801377 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:46:03.803018 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:46:03.803136 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:46:03.860360 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.06 seconds
2020-04-28 19:46:03.860788 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:46:03.861057 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:46:03.954190 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.09 seconds
2020-04-28 19:46:03.960328 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:46:04.024057 (MainThread): Using postgres connection "master".
2020-04-28 19:46:04.024190 (MainThread): On master: BEGIN
2020-04-28 19:46:04.365433 (MainThread): SQL status: BEGIN in 0.34 seconds
2020-04-28 19:46:04.366169 (MainThread): Using postgres connection "master".
2020-04-28 19:46:04.366375 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:46:04.494207 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 19:46:04.573240 (MainThread): On master: ROLLBACK
2020-04-28 19:46:04.610903 (MainThread): Using postgres connection "master".
2020-04-28 19:46:04.611315 (MainThread): On master: BEGIN
2020-04-28 19:46:04.684848 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 19:46:04.685038 (MainThread): On master: COMMIT
2020-04-28 19:46:04.685142 (MainThread): Using postgres connection "master".
2020-04-28 19:46:04.685231 (MainThread): On master: COMMIT
2020-04-28 19:46:04.721777 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:46:04.722672 (MainThread): 12:46:04 | Concurrency: 1 threads (target='dev')
2020-04-28 19:46:04.722920 (MainThread): 12:46:04 | 
2020-04-28 19:46:04.725149 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:46:04.725418 (Thread-1): 12:46:04 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:46:04.725801 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.725936 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:46:04.726083 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:46:04.741209 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:46:04.741622 (Thread-1): finished collecting timing info
2020-04-28 19:46:04.778669 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.778834 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:46:04.864669 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2020-04-28 19:46:04.869659 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.869814 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:46:04.910108 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:46:04.913027 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:46:04.913686 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.913840 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:46:04.953321 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:04.953555 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:04.953691 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:46:05.009511 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:46:05.013733 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:05.013858 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:46:05.054655 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:05.058968 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:05.059124 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:46:05.099793 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:05.101751 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:46:05.101949 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:05.102110 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:46:05.280446 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:46:05.283912 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:46:05.284076 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:46:05.517048 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2020-04-28 19:46:05.521327 (Thread-1): finished collecting timing info
2020-04-28 19:46:05.522189 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10effe7d0>]}
2020-04-28 19:46:05.522504 (Thread-1): 12:46:05 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 0.80s]
2020-04-28 19:46:05.522692 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:46:05.522877 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:46:05.523060 (Thread-1): 12:46:05 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:46:05.523518 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.523802 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:46:05.524099 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:46:05.530294 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:46:05.530724 (Thread-1): finished collecting timing info
2020-04-28 19:46:05.538235 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.538384 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:46:05.713845 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:05.717979 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.718142 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:46:05.890051 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:05.893109 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:46:05.893717 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.893876 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:46:05.934429 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:05.934862 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.935129 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:46:05.990578 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:46:05.995501 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:05.995693 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:46:06.036421 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:06.039886 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:06.040016 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:46:06.086477 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:46:06.088492 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:46:06.088688 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:06.088847 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:46:06.310166 (Thread-1): SQL status: COMMIT in 0.22 seconds
2020-04-28 19:46:06.313704 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:46:06.313844 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:46:06.498662 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:06.502906 (Thread-1): finished collecting timing info
2020-04-28 19:46:06.503754 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10efe9310>]}
2020-04-28 19:46:06.504063 (Thread-1): 12:46:06 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.98s]
2020-04-28 19:46:06.504248 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:46:06.504491 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:46:06.504871 (Thread-1): 12:46:06 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:46:06.505366 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:46:06.505538 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:46:06.505714 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:46:06.542806 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:46:06.543404 (Thread-1): finished collecting timing info
2020-04-28 19:46:06.551633 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:06.551838 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:46:08.018626 (Thread-1): SQL status: DROP VIEW in 1.47 seconds
2020-04-28 19:46:08.022726 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:08.022881 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:46:08.201924 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:08.204899 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:46:08.205548 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:08.205704 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:46:08.246115 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:08.246323 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:08.246440 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:46:09.230747 (Thread-1): SQL status: CREATE VIEW in 0.98 seconds
2020-04-28 19:46:09.236901 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:09.237054 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:46:09.283087 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:46:09.287236 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:09.287436 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:46:09.329383 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:09.331519 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:46:09.331717 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:09.331880 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:46:09.505000 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:46:09.508380 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:46:09.508548 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:46:09.686736 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:09.690985 (Thread-1): finished collecting timing info
2020-04-28 19:46:09.691837 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eed5c50>]}
2020-04-28 19:46:09.692144 (Thread-1): 12:46:09 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 3.19s]
2020-04-28 19:46:09.692324 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:46:09.692509 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:46:09.692944 (Thread-1): 12:46:09 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:46:09.693654 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:46:09.693810 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:46:09.693980 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:46:09.700928 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:46:09.701342 (Thread-1): finished collecting timing info
2020-04-28 19:46:09.708188 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:09.708306 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:46:09.878071 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:09.882185 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:09.882335 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:46:10.058534 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:10.061588 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:46:10.062219 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.062387 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:46:10.102812 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:10.103244 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.103524 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:46:10.155248 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:46:10.159593 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.159719 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:46:10.201578 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:10.204218 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.204332 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:46:10.245306 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:10.246345 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:46:10.246461 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.246554 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:46:10.417436 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:46:10.420982 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:46:10.421139 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:46:10.596802 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:10.599676 (Thread-1): finished collecting timing info
2020-04-28 19:46:10.600404 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f0b5750>]}
2020-04-28 19:46:10.600661 (Thread-1): 12:46:10 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 0.91s]
2020-04-28 19:46:10.600811 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:46:10.600964 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:46:10.601203 (Thread-1): 12:46:10 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:46:10.601508 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:10.601614 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:46:10.601717 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:46:10.608842 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:46:10.609273 (Thread-1): finished collecting timing info
2020-04-28 19:46:10.616515 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:10.616728 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:46:10.787275 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:10.792552 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:10.792713 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:46:11.007862 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:46:11.010935 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:46:11.011553 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.011710 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:46:11.052240 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:11.052668 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.052938 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-28 19:46:11.107733 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:46:11.111995 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.112151 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-28 19:46:11.153079 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:11.155043 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 19:46:11.155237 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.155397 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 19:46:11.338627 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:46:11.341915 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:46:11.342066 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:46:11.514284 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:11.518516 (Thread-1): finished collecting timing info
2020-04-28 19:46:11.519364 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee63310>]}
2020-04-28 19:46:11.519671 (Thread-1): 12:46:11 | 5 of 7 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.92s]
2020-04-28 19:46:11.519853 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:46:11.520036 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:46:11.520465 (Thread-1): 12:46:11 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:46:11.521007 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.521199 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:46:11.521337 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:46:11.530361 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:46:11.530793 (Thread-1): finished collecting timing info
2020-04-28 19:46:11.538095 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.538233 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:46:11.707667 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:11.711880 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.712032 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:46:11.893800 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:46:11.896777 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:46:11.897401 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.897563 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:46:11.938078 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:11.938529 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.938821 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:46:11.995086 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:46:11.997677 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:11.997794 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:46:12.038499 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:46:12.040362 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:46:12.040570 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:12.040733 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:46:12.240888 (Thread-1): SQL status: COMMIT in 0.20 seconds
2020-04-28 19:46:12.244260 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:46:12.244415 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:46:12.418496 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:12.422733 (Thread-1): finished collecting timing info
2020-04-28 19:46:12.423577 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f00f390>]}
2020-04-28 19:46:12.423884 (Thread-1): 12:46:12 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.90s]
2020-04-28 19:46:12.424065 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:46:12.424525 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:46:12.424743 (Thread-1): 12:46:12 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:46:12.425143 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:46:12.425283 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:46:12.425409 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:46:12.435525 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:46:12.435932 (Thread-1): finished collecting timing info
2020-04-28 19:46:12.443218 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.443358 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:46:12.640270 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2020-04-28 19:46:12.643147 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.643286 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:46:12.814798 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:12.817770 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:46:12.818383 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.818541 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:46:12.858854 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:12.859286 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.859555 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:46:12.920391 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:46:12.924665 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.924848 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:46:12.978335 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:46:12.979536 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:46:12.979669 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:12.979774 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:46:13.149259 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:46:13.152664 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:46:13.152808 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:46:13.321715 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:46:13.324698 (Thread-1): finished collecting timing info
2020-04-28 19:46:13.325437 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8262bf7b-3b75-451f-b2d3-a50774018cf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f157ad0>]}
2020-04-28 19:46:13.325695 (Thread-1): 12:46:13 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 0.90s]
2020-04-28 19:46:13.325844 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:46:13.378631 (MainThread): Using postgres connection "master".
2020-04-28 19:46:13.378946 (MainThread): On master: BEGIN
2020-04-28 19:46:13.417960 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:46:13.418411 (MainThread): On master: COMMIT
2020-04-28 19:46:13.418688 (MainThread): Using postgres connection "master".
2020-04-28 19:46:13.418867 (MainThread): On master: COMMIT
2020-04-28 19:46:13.455978 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:46:13.456902 (MainThread): 12:46:13 | 
2020-04-28 19:46:13.457135 (MainThread): 12:46:13 | Finished running 7 view models in 10.23s.
2020-04-28 19:46:13.457322 (MainThread): Connection 'master' was left open.
2020-04-28 19:46:13.457470 (MainThread): On master: Close
2020-04-28 19:46:13.457866 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:46:13.458024 (MainThread): On model.order_history.customers: Close
2020-04-28 19:46:13.480051 (MainThread): 
2020-04-28 19:46:13.480276 (MainThread): Completed successfully
2020-04-28 19:46:13.480425 (MainThread): 
Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
2020-04-28 19:46:13.480626 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ebe8790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f3e79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef0f090>]}
2020-04-28 19:46:13.480840 (MainThread): Flushing usage events
2020-04-28 19:48:19.861947 (MainThread): Running with dbt=0.16.1
2020-04-28 19:48:19.928666 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 19:48:19.929699 (MainThread): Tracking: tracking
2020-04-28 19:48:19.934931 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c4bf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c74310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c74c10>]}
2020-04-28 19:48:19.957433 (MainThread): Partial parsing not enabled
2020-04-28 19:48:19.961002 (MainThread): Parsing macros/core.sql
2020-04-28 19:48:19.969689 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 19:48:19.980078 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 19:48:19.982038 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 19:48:20.000417 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 19:48:20.034664 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 19:48:20.056621 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 19:48:20.058644 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 19:48:20.065223 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 19:48:20.078431 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 19:48:20.085587 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 19:48:20.092176 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 19:48:20.097383 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 19:48:20.098401 (MainThread): Parsing macros/etc/query.sql
2020-04-28 19:48:20.100115 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 19:48:20.102072 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 19:48:20.104467 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 19:48:20.115100 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 19:48:20.117267 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 19:48:20.118380 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 19:48:20.159399 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 19:48:20.160595 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 19:48:20.161530 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 19:48:20.162636 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 19:48:20.164891 (MainThread): Parsing macros/catalog.sql
2020-04-28 19:48:20.167270 (MainThread): Parsing macros/relations.sql
2020-04-28 19:48:20.168631 (MainThread): Parsing macros/adapters.sql
2020-04-28 19:48:20.185511 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 19:48:20.203108 (MainThread): Partial parsing not enabled
2020-04-28 19:48:20.229492 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:48:20.229591 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.244753 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:20.244841 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.248918 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:20.249005 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.253260 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:48:20.253345 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.257098 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:48:20.257181 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.261949 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:20.262047 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.266903 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:48:20.266995 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.412896 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 19:48:20.417582 (MainThread): 
2020-04-28 19:48:20.417895 (MainThread): Acquiring new postgres connection "master".
2020-04-28 19:48:20.418014 (MainThread): Opening a new connection, currently in state init
2020-04-28 19:48:20.439895 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 19:48:20.440035 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 19:48:20.522191 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 19:48:20.522324 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 19:48:20.965065 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.44 seconds
2020-04-28 19:48:20.999443 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:48:20.999581 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 19:48:21.001153 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:48:21.001269 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 19:48:21.416658 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.42 seconds
2020-04-28 19:48:21.417076 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 19:48:21.417327 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 19:48:21.660794 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.24 seconds
2020-04-28 19:48:21.669891 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 19:48:21.745686 (MainThread): Using postgres connection "master".
2020-04-28 19:48:21.745848 (MainThread): On master: BEGIN
2020-04-28 19:48:22.114600 (MainThread): SQL status: BEGIN in 0.37 seconds
2020-04-28 19:48:22.114820 (MainThread): Using postgres connection "master".
2020-04-28 19:48:22.114953 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 19:48:24.218129 (MainThread): SQL status: SELECT in 2.10 seconds
2020-04-28 19:48:24.294575 (MainThread): On master: ROLLBACK
2020-04-28 19:48:24.335860 (MainThread): Using postgres connection "master".
2020-04-28 19:48:24.336263 (MainThread): On master: BEGIN
2020-04-28 19:48:24.422451 (MainThread): SQL status: BEGIN in 0.09 seconds
2020-04-28 19:48:24.422789 (MainThread): On master: COMMIT
2020-04-28 19:48:24.422982 (MainThread): Using postgres connection "master".
2020-04-28 19:48:24.423173 (MainThread): On master: COMMIT
2020-04-28 19:48:24.463008 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:48:24.463454 (MainThread): 12:48:24 | Concurrency: 1 threads (target='dev')
2020-04-28 19:48:24.463629 (MainThread): 12:48:24 | 
2020-04-28 19:48:24.465502 (Thread-1): Began running node model.order_history.stg_customers
2020-04-28 19:48:24.465910 (Thread-1): 12:48:24 | 1 of 7 START view model data_science.stg_customers................... [RUN]
2020-04-28 19:48:24.466284 (Thread-1): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.466416 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 19:48:24.466543 (Thread-1): Compiling model.order_history.stg_customers
2020-04-28 19:48:24.482836 (Thread-1): Writing injected SQL for node "model.order_history.stg_customers"
2020-04-28 19:48:24.483479 (Thread-1): finished collecting timing info
2020-04-28 19:48:24.525082 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.525234 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2020-04-28 19:48:24.609716 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 19:48:24.614778 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.614930 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:48:24.655585 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 19:48:24.658523 (Thread-1): Writing runtime SQL for node "model.order_history.stg_customers"
2020-04-28 19:48:24.659174 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.659327 (Thread-1): On model.order_history.stg_customers: BEGIN
2020-04-28 19:48:24.699440 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:24.699840 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.700106 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    select
    customer_unique_id,
    email,
    first_name,
    last_name
from ticketing.customers
  );

2020-04-28 19:48:24.760767 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:24.767121 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.767272 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2020-04-28 19:48:24.812598 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:48:24.815617 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.815759 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2020-04-28 19:48:24.859876 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:24.861730 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:48:24.861927 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:24.862086 (Thread-1): On model.order_history.stg_customers: COMMIT
2020-04-28 19:48:25.129006 (Thread-1): SQL status: COMMIT in 0.27 seconds
2020-04-28 19:48:25.132281 (Thread-1): Using postgres connection "model.order_history.stg_customers".
2020-04-28 19:48:25.132452 (Thread-1): On model.order_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2020-04-28 19:48:25.608688 (Thread-1): SQL status: DROP VIEW in 0.48 seconds
2020-04-28 19:48:25.612985 (Thread-1): finished collecting timing info
2020-04-28 19:48:25.613841 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10447eb50>]}
2020-04-28 19:48:25.614162 (Thread-1): 12:48:25 | 1 of 7 OK created view model data_science.stg_customers.............. [CREATE VIEW in 1.15s]
2020-04-28 19:48:25.614350 (Thread-1): Finished running node model.order_history.stg_customers
2020-04-28 19:48:25.614613 (Thread-1): Began running node model.order_history.stg_flash
2020-04-28 19:48:25.615022 (Thread-1): 12:48:25 | 2 of 7 START view model data_science.stg_flash....................... [RUN]
2020-04-28 19:48:25.615652 (Thread-1): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:25.615821 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_customers).
2020-04-28 19:48:25.616008 (Thread-1): Compiling model.order_history.stg_flash
2020-04-28 19:48:25.622088 (Thread-1): Writing injected SQL for node "model.order_history.stg_flash"
2020-04-28 19:48:25.622541 (Thread-1): finished collecting timing info
2020-04-28 19:48:25.629908 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:25.630036 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2020-04-28 19:48:25.799323 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:25.803552 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:25.803711 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:48:25.975498 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:25.978569 (Thread-1): Writing runtime SQL for node "model.order_history.stg_flash"
2020-04-28 19:48:25.979175 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:25.979337 (Thread-1): On model.order_history.stg_flash: BEGIN
2020-04-28 19:48:26.019876 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:26.020175 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.020350 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2020-04-28 19:48:26.082822 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:26.089098 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.089254 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2020-04-28 19:48:26.130126 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:26.134475 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.134631 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2020-04-28 19:48:26.178903 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:26.180108 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:48:26.180240 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.180345 (Thread-1): On model.order_history.stg_flash: COMMIT
2020-04-28 19:48:26.350796 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:48:26.355538 (Thread-1): Using postgres connection "model.order_history.stg_flash".
2020-04-28 19:48:26.355692 (Thread-1): On model.order_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2020-04-28 19:48:26.543384 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:48:26.545948 (Thread-1): finished collecting timing info
2020-04-28 19:48:26.546574 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10451eb10>]}
2020-04-28 19:48:26.546801 (Thread-1): 12:48:26 | 2 of 7 OK created view model data_science.stg_flash.................. [CREATE VIEW in 0.93s]
2020-04-28 19:48:26.546935 (Thread-1): Finished running node model.order_history.stg_flash
2020-04-28 19:48:26.547080 (Thread-1): Began running node model.order_history.stg_order
2020-04-28 19:48:26.547335 (Thread-1): 12:48:26 | 3 of 7 START view model data_science.stg_order....................... [RUN]
2020-04-28 19:48:26.547684 (Thread-1): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.547798 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_flash).
2020-04-28 19:48:26.547902 (Thread-1): Compiling model.order_history.stg_order
2020-04-28 19:48:26.582748 (Thread-1): Writing injected SQL for node "model.order_history.stg_order"
2020-04-28 19:48:26.583235 (Thread-1): finished collecting timing info
2020-04-28 19:48:26.591226 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.591439 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_tmp" cascade
2020-04-28 19:48:26.776841 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:48:26.780948 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.781103 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:48:26.954118 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:26.957108 (Thread-1): Writing runtime SQL for node "model.order_history.stg_order"
2020-04-28 19:48:26.957718 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.957876 (Thread-1): On model.order_history.stg_order: BEGIN
2020-04-28 19:48:26.997142 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:26.997568 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:26.997831 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */

  create view "data_platform_prod"."data_science"."stg_order__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    zone_unique_id,
    pricing_mode_id,
    seat_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
WHERE is_canceled is FALSE -- where shall this condition lives?
  );

2020-04-28 19:48:27.053397 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:27.059525 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:27.059682 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order" rename to "stg_order__dbt_backup"
2020-04-28 19:48:27.102157 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:27.106459 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:27.106616 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
alter table "data_platform_prod"."data_science"."stg_order__dbt_tmp" rename to "stg_order"
2020-04-28 19:48:27.146388 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:27.148483 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:48:27.148679 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:27.148837 (Thread-1): On model.order_history.stg_order: COMMIT
2020-04-28 19:48:27.319941 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:48:27.323320 (Thread-1): Using postgres connection "model.order_history.stg_order".
2020-04-28 19:48:27.323474 (Thread-1): On model.order_history.stg_order: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_order"} */
drop view if exists "data_platform_prod"."data_science"."stg_order__dbt_backup" cascade
2020-04-28 19:48:27.500577 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:48:27.504047 (Thread-1): finished collecting timing info
2020-04-28 19:48:27.504878 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103eeb650>]}
2020-04-28 19:48:27.505172 (Thread-1): 12:48:27 | 3 of 7 OK created view model data_science.stg_order.................. [CREATE VIEW in 0.96s]
2020-04-28 19:48:27.505346 (Thread-1): Finished running node model.order_history.stg_order
2020-04-28 19:48:27.505542 (Thread-1): Began running node model.order_history.stg_events
2020-04-28 19:48:27.505777 (Thread-1): 12:48:27 | 4 of 7 START view model data_science.stg_events...................... [RUN]
2020-04-28 19:48:27.506422 (Thread-1): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.506572 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_order).
2020-04-28 19:48:27.506702 (Thread-1): Compiling model.order_history.stg_events
2020-04-28 19:48:27.514397 (Thread-1): Writing injected SQL for node "model.order_history.stg_events"
2020-04-28 19:48:27.514862 (Thread-1): finished collecting timing info
2020-04-28 19:48:27.522497 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.522650 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2020-04-28 19:48:27.696138 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:27.700290 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.700444 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:48:27.917236 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2020-04-28 19:48:27.918965 (Thread-1): Writing runtime SQL for node "model.order_history.stg_events"
2020-04-28 19:48:27.919381 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.919493 (Thread-1): On model.order_history.stg_events: BEGIN
2020-04-28 19:48:27.958565 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:27.958993 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:27.959262 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    SELECT
    event_unique_id
FROM
    ticketing.events
WHERE event_name NOT ilike 'test event%'
      AND event_name NOT ilike '%base event%'
      AND event_name NOT ilike '% test event%'
      AND event_name NOT ilike '%- RR Base%'
  );

2020-04-28 19:48:28.010639 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:48:28.016020 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:28.016177 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2020-04-28 19:48:28.058788 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:28.063175 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:28.063329 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2020-04-28 19:48:28.109392 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2020-04-28 19:48:28.111216 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:48:28.111418 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:28.111579 (Thread-1): On model.order_history.stg_events: COMMIT
2020-04-28 19:48:28.317864 (Thread-1): SQL status: COMMIT in 0.21 seconds
2020-04-28 19:48:28.321225 (Thread-1): Using postgres connection "model.order_history.stg_events".
2020-04-28 19:48:28.321384 (Thread-1): On model.order_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2020-04-28 19:48:28.496773 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:48:28.499765 (Thread-1): finished collecting timing info
2020-04-28 19:48:28.500641 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10412a310>]}
2020-04-28 19:48:28.500967 (Thread-1): 12:48:28 | 4 of 7 OK created view model data_science.stg_events................. [CREATE VIEW in 0.99s]
2020-04-28 19:48:28.501128 (Thread-1): Finished running node model.order_history.stg_events
2020-04-28 19:48:28.501297 (Thread-1): Began running node model.order_history.customer_broker
2020-04-28 19:48:28.501551 (Thread-1): 12:48:28 | 5 of 7 START view model data_science.customer_broker................. [RUN]
2020-04-28 19:48:28.501885 (Thread-1): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.502003 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.stg_events).
2020-04-28 19:48:28.502117 (Thread-1): Compiling model.order_history.customer_broker
2020-04-28 19:48:28.510646 (Thread-1): Writing injected SQL for node "model.order_history.customer_broker"
2020-04-28 19:48:28.511181 (Thread-1): finished collecting timing info
2020-04-28 19:48:28.519367 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.519551 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_tmp" cascade
2020-04-28 19:48:28.692350 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:28.697750 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.697906 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:48:28.867120 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 19:48:28.869088 (Thread-1): Writing runtime SQL for node "model.order_history.customer_broker"
2020-04-28 19:48:28.869607 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.869735 (Thread-1): On model.order_history.customer_broker: BEGIN
2020-04-28 19:48:28.908462 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:28.908646 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.908746 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */

  create view "data_platform_prod"."data_science"."customer_broker__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),

brokers as (
    SELECT email as broker_email
    FROM analytics.yield_manager_partners
),

final as (
    SELECT 
    customer_unique_id,
    email,
    broker_email,
    first_name,
    last_name
    FROM customers LEFT JOIN brokers on lower(customers.email)=brokers.broker_email
)
select * from final
  );

2020-04-28 19:48:28.961234 (Thread-1): SQL status: CREATE VIEW in 0.05 seconds
2020-04-28 19:48:28.965323 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:28.965482 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
alter table "data_platform_prod"."data_science"."customer_broker__dbt_tmp" rename to "customer_broker"
2020-04-28 19:48:29.005233 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:29.007156 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 19:48:29.007352 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:29.007510 (Thread-1): On model.order_history.customer_broker: COMMIT
2020-04-28 19:48:29.176227 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:48:29.178538 (Thread-1): Using postgres connection "model.order_history.customer_broker".
2020-04-28 19:48:29.178672 (Thread-1): On model.order_history.customer_broker: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customer_broker"} */
drop view if exists "data_platform_prod"."data_science"."customer_broker__dbt_backup" cascade
2020-04-28 19:48:29.385218 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:48:29.388945 (Thread-1): finished collecting timing info
2020-04-28 19:48:29.389760 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104062310>]}
2020-04-28 19:48:29.390022 (Thread-1): 12:48:29 | 5 of 7 OK created view model data_science.customer_broker............ [CREATE VIEW in 0.89s]
2020-04-28 19:48:29.390179 (Thread-1): Finished running node model.order_history.customer_broker
2020-04-28 19:48:29.390339 (Thread-1): Began running node model.order_history.order_flash
2020-04-28 19:48:29.390497 (Thread-1): 12:48:29 | 6 of 7 START view model data_science.order_flash..................... [RUN]
2020-04-28 19:48:29.390918 (Thread-1): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.391036 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.customer_broker).
2020-04-28 19:48:29.391147 (Thread-1): Compiling model.order_history.order_flash
2020-04-28 19:48:29.399959 (Thread-1): Writing injected SQL for node "model.order_history.order_flash"
2020-04-28 19:48:29.400379 (Thread-1): finished collecting timing info
2020-04-28 19:48:29.408072 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.408223 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_tmp" cascade
2020-04-28 19:48:29.587853 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2020-04-28 19:48:29.592376 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.592550 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:48:29.785502 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2020-04-28 19:48:29.787421 (Thread-1): Writing runtime SQL for node "model.order_history.order_flash"
2020-04-28 19:48:29.787900 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.788024 (Thread-1): On model.order_history.order_flash: BEGIN
2020-04-28 19:48:29.827283 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:29.827471 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.827573 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */

  create view "data_platform_prod"."data_science"."order_flash__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    sale_datetime,
    pricing_mode_id,
    transfer_action_id,
    ticket_id,
    ticket_state
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
)
select * from final
  );

2020-04-28 19:48:29.882873 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:29.887132 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.887285 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
alter table "data_platform_prod"."data_science"."order_flash__dbt_tmp" rename to "order_flash"
2020-04-28 19:48:29.927605 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:29.929556 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:48:29.929755 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:29.929915 (Thread-1): On model.order_history.order_flash: COMMIT
2020-04-28 19:48:30.099456 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 19:48:30.102845 (Thread-1): Using postgres connection "model.order_history.order_flash".
2020-04-28 19:48:30.102996 (Thread-1): On model.order_history.order_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.order_flash"} */
drop view if exists "data_platform_prod"."data_science"."order_flash__dbt_backup" cascade
2020-04-28 19:48:30.308956 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:48:30.313159 (Thread-1): finished collecting timing info
2020-04-28 19:48:30.313994 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041a9a50>]}
2020-04-28 19:48:30.314296 (Thread-1): 12:48:30 | 6 of 7 OK created view model data_science.order_flash................ [CREATE VIEW in 0.92s]
2020-04-28 19:48:30.314476 (Thread-1): Finished running node model.order_history.order_flash
2020-04-28 19:48:30.314918 (Thread-1): Began running node model.order_history.customers
2020-04-28 19:48:30.315140 (Thread-1): 12:48:30 | 7 of 7 START view model data_science.customers....................... [RUN]
2020-04-28 19:48:30.315489 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 19:48:30.315666 (Thread-1): Re-using an available connection from the pool (formerly model.order_history.order_flash).
2020-04-28 19:48:30.315832 (Thread-1): Compiling model.order_history.customers
2020-04-28 19:48:30.325997 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 19:48:30.326407 (Thread-1): finished collecting timing info
2020-04-28 19:48:30.333677 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:30.333814 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 19:48:30.778695 (Thread-1): SQL status: DROP VIEW in 0.44 seconds
2020-04-28 19:48:30.782519 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:30.782718 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:48:30.993952 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2020-04-28 19:48:30.996695 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 19:48:30.997372 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:30.997528 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 19:48:31.059682 (Thread-1): SQL status: BEGIN in 0.06 seconds
2020-04-28 19:48:31.060137 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:31.060313 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 19:48:31.118989 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 19:48:31.122144 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:31.122407 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 19:48:31.162662 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 19:48:31.163933 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:48:31.164092 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:31.164191 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 19:48:31.341501 (Thread-1): SQL status: COMMIT in 0.18 seconds
2020-04-28 19:48:31.345050 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 19:48:31.345209 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 19:48:31.872539 (Thread-1): SQL status: DROP VIEW in 0.53 seconds
2020-04-28 19:48:31.876610 (Thread-1): finished collecting timing info
2020-04-28 19:48:31.877456 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9f8341b-80ec-4ac7-b575-25658cf01c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104448850>]}
2020-04-28 19:48:31.877761 (Thread-1): 12:48:31 | 7 of 7 OK created view model data_science.customers.................. [CREATE VIEW in 1.56s]
2020-04-28 19:48:31.877944 (Thread-1): Finished running node model.order_history.customers
2020-04-28 19:48:31.948037 (MainThread): Using postgres connection "master".
2020-04-28 19:48:31.948382 (MainThread): On master: BEGIN
2020-04-28 19:48:31.989082 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 19:48:31.989538 (MainThread): On master: COMMIT
2020-04-28 19:48:31.989828 (MainThread): Using postgres connection "master".
2020-04-28 19:48:31.989983 (MainThread): On master: COMMIT
2020-04-28 19:48:32.030554 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 19:48:32.031494 (MainThread): 12:48:32 | 
2020-04-28 19:48:32.031733 (MainThread): 12:48:32 | Finished running 7 view models in 11.61s.
2020-04-28 19:48:32.031926 (MainThread): Connection 'master' was left open.
2020-04-28 19:48:32.032081 (MainThread): On master: Close
2020-04-28 19:48:32.032530 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 19:48:32.032702 (MainThread): On model.order_history.customers: Close
2020-04-28 19:48:32.054468 (MainThread): 
2020-04-28 19:48:32.054682 (MainThread): Completed successfully
2020-04-28 19:48:32.054828 (MainThread): 
Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
2020-04-28 19:48:32.055027 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104141d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041bd610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104099350>]}
2020-04-28 19:48:32.055251 (MainThread): Flushing usage events
2020-04-28 20:55:15.567732 (MainThread): Running with dbt=0.16.1
2020-04-28 20:55:15.642929 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 20:55:15.643745 (MainThread): Tracking: tracking
2020-04-28 20:55:15.649325 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10909f110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092e8bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086e6d50>]}
2020-04-28 20:55:15.668859 (MainThread): Partial parsing not enabled
2020-04-28 20:55:15.672321 (MainThread): Parsing macros/core.sql
2020-04-28 20:55:15.677106 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 20:55:15.685510 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 20:55:15.687371 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 20:55:15.705919 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 20:55:15.740088 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 20:55:15.762003 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 20:55:15.764511 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 20:55:15.771766 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 20:55:15.785533 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 20:55:15.792797 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 20:55:15.800095 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 20:55:15.805244 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 20:55:15.806181 (MainThread): Parsing macros/etc/query.sql
2020-04-28 20:55:15.807571 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 20:55:15.809545 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 20:55:15.811990 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 20:55:15.822007 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 20:55:15.824056 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 20:55:15.825106 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 20:55:15.866280 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 20:55:15.867944 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 20:55:15.869188 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 20:55:15.870733 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 20:55:15.872954 (MainThread): Parsing macros/catalog.sql
2020-04-28 20:55:15.875240 (MainThread): Parsing macros/relations.sql
2020-04-28 20:55:15.876572 (MainThread): Parsing macros/adapters.sql
2020-04-28 20:55:15.894869 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 20:55:15.919034 (MainThread): Partial parsing not enabled
2020-04-28 20:55:15.946944 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:55:15.947061 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.962511 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 20:55:15.962598 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.966500 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 20:55:15.966584 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.970865 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 20:55:15.970953 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.974894 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 20:55:15.974982 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.980293 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 20:55:15.980399 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:15.985560 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 20:55:15.985653 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:16.130038 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 20:55:16.133363 (MainThread): 
2020-04-28 20:55:16.133757 (MainThread): Acquiring new postgres connection "master".
2020-04-28 20:55:16.133843 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:55:16.138768 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 20:55:16.138924 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 20:55:16.224565 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 20:55:16.224744 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 20:55:16.783610 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.56 seconds
2020-04-28 20:55:16.820701 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:55:16.820936 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 20:55:16.822562 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:55:16.822673 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 20:55:16.864723 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:55:16.865173 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:55:16.865349 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 20:55:16.978642 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 20:55:16.987788 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 20:55:17.079473 (MainThread): Using postgres connection "master".
2020-04-28 20:55:17.079624 (MainThread): On master: BEGIN
2020-04-28 20:55:17.414736 (MainThread): SQL status: BEGIN in 0.33 seconds
2020-04-28 20:55:17.415151 (MainThread): Using postgres connection "master".
2020-04-28 20:55:17.415419 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 20:55:17.543593 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 20:55:17.630501 (MainThread): On master: ROLLBACK
2020-04-28 20:55:17.667128 (MainThread): Using postgres connection "master".
2020-04-28 20:55:17.667529 (MainThread): On master: BEGIN
2020-04-28 20:55:17.741542 (MainThread): SQL status: BEGIN in 0.07 seconds
2020-04-28 20:55:17.741991 (MainThread): On master: COMMIT
2020-04-28 20:55:17.742298 (MainThread): Using postgres connection "master".
2020-04-28 20:55:17.742465 (MainThread): On master: COMMIT
2020-04-28 20:55:17.778817 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 20:55:17.779701 (MainThread): 13:55:17 | Concurrency: 1 threads (target='dev')
2020-04-28 20:55:17.779944 (MainThread): 13:55:17 | 
2020-04-28 20:55:17.782070 (Thread-1): Began running node model.order_history.customers
2020-04-28 20:55:17.782335 (Thread-1): 13:55:17 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 20:55:17.782900 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:55:17.783042 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 20:55:17.783184 (Thread-1): Compiling model.order_history.customers
2020-04-28 20:55:17.802766 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 20:55:17.803295 (Thread-1): finished collecting timing info
2020-04-28 20:55:17.842551 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:55:17.842715 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 20:55:17.926970 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 20:55:17.930500 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:55:17.930681 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 20:55:17.972835 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 20:55:17.976709 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 20:55:17.978627 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:55:17.978986 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 20:55:18.020685 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:55:18.021129 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:55:18.021421 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."stg_customers"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,

        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        CASE WHEN yield_manager_partners.email is not null THEN 1 ELSE 0 END AS is_broker,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 20:55:18.073093 (Thread-1): Postgres error: relation "yield_manager_partners" does not exist

2020-04-28 20:55:18.073504 (Thread-1): On model.order_history.customers: ROLLBACK
2020-04-28 20:55:18.115656 (Thread-1): finished collecting timing info
2020-04-28 20:55:18.116691 (Thread-1): Database Error in model customers (models/customers.sql)
  relation "yield_manager_partners" does not exist
  compiled SQL at target/run/order_history/customers.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "yield_manager_partners" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 60, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model customers (models/customers.sql)
  relation "yield_manager_partners" does not exist
  compiled SQL at target/run/order_history/customers.sql
2020-04-28 20:55:18.119731 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8bd1e3d3-af73-479a-b71a-5d75e9ea201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098c3590>]}
2020-04-28 20:55:18.120034 (Thread-1): 13:55:18 | 1 of 1 ERROR creating view model data_science.customers.............. [ERROR in 0.34s]
2020-04-28 20:55:18.120212 (Thread-1): Finished running node model.order_history.customers
2020-04-28 20:55:18.195817 (MainThread): Using postgres connection "master".
2020-04-28 20:55:18.196140 (MainThread): On master: BEGIN
2020-04-28 20:55:18.233372 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:55:18.233605 (MainThread): On master: COMMIT
2020-04-28 20:55:18.233738 (MainThread): Using postgres connection "master".
2020-04-28 20:55:18.233857 (MainThread): On master: COMMIT
2020-04-28 20:55:18.269525 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 20:55:18.270130 (MainThread): 13:55:18 | 
2020-04-28 20:55:18.270355 (MainThread): 13:55:18 | Finished running 1 view model in 2.14s.
2020-04-28 20:55:18.270555 (MainThread): Connection 'master' was left open.
2020-04-28 20:55:18.270709 (MainThread): On master: Close
2020-04-28 20:55:18.271081 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 20:55:18.271247 (MainThread): On model.order_history.customers: Close
2020-04-28 20:55:18.275879 (MainThread): 
2020-04-28 20:55:18.276046 (MainThread): Completed with 1 error and 0 warnings:
2020-04-28 20:55:18.276166 (MainThread): 
2020-04-28 20:55:18.276279 (MainThread): Database Error in model customers (models/customers.sql)
2020-04-28 20:55:18.276381 (MainThread):   relation "yield_manager_partners" does not exist
2020-04-28 20:55:18.276477 (MainThread):   compiled SQL at target/run/order_history/customers.sql
2020-04-28 20:55:18.276575 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2020-04-28 20:55:18.276747 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109602050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093a2110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098acad0>]}
2020-04-28 20:55:18.276933 (MainThread): Flushing usage events
2020-04-28 20:56:09.488253 (MainThread): Running with dbt=0.16.1
2020-04-28 20:56:09.561554 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 20:56:09.562404 (MainThread): Tracking: tracking
2020-04-28 20:56:09.569014 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a2da50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a36fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a36a50>]}
2020-04-28 20:56:09.596215 (MainThread): Partial parsing not enabled
2020-04-28 20:56:09.598110 (MainThread): Parsing macros/core.sql
2020-04-28 20:56:09.603232 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 20:56:09.611956 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 20:56:09.613816 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 20:56:09.633024 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 20:56:09.668764 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 20:56:09.691667 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 20:56:09.693673 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 20:56:09.700660 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 20:56:09.714136 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 20:56:09.721239 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 20:56:09.727872 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 20:56:09.733165 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 20:56:09.734182 (MainThread): Parsing macros/etc/query.sql
2020-04-28 20:56:09.735318 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 20:56:09.737084 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 20:56:09.739327 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 20:56:09.748879 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 20:56:09.751016 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 20:56:09.752165 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 20:56:09.795252 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 20:56:09.796475 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 20:56:09.797439 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 20:56:09.798578 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 20:56:09.800893 (MainThread): Parsing macros/catalog.sql
2020-04-28 20:56:09.803473 (MainThread): Parsing macros/relations.sql
2020-04-28 20:56:09.804888 (MainThread): Parsing macros/adapters.sql
2020-04-28 20:56:09.823590 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 20:56:09.841643 (MainThread): Partial parsing not enabled
2020-04-28 20:56:09.869755 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:56:09.869864 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.886959 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 20:56:09.887088 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.891649 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 20:56:09.891755 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.896629 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 20:56:09.896738 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.901472 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 20:56:09.901619 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.908161 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 20:56:09.908300 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.915663 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 20:56:09.915775 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:09.958557 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d5d5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e6f110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e6f890>]}
2020-04-28 20:56:09.958764 (MainThread): Flushing usage events
2020-04-28 20:56:10.266963 (MainThread): Connection 'model.order_history.order_flash' was properly closed.
2020-04-28 20:56:10.267194 (MainThread): Encountered an error:
2020-04-28 20:56:10.267398 (MainThread): Compilation Error in model customers (models/customers.sql)
  Model 'model.order_history.customers' depends on a node named 'stg_customer_broker' which was not found or is disabled
2020-04-28 20:56:10.269580 (MainThread): Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 302, in process_manifest
    process_refs(manifest, project_name)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 553, in process_refs
    _process_refs_for_node(manifest, current_project, node)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/parser/manifest.py", line 534, in _process_refs_for_node
    disabled=(isinstance(target_model, Disabled))
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/utils.py", line 338, in invalid_ref_fail_unless_test
    target_model_package)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 488, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model customers (models/customers.sql)
  Model 'model.order_history.customers' depends on a node named 'stg_customer_broker' which was not found or is disabled

2020-04-28 20:56:20.018625 (MainThread): Running with dbt=0.16.1
2020-04-28 20:56:20.082011 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['customers'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2020-04-28 20:56:20.082799 (MainThread): Tracking: tracking
2020-04-28 20:56:20.087726 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fdce10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d48690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d84810>]}
2020-04-28 20:56:20.105919 (MainThread): Partial parsing not enabled
2020-04-28 20:56:20.107752 (MainThread): Parsing macros/core.sql
2020-04-28 20:56:20.112305 (MainThread): Parsing macros/materializations/helpers.sql
2020-04-28 20:56:20.120259 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2020-04-28 20:56:20.122004 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2020-04-28 20:56:20.139880 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2020-04-28 20:56:20.172677 (MainThread): Parsing macros/materializations/seed/seed.sql
2020-04-28 20:56:20.193376 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2020-04-28 20:56:20.195295 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2020-04-28 20:56:20.201480 (MainThread): Parsing macros/materializations/common/merge.sql
2020-04-28 20:56:20.214178 (MainThread): Parsing macros/materializations/table/table.sql
2020-04-28 20:56:20.220886 (MainThread): Parsing macros/materializations/view/view.sql
2020-04-28 20:56:20.227044 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2020-04-28 20:56:20.231931 (MainThread): Parsing macros/etc/get_custom_alias.sql
2020-04-28 20:56:20.232871 (MainThread): Parsing macros/etc/query.sql
2020-04-28 20:56:20.233934 (MainThread): Parsing macros/etc/is_incremental.sql
2020-04-28 20:56:20.235583 (MainThread): Parsing macros/etc/get_relation_comment.sql
2020-04-28 20:56:20.237667 (MainThread): Parsing macros/etc/datetime.sql
2020-04-28 20:56:20.247472 (MainThread): Parsing macros/etc/get_custom_schema.sql
2020-04-28 20:56:20.249517 (MainThread): Parsing macros/etc/get_custom_database.sql
2020-04-28 20:56:20.250590 (MainThread): Parsing macros/adapters/common.sql
2020-04-28 20:56:20.300748 (MainThread): Parsing macros/schema_tests/relationships.sql
2020-04-28 20:56:20.302038 (MainThread): Parsing macros/schema_tests/not_null.sql
2020-04-28 20:56:20.303040 (MainThread): Parsing macros/schema_tests/unique.sql
2020-04-28 20:56:20.304229 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2020-04-28 20:56:20.306652 (MainThread): Parsing macros/catalog.sql
2020-04-28 20:56:20.309081 (MainThread): Parsing macros/relations.sql
2020-04-28 20:56:20.310492 (MainThread): Parsing macros/adapters.sql
2020-04-28 20:56:20.328185 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2020-04-28 20:56:20.346051 (MainThread): Partial parsing not enabled
2020-04-28 20:56:20.373349 (MainThread): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:56:20.373444 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.389357 (MainThread): Acquiring new postgres connection "model.order_history.stg_customers".
2020-04-28 20:56:20.389451 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.393394 (MainThread): Acquiring new postgres connection "model.order_history.stg_flash".
2020-04-28 20:56:20.393483 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.397794 (MainThread): Acquiring new postgres connection "model.order_history.stg_order".
2020-04-28 20:56:20.397882 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.401777 (MainThread): Acquiring new postgres connection "model.order_history.stg_events".
2020-04-28 20:56:20.401864 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.406309 (MainThread): Acquiring new postgres connection "model.order_history.customer_broker".
2020-04-28 20:56:20.406406 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.411330 (MainThread): Acquiring new postgres connection "model.order_history.order_flash".
2020-04-28 20:56:20.411420 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.546536 (MainThread): Found 7 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2020-04-28 20:56:20.550515 (MainThread): 
2020-04-28 20:56:20.550842 (MainThread): Acquiring new postgres connection "master".
2020-04-28 20:56:20.550930 (MainThread): Opening a new connection, currently in state init
2020-04-28 20:56:20.555332 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2020-04-28 20:56:20.555515 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2020-04-28 20:56:20.648127 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2020-04-28 20:56:20.648258 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2020-04-28 20:56:21.067699 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.42 seconds
2020-04-28 20:56:21.099522 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:56:21.099659 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2020-04-28 20:56:21.101105 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:56:21.101208 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2020-04-28 20:56:21.140395 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:56:21.140819 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2020-04-28 20:56:21.141087 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2020-04-28 20:56:21.255303 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.11 seconds
2020-04-28 20:56:21.263258 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2020-04-28 20:56:21.328758 (MainThread): Using postgres connection "master".
2020-04-28 20:56:21.328880 (MainThread): On master: BEGIN
2020-04-28 20:56:21.726160 (MainThread): SQL status: BEGIN in 0.40 seconds
2020-04-28 20:56:21.726576 (MainThread): Using postgres connection "master".
2020-04-28 20:56:21.726845 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2020-04-28 20:56:21.858287 (MainThread): SQL status: SELECT in 0.13 seconds
2020-04-28 20:56:21.937615 (MainThread): On master: ROLLBACK
2020-04-28 20:56:21.976872 (MainThread): Using postgres connection "master".
2020-04-28 20:56:21.977037 (MainThread): On master: BEGIN
2020-04-28 20:56:22.054044 (MainThread): SQL status: BEGIN in 0.08 seconds
2020-04-28 20:56:22.054494 (MainThread): On master: COMMIT
2020-04-28 20:56:22.054789 (MainThread): Using postgres connection "master".
2020-04-28 20:56:22.054970 (MainThread): On master: COMMIT
2020-04-28 20:56:22.092849 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 20:56:22.093727 (MainThread): 13:56:22 | Concurrency: 1 threads (target='dev')
2020-04-28 20:56:22.093973 (MainThread): 13:56:22 | 
2020-04-28 20:56:22.096238 (Thread-1): Began running node model.order_history.customers
2020-04-28 20:56:22.096495 (Thread-1): 13:56:22 | 1 of 1 START view model data_science.customers....................... [RUN]
2020-04-28 20:56:22.097068 (Thread-1): Acquiring new postgres connection "model.order_history.customers".
2020-04-28 20:56:22.097193 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2020-04-28 20:56:22.097323 (Thread-1): Compiling model.order_history.customers
2020-04-28 20:56:22.116402 (Thread-1): Writing injected SQL for node "model.order_history.customers"
2020-04-28 20:56:22.116959 (Thread-1): finished collecting timing info
2020-04-28 20:56:22.155322 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.155481 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_tmp" cascade
2020-04-28 20:56:22.234245 (Thread-1): SQL status: DROP VIEW in 0.08 seconds
2020-04-28 20:56:22.238569 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.238725 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 20:56:22.277430 (Thread-1): SQL status: DROP VIEW in 0.04 seconds
2020-04-28 20:56:22.279954 (Thread-1): Writing runtime SQL for node "model.order_history.customers"
2020-04-28 20:56:22.280438 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.280561 (Thread-1): On model.order_history.customers: BEGIN
2020-04-28 20:56:22.318694 (Thread-1): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:56:22.318873 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.318971 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */

  create view "data_platform_prod"."data_science"."customers__dbt_tmp" as (
    with customers as (
    select * from "data_platform_prod"."data_science"."customer_broker"
),
order_flash as (
    select * from "data_platform_prod"."data_science"."order_flash"
),

customer_orders as (
    select
        customer_unique_id,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,

        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        SUM(DISTINCT amount_gross) AS total_revenue,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers

    from order_flash
    group by 1
),
final as (
    select
        customers.customer_unique_id,
        customers.email,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        CASE WHEN broker_email is not null THEN 1 ELSE 0 END AS is_broker,
        coalesce(customer_orders.tickets_sold_no_comps, 0) as tickets_sold_no_comps,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        coalesce(customer_orders.number_of_tickets_sold, 0) as number_of_tickets_sold,
        coalesce(customer_orders.total_revenue, 0) as total_revenue,
        coalesce(customer_orders.count_transferred_tickets, 0) as count_transferred_tickets,
        coalesce(customer_orders.count_transfers, 0) as count_transfers
    from customers
    left join customer_orders using (customer_unique_id)
)
select * from final
  );

2020-04-28 20:56:22.376459 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2020-04-28 20:56:22.382724 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.382888 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers" rename to "customers__dbt_backup"
2020-04-28 20:56:22.423256 (Thread-1): SQL status: ALTER TABLE in 0.04 seconds
2020-04-28 20:56:22.427543 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.427700 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
alter table "data_platform_prod"."data_science"."customers__dbt_tmp" rename to "customers"
2020-04-28 20:56:22.494469 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2020-04-28 20:56:22.496428 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 20:56:22.496624 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.496784 (Thread-1): On model.order_history.customers: COMMIT
2020-04-28 20:56:22.665981 (Thread-1): SQL status: COMMIT in 0.17 seconds
2020-04-28 20:56:22.667917 (Thread-1): Using postgres connection "model.order_history.customers".
2020-04-28 20:56:22.668042 (Thread-1): On model.order_history.customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.order_history.customers"} */
drop view if exists "data_platform_prod"."data_science"."customers__dbt_backup" cascade
2020-04-28 20:56:22.842050 (Thread-1): SQL status: DROP VIEW in 0.17 seconds
2020-04-28 20:56:22.846271 (Thread-1): finished collecting timing info
2020-04-28 20:56:22.847123 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7656adda-eb75-4c0b-be1e-b8034901c4d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051d0090>]}
2020-04-28 20:56:22.847434 (Thread-1): 13:56:22 | 1 of 1 OK created view model data_science.customers.................. [CREATE VIEW in 0.75s]
2020-04-28 20:56:22.847622 (Thread-1): Finished running node model.order_history.customers
2020-04-28 20:56:22.921722 (MainThread): Using postgres connection "master".
2020-04-28 20:56:22.921970 (MainThread): On master: BEGIN
2020-04-28 20:56:22.960899 (MainThread): SQL status: BEGIN in 0.04 seconds
2020-04-28 20:56:22.961218 (MainThread): On master: COMMIT
2020-04-28 20:56:22.961400 (MainThread): Using postgres connection "master".
2020-04-28 20:56:22.961575 (MainThread): On master: COMMIT
2020-04-28 20:56:22.999429 (MainThread): SQL status: COMMIT in 0.04 seconds
2020-04-28 20:56:23.000335 (MainThread): 13:56:23 | 
2020-04-28 20:56:23.000575 (MainThread): 13:56:23 | Finished running 1 view model in 2.45s.
2020-04-28 20:56:23.000778 (MainThread): Connection 'master' was left open.
2020-04-28 20:56:23.000933 (MainThread): On master: Close
2020-04-28 20:56:23.001316 (MainThread): Connection 'model.order_history.customers' was left open.
2020-04-28 20:56:23.001476 (MainThread): On model.order_history.customers: Close
2020-04-28 20:56:23.008068 (MainThread): 
2020-04-28 20:56:23.008308 (MainThread): Completed successfully
2020-04-28 20:56:23.008485 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2020-04-28 20:56:23.008729 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053add90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10558f2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105583750>]}
2020-04-28 20:56:23.008976 (MainThread): Flushing usage events
