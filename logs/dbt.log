2021-08-05 17:19:13.552017 (MainThread): Running with dbt=0.16.1
2021-08-05 17:19:13.624426 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-08-05 17:19:13.625522 (MainThread): Tracking: tracking
2021-08-05 17:19:13.632038 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e47fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090ae390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090aec90>]}
2021-08-05 17:19:13.654312 (MainThread): Partial parsing not enabled
2021-08-05 17:19:13.657117 (MainThread): Parsing macros/core.sql
2021-08-05 17:19:13.662813 (MainThread): Parsing macros/materializations/helpers.sql
2021-08-05 17:19:13.671582 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-08-05 17:19:13.673639 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-08-05 17:19:13.692930 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-08-05 17:19:13.728397 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-08-05 17:19:13.751585 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-08-05 17:19:13.753865 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-08-05 17:19:13.760773 (MainThread): Parsing macros/materializations/common/merge.sql
2021-08-05 17:19:13.774656 (MainThread): Parsing macros/materializations/table/table.sql
2021-08-05 17:19:13.782120 (MainThread): Parsing macros/materializations/view/view.sql
2021-08-05 17:19:13.789115 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-08-05 17:19:13.794732 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-08-05 17:19:13.795901 (MainThread): Parsing macros/etc/query.sql
2021-08-05 17:19:13.797210 (MainThread): Parsing macros/etc/is_incremental.sql
2021-08-05 17:19:13.799201 (MainThread): Parsing macros/etc/get_relation_comment.sql
2021-08-05 17:19:13.801609 (MainThread): Parsing macros/etc/datetime.sql
2021-08-05 17:19:13.811793 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-08-05 17:19:13.814175 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-08-05 17:19:13.815733 (MainThread): Parsing macros/adapters/common.sql
2021-08-05 17:19:13.860052 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-08-05 17:19:13.861718 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-08-05 17:19:13.862927 (MainThread): Parsing macros/schema_tests/unique.sql
2021-08-05 17:19:13.864271 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-08-05 17:19:13.866923 (MainThread): Parsing macros/catalog.sql
2021-08-05 17:19:13.869719 (MainThread): Parsing macros/relations.sql
2021-08-05 17:19:13.871314 (MainThread): Parsing macros/adapters.sql
2021-08-05 17:19:13.889624 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2021-08-05 17:19:13.909039 (MainThread): Partial parsing not enabled
2021-08-05 17:19:13.939113 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 17:19:13.939245 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.956227 (MainThread): Acquiring new postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 17:19:13.956350 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.965047 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:13.965149 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.970009 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:13.970117 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.974846 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:13.974945 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.979541 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:13.979638 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.983967 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:13.984063 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.988652 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:13.988759 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.993539 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:13.993637 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:13.998530 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:19:13.998629 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.005695 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:14.005796 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.013342 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:19:14.013450 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.020532 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:14.020640 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.028187 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:19:14.028290 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.035484 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:14.035588 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.043442 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 17:19:14.043547 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.050579 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:14.050694 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.232724 (MainThread): Found 17 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2021-08-05 17:19:14.239247 (MainThread): 
2021-08-05 17:19:14.239554 (MainThread): Acquiring new postgres connection "master".
2021-08-05 17:19:14.239643 (MainThread): Opening a new connection, currently in state init
2021-08-05 17:19:14.291375 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2021-08-05 17:19:14.291518 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-08-05 17:19:14.382109 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2021-08-05 17:19:14.382233 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2021-08-05 17:19:15.009876 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.63 seconds
2021-08-05 17:19:15.064510 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2021-08-05 17:19:15.064697 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2021-08-05 17:19:15.066101 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2021-08-05 17:19:15.066210 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2021-08-05 17:19:15.115232 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:15.115385 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2021-08-05 17:19:15.115466 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2021-08-05 17:19:15.193951 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.08 seconds
2021-08-05 17:19:15.220547 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2021-08-05 17:19:15.337025 (MainThread): Using postgres connection "master".
2021-08-05 17:19:15.337156 (MainThread): On master: BEGIN
2021-08-05 17:19:15.870684 (MainThread): SQL status: BEGIN in 0.53 seconds
2021-08-05 17:19:15.870951 (MainThread): Using postgres connection "master".
2021-08-05 17:19:15.871050 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2021-08-05 17:19:16.026927 (MainThread): SQL status: SELECT in 0.16 seconds
2021-08-05 17:19:16.267300 (MainThread): On master: ROLLBACK
2021-08-05 17:19:16.316600 (MainThread): Using postgres connection "master".
2021-08-05 17:19:16.316761 (MainThread): On master: BEGIN
2021-08-05 17:19:16.411495 (MainThread): SQL status: BEGIN in 0.09 seconds
2021-08-05 17:19:16.411664 (MainThread): On master: COMMIT
2021-08-05 17:19:16.411754 (MainThread): Using postgres connection "master".
2021-08-05 17:19:16.411831 (MainThread): On master: COMMIT
2021-08-05 17:19:16.464081 (MainThread): SQL status: COMMIT in 0.05 seconds
2021-08-05 17:19:16.464475 (MainThread): 10:19:16 | Concurrency: 1 threads (target='dev')
2021-08-05 17:19:16.464619 (MainThread): 10:19:16 | 
2021-08-05 17:19:16.466723 (Thread-1): Began running node model.customer_history.stg_events
2021-08-05 17:19:16.466920 (Thread-1): 10:19:16 | 1 of 17 START view model data_science.stg_events..................... [RUN]
2021-08-05 17:19:16.467228 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:16.467330 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2021-08-05 17:19:16.467436 (Thread-1): Compiling model.customer_history.stg_events
2021-08-05 17:19:16.481252 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2021-08-05 17:19:16.481949 (Thread-1): finished collecting timing info
2021-08-05 17:19:16.519264 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:16.519404 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2021-08-05 17:19:16.612955 (Thread-1): SQL status: DROP VIEW in 0.09 seconds
2021-08-05 17:19:16.615299 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:16.615400 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2021-08-05 17:19:16.664812 (Thread-1): SQL status: DROP VIEW in 0.05 seconds
2021-08-05 17:19:16.666842 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2021-08-05 17:19:16.667428 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:16.667522 (Thread-1): On model.customer_history.stg_events: BEGIN
2021-08-05 17:19:16.712302 (Thread-1): SQL status: BEGIN in 0.04 seconds
2021-08-05 17:19:16.712459 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:16.712544 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    with events as (
    SELECT
        event_unique_id,
        onsale_date,
        event_datetime,
        venue_unique_id,
        major_category_name
    FROM
        ticketing.events
        INNER JOIN analytics.event_onsale USING (event_unique_id)
        LEFT JOIN analytics.mdl_major_category_event USING (event_unique_id)
    WHERE event_name NOT ilike 'test event%'
        AND event_name NOT ilike '%base event%'
        AND event_name NOT ilike '% test event%'
        AND event_name NOT ilike '%- RR Base%'
        AND (nvl(ticketing.events.is_exclude,false)) is false
),
venues as (
    SELECT
        venue_unique_id,
        left(venue_zip, 5) as venue_zip,
        venue_type
        from ticketing.venues LEFT JOIN data_science.venue_type
        USING (venue_unique_id)
),
final as (
    SELECT
        *
    FROM events INNER JOIN venues USING (venue_unique_id)
)
SELECT * FROM final
  );

2021-08-05 17:19:16.779916 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2021-08-05 17:19:16.783359 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:16.783453 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2021-08-05 17:19:16.835554 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:16.837806 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:16.837898 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2021-08-05 17:19:16.890034 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:16.890932 (Thread-1): On model.customer_history.stg_events: COMMIT
2021-08-05 17:19:16.891026 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:16.891100 (Thread-1): On model.customer_history.stg_events: COMMIT
2021-08-05 17:19:17.089295 (Thread-1): SQL status: COMMIT in 0.20 seconds
2021-08-05 17:19:17.091297 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 17:19:17.091402 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2021-08-05 17:19:17.323386 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 17:19:17.325737 (Thread-1): finished collecting timing info
2021-08-05 17:19:17.326611 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109278090>]}
2021-08-05 17:19:17.326994 (Thread-1): 10:19:17 | 1 of 17 OK created view model data_science.stg_events................ [CREATE VIEW in 0.86s]
2021-08-05 17:19:17.327110 (Thread-1): Finished running node model.customer_history.stg_events
2021-08-05 17:19:17.327224 (Thread-1): Began running node model.customer_history.stg_flash
2021-08-05 17:19:17.327424 (Thread-1): 10:19:17 | 2 of 17 START view model data_science.stg_flash...................... [RUN]
2021-08-05 17:19:17.327790 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:17.327868 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2021-08-05 17:19:17.328027 (Thread-1): Compiling model.customer_history.stg_flash
2021-08-05 17:19:17.333027 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2021-08-05 17:19:17.333483 (Thread-1): finished collecting timing info
2021-08-05 17:19:17.339785 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:17.339888 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2021-08-05 17:19:17.533537 (Thread-1): SQL status: DROP VIEW in 0.19 seconds
2021-08-05 17:19:17.536799 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:17.536915 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2021-08-05 17:19:17.720880 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2021-08-05 17:19:17.722346 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2021-08-05 17:19:17.723046 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:17.723145 (Thread-1): On model.customer_history.stg_flash: BEGIN
2021-08-05 17:19:17.771025 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:17.771192 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:17.771270 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2021-08-05 17:19:17.850832 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2021-08-05 17:19:17.854340 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:17.854436 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2021-08-05 17:19:17.902814 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:17.905166 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:17.905289 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2021-08-05 17:19:17.950529 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:17.951426 (Thread-1): On model.customer_history.stg_flash: COMMIT
2021-08-05 17:19:17.951520 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:17.951593 (Thread-1): On model.customer_history.stg_flash: COMMIT
2021-08-05 17:19:18.159579 (Thread-1): SQL status: COMMIT in 0.21 seconds
2021-08-05 17:19:18.162188 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 17:19:18.162286 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2021-08-05 17:19:18.370550 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 17:19:18.372982 (Thread-1): finished collecting timing info
2021-08-05 17:19:18.373525 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109523510>]}
2021-08-05 17:19:18.373715 (Thread-1): 10:19:18 | 2 of 17 OK created view model data_science.stg_flash................. [CREATE VIEW in 1.05s]
2021-08-05 17:19:18.373825 (Thread-1): Finished running node model.customer_history.stg_flash
2021-08-05 17:19:18.373937 (Thread-1): Began running node model.customer_history.stg_customers
2021-08-05 17:19:18.374082 (Thread-1): 10:19:18 | 3 of 17 START view model data_science.stg_customers.................. [RUN]
2021-08-05 17:19:18.374532 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:18.374782 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2021-08-05 17:19:18.374884 (Thread-1): Compiling model.customer_history.stg_customers
2021-08-05 17:19:18.379511 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2021-08-05 17:19:18.379949 (Thread-1): finished collecting timing info
2021-08-05 17:19:18.385912 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:18.386016 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2021-08-05 17:19:18.596170 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 17:19:18.599205 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:18.599323 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2021-08-05 17:19:18.817952 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 17:19:18.819447 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2021-08-05 17:19:18.820124 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:18.820213 (Thread-1): On model.customer_history.stg_customers: BEGIN
2021-08-05 17:19:18.890005 (Thread-1): SQL status: BEGIN in 0.07 seconds
2021-08-05 17:19:18.890210 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:18.890291 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    SELECT 
    axs_customer_id as customer_unique_id,
    axs_email_hash,
    -- left(zip, 5) as zip -- eleminate situation as 01234-1234
    zip_code as zip
FROM analytics.demographics_all -- instead of ticketing.customers

--  no need to join SQL at this moment
--     CASE WHEN b.email is not null THEN 1 ELSE 0 END AS is_broker
-- FROM ticketing.customers c LEFT JOIN analytics.yield_manager_partners b 
-- on lower(c.email)=lower(b.email)
  );

2021-08-05 17:19:18.953187 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2021-08-05 17:19:18.956799 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:18.956905 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2021-08-05 17:19:19.008679 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:19.011430 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:19.011551 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2021-08-05 17:19:19.059978 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:19.061127 (Thread-1): On model.customer_history.stg_customers: COMMIT
2021-08-05 17:19:19.061253 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:19.061353 (Thread-1): On model.customer_history.stg_customers: COMMIT
2021-08-05 17:19:19.706070 (Thread-1): SQL status: COMMIT in 0.64 seconds
2021-08-05 17:19:19.707859 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 17:19:19.707958 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2021-08-05 17:19:19.914726 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 17:19:19.917011 (Thread-1): finished collecting timing info
2021-08-05 17:19:19.917712 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090fc3d0>]}
2021-08-05 17:19:19.917908 (Thread-1): 10:19:19 | 3 of 17 OK created view model data_science.stg_customers............. [CREATE VIEW in 1.54s]
2021-08-05 17:19:19.918022 (Thread-1): Finished running node model.customer_history.stg_customers
2021-08-05 17:19:19.918137 (Thread-1): Began running node model.customer_history.stg_order_cst
2021-08-05 17:19:19.918253 (Thread-1): 10:19:19 | 4 of 17 START view model data_science.stg_order_cst.................. [RUN]
2021-08-05 17:19:19.918478 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:19.918560 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2021-08-05 17:19:19.918640 (Thread-1): Compiling model.customer_history.stg_order_cst
2021-08-05 17:19:19.923289 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order_cst"
2021-08-05 17:19:19.923676 (Thread-1): finished collecting timing info
2021-08-05 17:19:19.931023 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:19.931153 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_cst__dbt_tmp" cascade
2021-08-05 17:19:20.211353 (Thread-1): SQL status: DROP VIEW in 0.28 seconds
2021-08-05 17:19:20.213578 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:20.213676 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_cst__dbt_backup" cascade
2021-08-05 17:19:20.427543 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 17:19:20.429104 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order_cst"
2021-08-05 17:19:20.429528 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:20.429616 (Thread-1): On model.customer_history.stg_order_cst: BEGIN
2021-08-05 17:19:20.476412 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:20.476606 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:20.476686 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */

  create view "data_platform_prod"."data_science"."stg_order_cst__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    -- zone_unique_id,
    pricing_mode_id,
    -- price_code_type,
    CASE WHEN price_code_type ilike '%season%' THEN 1 ELSE 0 END AS is_season_ticket,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN config.context_configuration config USING(context_id)
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
config.timezone = 'CST' and lower(zone_type_description) in ('admissions', 'premium seating')
  );

2021-08-05 17:19:20.543591 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2021-08-05 17:19:20.547896 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:20.548048 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
alter table "data_platform_prod"."data_science"."stg_order_cst" rename to "stg_order_cst__dbt_backup"
2021-08-05 17:19:20.598840 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:20.601533 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:20.601644 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
alter table "data_platform_prod"."data_science"."stg_order_cst__dbt_tmp" rename to "stg_order_cst"
2021-08-05 17:19:20.649745 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:20.650685 (Thread-1): On model.customer_history.stg_order_cst: COMMIT
2021-08-05 17:19:20.650777 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:20.650852 (Thread-1): On model.customer_history.stg_order_cst: COMMIT
2021-08-05 17:19:20.853721 (Thread-1): SQL status: COMMIT in 0.20 seconds
2021-08-05 17:19:20.855490 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 17:19:20.855583 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_cst__dbt_backup" cascade
2021-08-05 17:19:21.079133 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 17:19:21.081340 (Thread-1): finished collecting timing info
2021-08-05 17:19:21.081878 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094d4450>]}
2021-08-05 17:19:21.082071 (Thread-1): 10:19:21 | 4 of 17 OK created view model data_science.stg_order_cst............. [CREATE VIEW in 1.16s]
2021-08-05 17:19:21.082182 (Thread-1): Finished running node model.customer_history.stg_order_cst
2021-08-05 17:19:21.082295 (Thread-1): Began running node model.customer_history.stg_order_est
2021-08-05 17:19:21.082453 (Thread-1): 10:19:21 | 5 of 17 START view model data_science.stg_order_est.................. [RUN]
2021-08-05 17:19:21.082915 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:21.083416 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order_cst).
2021-08-05 17:19:21.083512 (Thread-1): Compiling model.customer_history.stg_order_est
2021-08-05 17:19:21.088184 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order_est"
2021-08-05 17:19:21.088588 (Thread-1): finished collecting timing info
2021-08-05 17:19:21.094904 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:21.095018 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_est__dbt_tmp" cascade
2021-08-05 17:19:21.297750 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2021-08-05 17:19:21.300008 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:21.300101 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_est__dbt_backup" cascade
2021-08-05 17:19:21.462022 (Thread-1): SQL status: DROP VIEW in 0.16 seconds
2021-08-05 17:19:21.463514 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order_est"
2021-08-05 17:19:21.464043 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:21.464140 (Thread-1): On model.customer_history.stg_order_est: BEGIN
2021-08-05 17:19:21.523632 (Thread-1): SQL status: BEGIN in 0.06 seconds
2021-08-05 17:19:21.523783 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:21.523864 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */

  create view "data_platform_prod"."data_science"."stg_order_est__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    -- zone_unique_id,
    pricing_mode_id,
    -- price_code_type,
    CASE WHEN price_code_type ilike '%season%' THEN 1 ELSE 0 END AS is_season_ticket,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN config.context_configuration config USING(context_id)
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
config.timezone = 'EST' and lower(zone_type_description) in ('admissions', 'premium seating')
  );

2021-08-05 17:19:21.612034 (Thread-1): SQL status: CREATE VIEW in 0.09 seconds
2021-08-05 17:19:21.615764 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:21.615876 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
alter table "data_platform_prod"."data_science"."stg_order_est" rename to "stg_order_est__dbt_backup"
2021-08-05 17:19:21.702453 (Thread-1): SQL status: ALTER TABLE in 0.09 seconds
2021-08-05 17:19:21.705533 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:21.705632 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
alter table "data_platform_prod"."data_science"."stg_order_est__dbt_tmp" rename to "stg_order_est"
2021-08-05 17:19:21.816176 (Thread-1): SQL status: ALTER TABLE in 0.11 seconds
2021-08-05 17:19:21.817209 (Thread-1): On model.customer_history.stg_order_est: COMMIT
2021-08-05 17:19:21.817320 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:21.817408 (Thread-1): On model.customer_history.stg_order_est: COMMIT
2021-08-05 17:19:22.053456 (Thread-1): SQL status: COMMIT in 0.24 seconds
2021-08-05 17:19:22.055221 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 17:19:22.055311 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_est__dbt_backup" cascade
2021-08-05 17:19:22.361515 (Thread-1): SQL status: DROP VIEW in 0.31 seconds
2021-08-05 17:19:22.364355 (Thread-1): finished collecting timing info
2021-08-05 17:19:22.365073 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109312f10>]}
2021-08-05 17:19:22.365323 (Thread-1): 10:19:22 | 5 of 17 OK created view model data_science.stg_order_est............. [CREATE VIEW in 1.28s]
2021-08-05 17:19:22.365469 (Thread-1): Finished running node model.customer_history.stg_order_est
2021-08-05 17:19:22.365616 (Thread-1): Began running node model.customer_history.stg_order_mst
2021-08-05 17:19:22.365770 (Thread-1): 10:19:22 | 6 of 17 START view model data_science.stg_order_mst.................. [RUN]
2021-08-05 17:19:22.366309 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:22.366577 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order_est).
2021-08-05 17:19:22.366875 (Thread-1): Compiling model.customer_history.stg_order_mst
2021-08-05 17:19:22.373242 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order_mst"
2021-08-05 17:19:22.373700 (Thread-1): finished collecting timing info
2021-08-05 17:19:22.381854 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:22.381988 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_mst__dbt_tmp" cascade
2021-08-05 17:19:22.649158 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2021-08-05 17:19:22.652031 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:22.652158 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_mst__dbt_backup" cascade
2021-08-05 17:19:23.829785 (Thread-1): SQL status: DROP VIEW in 1.18 seconds
2021-08-05 17:19:23.831367 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order_mst"
2021-08-05 17:19:23.831858 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:23.831953 (Thread-1): On model.customer_history.stg_order_mst: BEGIN
2021-08-05 17:19:23.927093 (Thread-1): SQL status: BEGIN in 0.10 seconds
2021-08-05 17:19:23.927257 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:23.927340 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */

  create view "data_platform_prod"."data_science"."stg_order_mst__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    -- zone_unique_id,
    pricing_mode_id,
    -- price_code_type,
    CASE WHEN price_code_type ilike '%season%' THEN 1 ELSE 0 END AS is_season_ticket,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN config.context_configuration config USING(context_id)
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
config.timezone = 'MST' and lower(zone_type_description) in ('admissions', 'premium seating')
  );

2021-08-05 17:19:24.287693 (Thread-1): SQL status: CREATE VIEW in 0.36 seconds
2021-08-05 17:19:24.291786 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:24.291907 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
alter table "data_platform_prod"."data_science"."stg_order_mst" rename to "stg_order_mst__dbt_backup"
2021-08-05 17:19:24.376452 (Thread-1): SQL status: ALTER TABLE in 0.08 seconds
2021-08-05 17:19:24.379047 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:24.379169 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
alter table "data_platform_prod"."data_science"."stg_order_mst__dbt_tmp" rename to "stg_order_mst"
2021-08-05 17:19:24.438162 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 17:19:24.439201 (Thread-1): On model.customer_history.stg_order_mst: COMMIT
2021-08-05 17:19:24.439330 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:24.439421 (Thread-1): On model.customer_history.stg_order_mst: COMMIT
2021-08-05 17:19:24.645105 (Thread-1): SQL status: COMMIT in 0.21 seconds
2021-08-05 17:19:24.647008 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 17:19:24.647114 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_mst__dbt_backup" cascade
2021-08-05 17:19:24.859592 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 17:19:24.861817 (Thread-1): finished collecting timing info
2021-08-05 17:19:24.862368 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109312f10>]}
2021-08-05 17:19:24.862565 (Thread-1): 10:19:24 | 6 of 17 OK created view model data_science.stg_order_mst............. [CREATE VIEW in 2.50s]
2021-08-05 17:19:24.862680 (Thread-1): Finished running node model.customer_history.stg_order_mst
2021-08-05 17:19:24.862806 (Thread-1): Began running node model.customer_history.stg_order_pst
2021-08-05 17:19:24.862942 (Thread-1): 10:19:24 | 7 of 17 START view model data_science.stg_order_pst.................. [RUN]
2021-08-05 17:19:24.863296 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:24.863475 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order_mst).
2021-08-05 17:19:24.863583 (Thread-1): Compiling model.customer_history.stg_order_pst
2021-08-05 17:19:24.868720 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order_pst"
2021-08-05 17:19:24.869215 (Thread-1): finished collecting timing info
2021-08-05 17:19:24.876338 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:24.876459 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_pst__dbt_tmp" cascade
2021-08-05 17:19:25.094435 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 17:19:25.096672 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:25.096831 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_pst__dbt_backup" cascade
2021-08-05 17:19:25.313549 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 17:19:25.315107 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order_pst"
2021-08-05 17:19:25.315614 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:25.315703 (Thread-1): On model.customer_history.stg_order_pst: BEGIN
2021-08-05 17:19:25.361570 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:25.361733 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:25.361819 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */

  create view "data_platform_prod"."data_science"."stg_order_pst__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    -- zone_unique_id,
    pricing_mode_id,
    -- price_code_type,
    CASE WHEN price_code_type ilike '%season%' THEN 1 ELSE 0 END AS is_season_ticket,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN config.context_configuration config USING(context_id)
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
config.timezone = 'PST' and lower(zone_type_description) in ('admissions', 'premium seating')
  );

2021-08-05 17:19:25.426875 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2021-08-05 17:19:25.430329 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:25.430455 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
alter table "data_platform_prod"."data_science"."stg_order_pst" rename to "stg_order_pst__dbt_backup"
2021-08-05 17:19:25.480566 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:25.482866 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:25.482958 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
alter table "data_platform_prod"."data_science"."stg_order_pst__dbt_tmp" rename to "stg_order_pst"
2021-08-05 17:19:25.531978 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:25.532964 (Thread-1): On model.customer_history.stg_order_pst: COMMIT
2021-08-05 17:19:25.533058 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:25.533132 (Thread-1): On model.customer_history.stg_order_pst: COMMIT
2021-08-05 17:19:25.735328 (Thread-1): SQL status: COMMIT in 0.20 seconds
2021-08-05 17:19:25.737350 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 17:19:25.737492 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_pst__dbt_backup" cascade
2021-08-05 17:19:25.937019 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2021-08-05 17:19:25.939193 (Thread-1): finished collecting timing info
2021-08-05 17:19:25.939727 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094ed290>]}
2021-08-05 17:19:25.939917 (Thread-1): 10:19:25 | 7 of 17 OK created view model data_science.stg_order_pst............. [CREATE VIEW in 1.08s]
2021-08-05 17:19:25.940026 (Thread-1): Finished running node model.customer_history.stg_order_pst
2021-08-05 17:19:25.940137 (Thread-1): Began running node model.customer_history.order_flash_events_cst
2021-08-05 17:19:25.940310 (Thread-1): 10:19:25 | 8 of 17 START view model data_science.order_flash_events_cst......... [RUN]
2021-08-05 17:19:25.940795 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:25.941051 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order_pst).
2021-08-05 17:19:25.941149 (Thread-1): Compiling model.customer_history.order_flash_events_cst
2021-08-05 17:19:25.949666 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events_cst"
2021-08-05 17:19:25.950156 (Thread-1): finished collecting timing info
2021-08-05 17:19:25.956381 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:25.956488 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_cst__dbt_tmp" cascade
2021-08-05 17:19:26.207238 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2021-08-05 17:19:26.209531 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:26.209633 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_cst__dbt_backup" cascade
2021-08-05 17:19:26.449406 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2021-08-05 17:19:26.450893 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events_cst"
2021-08-05 17:19:26.451476 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:26.451574 (Thread-1): On model.customer_history.order_flash_events_cst: BEGIN
2021-08-05 17:19:26.503897 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:26.504045 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:26.504124 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */

  create view "data_platform_prod"."data_science"."order_flash_events_cst__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order_cst"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),

final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    pricing_mode_id,
    is_season_ticket,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    venue_unique_id,
    venue_zip,
    venue_type,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    major_category_name,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
  );

2021-08-05 17:19:26.598353 (Thread-1): SQL status: CREATE VIEW in 0.09 seconds
2021-08-05 17:19:26.600988 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:26.601143 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */
alter table "data_platform_prod"."data_science"."order_flash_events_cst__dbt_tmp" rename to "order_flash_events_cst"
2021-08-05 17:19:26.652625 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:26.653527 (Thread-1): On model.customer_history.order_flash_events_cst: COMMIT
2021-08-05 17:19:26.653623 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:26.653698 (Thread-1): On model.customer_history.order_flash_events_cst: COMMIT
2021-08-05 17:19:26.820934 (Thread-1): SQL status: COMMIT in 0.17 seconds
2021-08-05 17:19:26.823679 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 17:19:26.823777 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_cst__dbt_backup" cascade
2021-08-05 17:19:27.030491 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 17:19:27.032938 (Thread-1): finished collecting timing info
2021-08-05 17:19:27.033514 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10932a5d0>]}
2021-08-05 17:19:27.033711 (Thread-1): 10:19:27 | 8 of 17 OK created view model data_science.order_flash_events_cst.... [CREATE VIEW in 1.09s]
2021-08-05 17:19:27.033825 (Thread-1): Finished running node model.customer_history.order_flash_events_cst
2021-08-05 17:19:27.033941 (Thread-1): Began running node model.customer_history.order_flash_events_est
2021-08-05 17:19:27.034061 (Thread-1): 10:19:27 | 9 of 17 START view model data_science.order_flash_events_est......... [RUN]
2021-08-05 17:19:27.034515 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:27.034749 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events_cst).
2021-08-05 17:19:27.034859 (Thread-1): Compiling model.customer_history.order_flash_events_est
2021-08-05 17:19:27.044166 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events_est"
2021-08-05 17:19:27.044595 (Thread-1): finished collecting timing info
2021-08-05 17:19:27.051859 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:27.052039 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_est__dbt_tmp" cascade
2021-08-05 17:19:27.270123 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 17:19:27.272790 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:27.272897 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_est__dbt_backup" cascade
2021-08-05 17:19:27.501509 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 17:19:27.503061 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events_est"
2021-08-05 17:19:27.503587 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:27.503688 (Thread-1): On model.customer_history.order_flash_events_est: BEGIN
2021-08-05 17:19:27.548941 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:27.549119 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:27.549200 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */

  create view "data_platform_prod"."data_science"."order_flash_events_est__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order_est"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),

final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    pricing_mode_id,
    is_season_ticket,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    venue_unique_id,
    venue_zip,
    venue_type,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    major_category_name,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
  );

2021-08-05 17:19:27.632253 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2021-08-05 17:19:27.634522 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:27.634616 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */
alter table "data_platform_prod"."data_science"."order_flash_events_est__dbt_tmp" rename to "order_flash_events_est"
2021-08-05 17:19:27.682319 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:27.683225 (Thread-1): On model.customer_history.order_flash_events_est: COMMIT
2021-08-05 17:19:27.683322 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:27.683398 (Thread-1): On model.customer_history.order_flash_events_est: COMMIT
2021-08-05 17:19:27.888717 (Thread-1): SQL status: COMMIT in 0.21 seconds
2021-08-05 17:19:27.890658 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 17:19:27.890769 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_est__dbt_backup" cascade
2021-08-05 17:19:28.090237 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2021-08-05 17:19:28.092485 (Thread-1): finished collecting timing info
2021-08-05 17:19:28.093039 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109540110>]}
2021-08-05 17:19:28.093232 (Thread-1): 10:19:28 | 9 of 17 OK created view model data_science.order_flash_events_est.... [CREATE VIEW in 1.06s]
2021-08-05 17:19:28.093345 (Thread-1): Finished running node model.customer_history.order_flash_events_est
2021-08-05 17:19:28.093459 (Thread-1): Began running node model.customer_history.order_flash_events_mst
2021-08-05 17:19:28.093646 (Thread-1): 10:19:28 | 10 of 17 START view model data_science.order_flash_events_mst........ [RUN]
2021-08-05 17:19:28.094128 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:28.094263 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events_est).
2021-08-05 17:19:28.094391 (Thread-1): Compiling model.customer_history.order_flash_events_mst
2021-08-05 17:19:28.102445 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events_mst"
2021-08-05 17:19:28.102891 (Thread-1): finished collecting timing info
2021-08-05 17:19:28.108832 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:28.109035 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_mst__dbt_tmp" cascade
2021-08-05 17:19:28.812690 (Thread-1): SQL status: DROP VIEW in 0.70 seconds
2021-08-05 17:19:28.815204 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:28.815318 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_mst__dbt_backup" cascade
2021-08-05 17:19:29.389702 (Thread-1): SQL status: DROP VIEW in 0.57 seconds
2021-08-05 17:19:29.392208 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events_mst"
2021-08-05 17:19:29.392618 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:29.392707 (Thread-1): On model.customer_history.order_flash_events_mst: BEGIN
2021-08-05 17:19:29.443106 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:29.443331 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:29.443412 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */

  create view "data_platform_prod"."data_science"."order_flash_events_mst__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order_mst"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),

final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    pricing_mode_id,
    is_season_ticket,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    venue_unique_id,
    venue_zip,
    venue_type,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    major_category_name,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
  );

2021-08-05 17:19:29.517334 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2021-08-05 17:19:29.519564 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:29.519661 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */
alter table "data_platform_prod"."data_science"."order_flash_events_mst__dbt_tmp" rename to "order_flash_events_mst"
2021-08-05 17:19:29.573961 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:29.574908 (Thread-1): On model.customer_history.order_flash_events_mst: COMMIT
2021-08-05 17:19:29.575003 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:29.575077 (Thread-1): On model.customer_history.order_flash_events_mst: COMMIT
2021-08-05 17:19:29.806755 (Thread-1): SQL status: COMMIT in 0.23 seconds
2021-08-05 17:19:29.808513 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 17:19:29.808604 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_mst__dbt_backup" cascade
2021-08-05 17:19:30.150468 (Thread-1): SQL status: DROP VIEW in 0.34 seconds
2021-08-05 17:19:30.153055 (Thread-1): finished collecting timing info
2021-08-05 17:19:30.153702 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109278090>]}
2021-08-05 17:19:30.153931 (Thread-1): 10:19:30 | 10 of 17 OK created view model data_science.order_flash_events_mst... [CREATE VIEW in 2.06s]
2021-08-05 17:19:30.154062 (Thread-1): Finished running node model.customer_history.order_flash_events_mst
2021-08-05 17:19:30.154197 (Thread-1): Began running node model.customer_history.order_flash_events_pst
2021-08-05 17:19:30.154583 (Thread-1): 10:19:30 | 11 of 17 START view model data_science.order_flash_events_pst........ [RUN]
2021-08-05 17:19:30.154953 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:30.155059 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events_mst).
2021-08-05 17:19:30.155154 (Thread-1): Compiling model.customer_history.order_flash_events_pst
2021-08-05 17:19:30.164248 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events_pst"
2021-08-05 17:19:30.164633 (Thread-1): finished collecting timing info
2021-08-05 17:19:30.171870 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:30.171996 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_pst__dbt_tmp" cascade
2021-08-05 17:19:31.047454 (Thread-1): SQL status: DROP VIEW in 0.88 seconds
2021-08-05 17:19:31.049655 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:31.049751 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_pst__dbt_backup" cascade
2021-08-05 17:19:31.291309 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2021-08-05 17:19:31.292805 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events_pst"
2021-08-05 17:19:31.293299 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:31.293417 (Thread-1): On model.customer_history.order_flash_events_pst: BEGIN
2021-08-05 17:19:31.342348 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:31.342499 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:31.342579 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */

  create view "data_platform_prod"."data_science"."order_flash_events_pst__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order_pst"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),

final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    pricing_mode_id,
    is_season_ticket,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    venue_unique_id,
    venue_zip,
    venue_type,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    major_category_name,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
  );

2021-08-05 17:19:31.405131 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2021-08-05 17:19:31.407437 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:31.407535 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */
alter table "data_platform_prod"."data_science"."order_flash_events_pst__dbt_tmp" rename to "order_flash_events_pst"
2021-08-05 17:19:31.454155 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:19:31.455096 (Thread-1): On model.customer_history.order_flash_events_pst: COMMIT
2021-08-05 17:19:31.455186 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:31.455260 (Thread-1): On model.customer_history.order_flash_events_pst: COMMIT
2021-08-05 17:19:31.808015 (Thread-1): SQL status: COMMIT in 0.35 seconds
2021-08-05 17:19:31.809877 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 17:19:31.809984 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_pst__dbt_backup" cascade
2021-08-05 17:19:32.011619 (Thread-1): SQL status: DROP VIEW in 0.20 seconds
2021-08-05 17:19:32.014199 (Thread-1): finished collecting timing info
2021-08-05 17:19:32.014837 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10932ab50>]}
2021-08-05 17:19:32.015063 (Thread-1): 10:19:32 | 11 of 17 OK created view model data_science.order_flash_events_pst... [CREATE VIEW in 1.86s]
2021-08-05 17:19:32.015194 (Thread-1): Finished running node model.customer_history.order_flash_events_pst
2021-08-05 17:19:32.015326 (Thread-1): Began running node model.customer_history.order_ticket_details_cst
2021-08-05 17:19:32.015514 (Thread-1): 10:19:32 | 12 of 17 START table model data_science.order_ticket_details_cst..... [RUN]
2021-08-05 17:19:32.015812 (Thread-1): Acquiring new postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:19:32.015911 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events_pst).
2021-08-05 17:19:32.016208 (Thread-1): Compiling model.customer_history.order_ticket_details_cst
2021-08-05 17:19:32.024923 (Thread-1): Writing injected SQL for node "model.customer_history.order_ticket_details_cst"
2021-08-05 17:19:32.025299 (Thread-1): finished collecting timing info
2021-08-05 17:19:32.049458 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:19:32.049626 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_tmp" cascade
2021-08-05 17:19:32.101820 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 17:19:32.104389 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:19:32.104492 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_backup" cascade
2021-08-05 17:19:32.157128 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 17:19:32.158658 (Thread-1): Writing runtime SQL for node "model.customer_history.order_ticket_details_cst"
2021-08-05 17:19:32.159079 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:19:32.159170 (Thread-1): On model.customer_history.order_ticket_details_cst: BEGIN
2021-08-05 17:19:32.213959 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:19:32.214115 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:19:32.214196 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */


  create  table "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_tmp"
  as (
    -- calculate distance between customer location vs event location -- 

with orders as (
    SELECT * FROM "data_platform_prod"."data_science"."order_flash_events_cst"       
),
customers as (
    SELECT * FROM "data_platform_prod"."data_science"."stg_customers"
),
zipcodes as (
    select zipcode, longitude, latitude from public.us_zipcodes
    where primary_record='P'
),
-- customers_zip as (
--     Select
--         customers.*,
--         CAST(longitude AS DOUBLE PRECISION) as customer_long,
--         CAST(latitude AS DOUBLE PRECISION) as customer_lat
--     FROM customers LEFT JOIN zipcodes ON customers.zip=zipcodes.zipcode
-- ),
-- orders_zip as (
--     Select
--         orders.*,
--         CAST(longitude AS DOUBLE PRECISION) as venue_long,
--         CAST(latitude AS DOUBLE PRECISION) as venue_lat
--     FROM orders LEFT JOIN zipcodes ON orders.venue_zip=zipcodes.zipcode
-- ),
final as (
    SELECT
        customer_unique_id,
        axs_email_hash,
        order_ticket_unique_id,
        ROW_NUMBER() over (PARTITION BY order_ticket_unique_id ORDER BY 
        order_ticket_unique_id) AS order_ticket_identifier, -- to remove duplicate order_ticket_unique_id
        order_unique_id,
        amount_gross,
        channel,
        sale_datetime,
        pricing_mode_id,
        -- price_code_type,
        is_season_ticket,
        transfer_action_id,
        event_unique_id,
        ticket_id,
        ticket_state,
        days_sold_after_onsale,
        days_sold_before_event,
        venue_unique_id,
        venue_type,
        major_category_name,
        is_canceled,
        round(ST_DistanceSphere(ST_Point(CAST(c_zip.longitude AS DOUBLE PRECISION), CAST(c_zip.latitude AS DOUBLE PRECISION)), 
        ST_Point(CAST(v_zip.longitude AS DOUBLE PRECISION), CAST(v_zip.latitude AS DOUBLE PRECISION))) / 1000, 0) AS order_distance_in_km,
        order_distance_in_km / 1.6 AS order_distance_in_miles
    from customers
    INNER join orders using (customer_unique_id)
    LEFT JOIN zipcodes c_zip ON customers.zip=c_zip.zipcode
    LEFT JOIN zipcodes v_zip ON orders.venue_zip=v_zip.zipcode
)

SELECT * FROM final
  );
2021-08-05 17:29:26.150521 (Thread-1): SQL status: SELECT in 593.94 seconds
2021-08-05 17:29:26.154960 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:29:26.155072 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_cst" rename to "order_ticket_details_cst__dbt_backup"
2021-08-05 17:29:26.209283 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:29:26.211800 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:29:26.211914 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_tmp" rename to "order_ticket_details_cst"
2021-08-05 17:29:26.280484 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 17:29:26.281672 (Thread-1): On model.customer_history.order_ticket_details_cst: COMMIT
2021-08-05 17:29:26.281825 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:29:26.281942 (Thread-1): On model.customer_history.order_ticket_details_cst: COMMIT
2021-08-05 17:29:28.635156 (Thread-1): SQL status: COMMIT in 2.35 seconds
2021-08-05 17:29:28.637130 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 17:29:28.637285 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_backup" cascade
2021-08-05 17:29:28.880468 (Thread-1): SQL status: DROP TABLE in 0.24 seconds
2021-08-05 17:29:28.882848 (Thread-1): finished collecting timing info
2021-08-05 17:29:28.883440 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090c9210>]}
2021-08-05 17:29:28.883640 (Thread-1): 10:29:28 | 12 of 17 OK created table model data_science.order_ticket_details_cst [SELECT in 596.87s]
2021-08-05 17:29:28.883756 (Thread-1): Finished running node model.customer_history.order_ticket_details_cst
2021-08-05 17:29:28.883873 (Thread-1): Began running node model.customer_history.order_ticket_details_est
2021-08-05 17:29:28.884024 (Thread-1): 10:29:28 | 13 of 17 START table model data_science.order_ticket_details_est..... [RUN]
2021-08-05 17:29:28.884365 (Thread-1): Acquiring new postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:29:28.884461 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_ticket_details_cst).
2021-08-05 17:29:28.884576 (Thread-1): Compiling model.customer_history.order_ticket_details_est
2021-08-05 17:29:28.892142 (Thread-1): Writing injected SQL for node "model.customer_history.order_ticket_details_est"
2021-08-05 17:29:28.892516 (Thread-1): finished collecting timing info
2021-08-05 17:29:28.898545 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:29:28.898657 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_est__dbt_tmp" cascade
2021-08-05 17:29:28.949681 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 17:29:28.952261 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:29:28.952384 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_est__dbt_backup" cascade
2021-08-05 17:29:29.006704 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 17:29:29.008650 (Thread-1): Writing runtime SQL for node "model.customer_history.order_ticket_details_est"
2021-08-05 17:29:29.009160 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:29:29.009333 (Thread-1): On model.customer_history.order_ticket_details_est: BEGIN
2021-08-05 17:29:29.056565 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:29:29.056749 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:29:29.056848 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */


  create  table "data_platform_prod"."data_science"."order_ticket_details_est__dbt_tmp"
  as (
    -- calculate distance between customer location vs event location -- 

with orders as (
    SELECT * FROM "data_platform_prod"."data_science"."order_flash_events_est"       
),
customers as (
    SELECT * FROM "data_platform_prod"."data_science"."stg_customers"
),
zipcodes as (
    select zipcode, longitude, latitude from public.us_zipcodes
    where primary_record='P'
),
-- customers_zip as (
--     Select
--         customers.*,
--         CAST(longitude AS DOUBLE PRECISION) as customer_long,
--         CAST(latitude AS DOUBLE PRECISION) as customer_lat
--     FROM customers LEFT JOIN zipcodes ON customers.zip=zipcodes.zipcode
-- ),
-- orders_zip as (
--     Select
--         orders.*,
--         CAST(longitude AS DOUBLE PRECISION) as venue_long,
--         CAST(latitude AS DOUBLE PRECISION) as venue_lat
--     FROM orders LEFT JOIN zipcodes ON orders.venue_zip=zipcodes.zipcode
-- ),
final as (
    SELECT
        customer_unique_id,
        axs_email_hash,
        order_ticket_unique_id,
        ROW_NUMBER() over (PARTITION BY order_ticket_unique_id ORDER BY 
        order_ticket_unique_id) AS order_ticket_identifier, -- to remove duplicate order_ticket_unique_id
        order_unique_id,
        amount_gross,
        channel,
        sale_datetime,
        pricing_mode_id,
        -- price_code_type,
        is_season_ticket,
        transfer_action_id,
        event_unique_id,
        ticket_id,
        ticket_state,
        days_sold_after_onsale,
        days_sold_before_event,
        venue_unique_id,
        venue_type,
        major_category_name,
        is_canceled,
        round(ST_DistanceSphere(ST_Point(CAST(c_zip.longitude AS DOUBLE PRECISION), CAST(c_zip.latitude AS DOUBLE PRECISION)), 
        ST_Point(CAST(v_zip.longitude AS DOUBLE PRECISION), CAST(v_zip.latitude AS DOUBLE PRECISION))) / 1000, 0) AS order_distance_in_km,
        order_distance_in_km / 1.6 AS order_distance_in_miles
    from customers
    INNER join orders using (customer_unique_id)
    LEFT JOIN zipcodes c_zip ON customers.zip=c_zip.zipcode
    LEFT JOIN zipcodes v_zip ON orders.venue_zip=v_zip.zipcode
)

SELECT * FROM final
  );
2021-08-05 17:40:01.561751 (Thread-1): SQL status: SELECT in 632.50 seconds
2021-08-05 17:40:01.567115 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:40:01.567256 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_est" rename to "order_ticket_details_est__dbt_backup"
2021-08-05 17:40:01.637122 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 17:40:01.640411 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:40:01.640517 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_est__dbt_tmp" rename to "order_ticket_details_est"
2021-08-05 17:40:01.690781 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:40:01.691820 (Thread-1): On model.customer_history.order_ticket_details_est: COMMIT
2021-08-05 17:40:01.691948 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:40:01.692046 (Thread-1): On model.customer_history.order_ticket_details_est: COMMIT
2021-08-05 17:40:03.953730 (Thread-1): SQL status: COMMIT in 2.26 seconds
2021-08-05 17:40:03.955706 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 17:40:03.955815 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_est__dbt_backup" cascade
2021-08-05 17:40:08.241040 (Thread-1): SQL status: DROP TABLE in 4.29 seconds
2021-08-05 17:40:08.243253 (Thread-1): finished collecting timing info
2021-08-05 17:40:08.243804 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109173250>]}
2021-08-05 17:40:08.244002 (Thread-1): 10:40:08 | 13 of 17 OK created table model data_science.order_ticket_details_est [SELECT in 639.36s]
2021-08-05 17:40:08.244118 (Thread-1): Finished running node model.customer_history.order_ticket_details_est
2021-08-05 17:40:08.244234 (Thread-1): Began running node model.customer_history.order_ticket_details_mst
2021-08-05 17:40:08.244980 (Thread-1): 10:40:08 | 14 of 17 START table model data_science.order_ticket_details_mst..... [RUN]
2021-08-05 17:40:08.245236 (Thread-1): Acquiring new postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:40:08.245321 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_ticket_details_est).
2021-08-05 17:40:08.245405 (Thread-1): Compiling model.customer_history.order_ticket_details_mst
2021-08-05 17:40:08.252762 (Thread-1): Writing injected SQL for node "model.customer_history.order_ticket_details_mst"
2021-08-05 17:40:08.253139 (Thread-1): finished collecting timing info
2021-08-05 17:40:08.259175 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:40:08.259283 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_tmp" cascade
2021-08-05 17:40:08.319601 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2021-08-05 17:40:08.322249 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:40:08.322408 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_backup" cascade
2021-08-05 17:40:08.445856 (Thread-1): SQL status: DROP TABLE in 0.12 seconds
2021-08-05 17:40:08.448249 (Thread-1): Writing runtime SQL for node "model.customer_history.order_ticket_details_mst"
2021-08-05 17:40:08.448685 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:40:08.448792 (Thread-1): On model.customer_history.order_ticket_details_mst: BEGIN
2021-08-05 17:40:08.497894 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 17:40:08.498123 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:40:08.498236 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */


  create  table "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_tmp"
  as (
    -- calculate distance between customer location vs event location -- 

with orders as (
    SELECT * FROM "data_platform_prod"."data_science"."order_flash_events_mst"       
),
customers as (
    SELECT * FROM "data_platform_prod"."data_science"."stg_customers"
),
zipcodes as (
    select zipcode, longitude, latitude from public.us_zipcodes
    where primary_record='P'
),
-- customers_zip as (
--     Select
--         customers.*,
--         CAST(longitude AS DOUBLE PRECISION) as customer_long,
--         CAST(latitude AS DOUBLE PRECISION) as customer_lat
--     FROM customers LEFT JOIN zipcodes ON customers.zip=zipcodes.zipcode
-- ),
-- orders_zip as (
--     Select
--         orders.*,
--         CAST(longitude AS DOUBLE PRECISION) as venue_long,
--         CAST(latitude AS DOUBLE PRECISION) as venue_lat
--     FROM orders LEFT JOIN zipcodes ON orders.venue_zip=zipcodes.zipcode
-- ),
final as (
    SELECT
        customer_unique_id,
        axs_email_hash,
        order_ticket_unique_id,
        ROW_NUMBER() over (PARTITION BY order_ticket_unique_id ORDER BY 
        order_ticket_unique_id) AS order_ticket_identifier, -- to remove duplicate order_ticket_unique_id
        order_unique_id,
        amount_gross,
        channel,
        sale_datetime,
        pricing_mode_id,
        -- price_code_type,
        is_season_ticket,
        transfer_action_id,
        event_unique_id,
        ticket_id,
        ticket_state,
        days_sold_after_onsale,
        days_sold_before_event,
        venue_unique_id,
        venue_type,
        major_category_name,
        is_canceled,
        round(ST_DistanceSphere(ST_Point(CAST(c_zip.longitude AS DOUBLE PRECISION), CAST(c_zip.latitude AS DOUBLE PRECISION)), 
        ST_Point(CAST(v_zip.longitude AS DOUBLE PRECISION), CAST(v_zip.latitude AS DOUBLE PRECISION))) / 1000, 0) AS order_distance_in_km,
        order_distance_in_km / 1.6 AS order_distance_in_miles
    from customers
    INNER join orders using (customer_unique_id)
    LEFT JOIN zipcodes c_zip ON customers.zip=c_zip.zipcode
    LEFT JOIN zipcodes v_zip ON orders.venue_zip=v_zip.zipcode
)

SELECT * FROM final
  );
2021-08-05 17:55:14.782516 (Thread-1): SQL status: SELECT in 906.28 seconds
2021-08-05 17:55:14.788896 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:55:14.789107 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_mst" rename to "order_ticket_details_mst__dbt_backup"
2021-08-05 17:55:14.882970 (Thread-1): SQL status: ALTER TABLE in 0.09 seconds
2021-08-05 17:55:14.887337 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:55:14.887523 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_tmp" rename to "order_ticket_details_mst"
2021-08-05 17:55:14.941893 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 17:55:14.943829 (Thread-1): On model.customer_history.order_ticket_details_mst: COMMIT
2021-08-05 17:55:14.944027 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:55:14.944190 (Thread-1): On model.customer_history.order_ticket_details_mst: COMMIT
2021-08-05 17:55:16.295974 (Thread-1): SQL status: COMMIT in 1.35 seconds
2021-08-05 17:55:16.298080 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 17:55:16.298206 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_backup" cascade
2021-08-05 17:55:16.546921 (Thread-1): SQL status: DROP TABLE in 0.25 seconds
2021-08-05 17:55:16.551089 (Thread-1): finished collecting timing info
2021-08-05 17:55:16.551943 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109173c90>]}
2021-08-05 17:55:16.552245 (Thread-1): 10:55:16 | 14 of 17 OK created table model data_science.order_ticket_details_mst [SELECT in 908.31s]
2021-08-05 17:55:16.552425 (Thread-1): Finished running node model.customer_history.order_ticket_details_mst
2021-08-05 17:55:16.552605 (Thread-1): Began running node model.customer_history.fct_order_ticket_details
2021-08-05 17:55:16.552783 (Thread-1): 10:55:16 | 15 of 17 START table model data_science.fct_order_ticket_details..... [RUN]
2021-08-05 17:55:16.553309 (Thread-1): Acquiring new postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 17:55:16.553456 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_ticket_details_mst).
2021-08-05 17:55:16.553588 (Thread-1): Compiling model.customer_history.fct_order_ticket_details
2021-08-05 17:55:16.566091 (Thread-1): Writing injected SQL for node "model.customer_history.fct_order_ticket_details"
2021-08-05 17:55:16.566570 (Thread-1): finished collecting timing info
2021-08-05 17:55:16.576081 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 17:55:16.576266 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp" cascade
2021-08-05 17:55:16.632026 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2021-08-05 17:55:16.635763 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 17:55:16.635937 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_backup" cascade
2021-08-05 17:55:16.689500 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 17:55:16.692194 (Thread-1): Writing runtime SQL for node "model.customer_history.fct_order_ticket_details"
2021-08-05 17:55:16.692800 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 17:55:16.692943 (Thread-1): On model.customer_history.fct_order_ticket_details: BEGIN
2021-08-05 17:55:16.823663 (Thread-1): SQL status: BEGIN in 0.13 seconds
2021-08-05 17:55:16.823946 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 17:55:16.824115 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */


  create  table "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp"
  as (
    

with cst as (
    select * from "data_platform_prod"."data_science"."order_flash_events_cst"
),
est as (
    select * from "data_platform_prod"."data_science"."order_flash_events_est"
),
mst as (
    select * from "data_platform_prod"."data_science"."order_flash_events_mst"
),
pst as (
    select * from "data_platform_prod"."data_science"."order_flash_events_pst"
),
final as (
    SELECT * from cst
    UNION
    SELECT * from est
    UNION
    SELECT * from mst
    UNION
    SELECT * from pst
)

SELECT * FROM final
  );
2021-08-05 18:00:43.896550 (Thread-1): SQL status: SELECT in 327.07 seconds
2021-08-05 18:00:43.900818 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:00:43.900971 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
alter table "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp" rename to "fct_order_ticket_details"
2021-08-05 18:00:43.952379 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:00:43.954332 (Thread-1): On model.customer_history.fct_order_ticket_details: COMMIT
2021-08-05 18:00:43.954558 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:00:43.954727 (Thread-1): On model.customer_history.fct_order_ticket_details: COMMIT
2021-08-05 18:00:44.355673 (Thread-1): SQL status: COMMIT in 0.40 seconds
2021-08-05 18:00:44.359132 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:00:44.359291 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_backup" cascade
2021-08-05 18:00:44.415422 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2021-08-05 18:00:44.420468 (Thread-1): finished collecting timing info
2021-08-05 18:00:44.421368 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10912f990>]}
2021-08-05 18:00:44.421680 (Thread-1): 11:00:44 | 15 of 17 OK created table model data_science.fct_order_ticket_details [SELECT in 327.87s]
2021-08-05 18:00:44.421861 (Thread-1): Finished running node model.customer_history.fct_order_ticket_details
2021-08-05 18:00:44.422047 (Thread-1): Began running node model.customer_history.order_ticket_details_pst
2021-08-05 18:00:44.422230 (Thread-1): 11:00:44 | 16 of 17 START table model data_science.order_ticket_details_pst..... [RUN]
2021-08-05 18:00:44.422977 (Thread-1): Acquiring new postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:00:44.423123 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.fct_order_ticket_details).
2021-08-05 18:00:44.423256 (Thread-1): Compiling model.customer_history.order_ticket_details_pst
2021-08-05 18:00:44.433680 (Thread-1): Writing injected SQL for node "model.customer_history.order_ticket_details_pst"
2021-08-05 18:00:44.434167 (Thread-1): finished collecting timing info
2021-08-05 18:00:44.442238 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:00:44.442383 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_tmp" cascade
2021-08-05 18:00:44.495188 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:00:44.500605 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:00:44.500805 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_backup" cascade
2021-08-05 18:00:44.555375 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:00:44.558274 (Thread-1): Writing runtime SQL for node "model.customer_history.order_ticket_details_pst"
2021-08-05 18:00:44.558959 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:00:44.559106 (Thread-1): On model.customer_history.order_ticket_details_pst: BEGIN
2021-08-05 18:00:44.608325 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:00:44.608760 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:00:44.608959 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */


  create  table "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_tmp"
  as (
    -- calculate distance between customer location vs event location -- 

with orders as (
    SELECT * FROM "data_platform_prod"."data_science"."order_flash_events_pst"       
),
customers as (
    SELECT * FROM "data_platform_prod"."data_science"."stg_customers"
),
zipcodes as (
    select zipcode, longitude, latitude from public.us_zipcodes
    where primary_record='P'
),
-- customers_zip as (
--     Select
--         customers.*,
--         CAST(longitude AS DOUBLE PRECISION) as customer_long,
--         CAST(latitude AS DOUBLE PRECISION) as customer_lat
--     FROM customers LEFT JOIN zipcodes ON customers.zip=zipcodes.zipcode
-- ),
-- orders_zip as (
--     Select
--         orders.*,
--         CAST(longitude AS DOUBLE PRECISION) as venue_long,
--         CAST(latitude AS DOUBLE PRECISION) as venue_lat
--     FROM orders LEFT JOIN zipcodes ON orders.venue_zip=zipcodes.zipcode
-- ),
final as (
    SELECT
        customer_unique_id,
        axs_email_hash,
        order_ticket_unique_id,
        ROW_NUMBER() over (PARTITION BY order_ticket_unique_id ORDER BY 
        order_ticket_unique_id) AS order_ticket_identifier, -- to remove duplicate order_ticket_unique_id
        order_unique_id,
        amount_gross,
        channel,
        sale_datetime,
        pricing_mode_id,
        -- price_code_type,
        is_season_ticket,
        transfer_action_id,
        event_unique_id,
        ticket_id,
        ticket_state,
        days_sold_after_onsale,
        days_sold_before_event,
        venue_unique_id,
        venue_type,
        major_category_name,
        is_canceled,
        round(ST_DistanceSphere(ST_Point(CAST(c_zip.longitude AS DOUBLE PRECISION), CAST(c_zip.latitude AS DOUBLE PRECISION)), 
        ST_Point(CAST(v_zip.longitude AS DOUBLE PRECISION), CAST(v_zip.latitude AS DOUBLE PRECISION))) / 1000, 0) AS order_distance_in_km,
        order_distance_in_km / 1.6 AS order_distance_in_miles
    from customers
    INNER join orders using (customer_unique_id)
    LEFT JOIN zipcodes c_zip ON customers.zip=c_zip.zipcode
    LEFT JOIN zipcodes v_zip ON orders.venue_zip=v_zip.zipcode
)

SELECT * FROM final
  );
2021-08-05 18:08:56.572742 (Thread-1): SQL status: SELECT in 491.96 seconds
2021-08-05 18:08:56.578995 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:08:56.579132 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_pst" rename to "order_ticket_details_pst__dbt_backup"
2021-08-05 18:08:56.632325 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:08:56.636613 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:08:56.636783 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_tmp" rename to "order_ticket_details_pst"
2021-08-05 18:08:56.688164 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:08:56.690165 (Thread-1): On model.customer_history.order_ticket_details_pst: COMMIT
2021-08-05 18:08:56.690375 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:08:56.690541 (Thread-1): On model.customer_history.order_ticket_details_pst: COMMIT
2021-08-05 18:08:56.962800 (Thread-1): SQL status: COMMIT in 0.27 seconds
2021-08-05 18:08:56.967560 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:08:56.967739 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_backup" cascade
2021-08-05 18:08:57.161100 (Thread-1): SQL status: DROP TABLE in 0.19 seconds
2021-08-05 18:08:57.164990 (Thread-1): finished collecting timing info
2021-08-05 18:08:57.166090 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109173250>]}
2021-08-05 18:08:57.166513 (Thread-1): 11:08:57 | 16 of 17 OK created table model data_science.order_ticket_details_pst [SELECT in 492.74s]
2021-08-05 18:08:57.166706 (Thread-1): Finished running node model.customer_history.order_ticket_details_pst
2021-08-05 18:08:57.166884 (Thread-1): Began running node model.customer_history.dim_customers_email_acxiom
2021-08-05 18:08:57.167231 (Thread-1): 11:08:57 | 17 of 17 START table model data_science.dim_customers_email_acxiom... [RUN]
2021-08-05 18:08:57.168710 (Thread-1): Acquiring new postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:08:57.168850 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_ticket_details_pst).
2021-08-05 18:08:57.168975 (Thread-1): Compiling model.customer_history.dim_customers_email_acxiom
2021-08-05 18:08:57.177717 (Thread-1): Writing injected SQL for node "model.customer_history.dim_customers_email_acxiom"
2021-08-05 18:08:57.178219 (Thread-1): finished collecting timing info
2021-08-05 18:08:57.186179 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:08:57.186336 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_tmp" cascade
2021-08-05 18:08:57.239905 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:08:57.242861 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:08:57.243001 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_backup" cascade
2021-08-05 18:08:57.294558 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:08:57.297262 (Thread-1): Writing runtime SQL for node "model.customer_history.dim_customers_email_acxiom"
2021-08-05 18:08:57.297943 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:08:57.298100 (Thread-1): On model.customer_history.dim_customers_email_acxiom: BEGIN
2021-08-05 18:08:57.344767 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:08:57.345186 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:08:57.345447 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */


  create  table "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_tmp"
  as (
    

with orders as (
    select * from "data_platform_prod"."data_science"."fct_order_ticket_details"
),

customer_orders as (
    select
        axs_email_hash,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        COUNT(DISTINCT event_unique_id) AS number_of_events,

        SUM(CASE WHEN order_ticket_identifier=1 THEN amount_gross ELSE 0 END) AS total_revenue,

        SUM(FLOOR(COALESCE((CASE WHEN order_ticket_identifier=1 THEN days_sold_after_onsale ELSE 0 END), 0))) / COUNT(DISTINCT CASE WHEN days_sold_after_onsale IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE((CASE WHEN order_ticket_identifier=1 THEN  days_sold_before_event ELSE 0 END), 0)))/ COUNT(DISTINCT CASE WHEN days_sold_before_event IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers,

        AVG(CASE WHEN order_ticket_identifier=1 THEN order_distance_in_km ELSE NULL END) AS average_order_distance_in_km,

        COUNT(DISTINCT venue_unique_id) AS number_of_venues,

        ROUND(COUNT(DISTINCT CASE WHEN channel='Back Office' THEN order_ticket_unique_id ELSE NULL END) *1.0 / number_of_tickets_sold, 2) AS channel_back_office_percent,
        ROUND(COUNT(DISTINCT CASE WHEN channel='Web' THEN
            order_ticket_unique_id ELSE NULL END) *1.0 / number_of_tickets_sold, 2) AS channel_web_percent,

        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Sports' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_sports_percent,
        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Music' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_music_percent,
        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Arts & Family' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_arts_family_percent,

        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Arena' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_arena_percent,
        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Large Music Venue' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_large_music_percent,
        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Club and Theater' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_club_theatre_percent,

        COUNT(DISTINCT CASE WHEN is_season_ticket = 1 THEN order_ticket_unique_id ELSE NULL END) AS number_of_season_tickets
        -- COUNT(DISTINCT CASE WHEN price_code_type ilike '%season%' THEN order_ticket_unique_id ELSE NULL END) AS number_of_season_tickets

    from orders
    WHERE is_canceled is FALSE -- shall this condition live elsewhere?
    group by 1 
)
select * from customer_orders
  );
2021-08-05 18:08:57.407543 (Thread-1): Postgres error: column "axs_email_hash" does not exist in orders

2021-08-05 18:08:57.407762 (Thread-1): On model.customer_history.dim_customers_email_acxiom: ROLLBACK
2021-08-05 18:08:57.473533 (Thread-1): finished collecting timing info
2021-08-05 18:08:57.474138 (Thread-1): Database Error in model dim_customers_email_acxiom (models/dim_customers_email_acxiom.sql)
  column "axs_email_hash" does not exist in orders
  compiled SQL at target/run/customer_history/dim_customers_email_acxiom.sql
Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedColumn: column "axs_email_hash" does not exist in orders


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 62, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/Cellar/dbt/0.16.1_1/libexec/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model dim_customers_email_acxiom (models/dim_customers_email_acxiom.sql)
  column "axs_email_hash" does not exist in orders
  compiled SQL at target/run/customer_history/dim_customers_email_acxiom.sql
2021-08-05 18:08:57.484247 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '514e5a93-640c-4b19-b52b-9dd92a0bdfe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092d6a50>]}
2021-08-05 18:08:57.484533 (Thread-1): 11:08:57 | 17 of 17 ERROR creating table model data_science.dim_customers_email_acxiom [ERROR in 0.32s]
2021-08-05 18:08:57.484693 (Thread-1): Finished running node model.customer_history.dim_customers_email_acxiom
2021-08-05 18:08:57.507570 (MainThread): Using postgres connection "master".
2021-08-05 18:08:57.507821 (MainThread): On master: BEGIN
2021-08-05 18:08:57.557138 (MainThread): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:08:57.557609 (MainThread): On master: COMMIT
2021-08-05 18:08:57.557789 (MainThread): Using postgres connection "master".
2021-08-05 18:08:57.557950 (MainThread): On master: COMMIT
2021-08-05 18:08:57.611845 (MainThread): SQL status: COMMIT in 0.05 seconds
2021-08-05 18:08:57.612890 (MainThread): 11:08:57 | 
2021-08-05 18:08:57.613136 (MainThread): 11:08:57 | Finished running 11 view models, 6 table models in 2983.37s.
2021-08-05 18:08:57.613331 (MainThread): Connection 'master' was left open.
2021-08-05 18:08:57.613499 (MainThread): On master: Close
2021-08-05 18:08:57.614033 (MainThread): Connection 'model.customer_history.dim_customers_email_acxiom' was left open.
2021-08-05 18:08:57.614205 (MainThread): On model.customer_history.dim_customers_email_acxiom: Close
2021-08-05 18:08:57.668414 (MainThread): 
2021-08-05 18:08:57.668633 (MainThread): Completed with 1 error and 0 warnings:
2021-08-05 18:08:57.668770 (MainThread): 
2021-08-05 18:08:57.668902 (MainThread): Database Error in model dim_customers_email_acxiom (models/dim_customers_email_acxiom.sql)
2021-08-05 18:08:57.669023 (MainThread):   column "axs_email_hash" does not exist in orders
2021-08-05 18:08:57.669133 (MainThread):   compiled SQL at target/run/customer_history/dim_customers_email_acxiom.sql
2021-08-05 18:08:57.669279 (MainThread): 
Done. PASS=16 WARN=0 ERROR=1 SKIP=0 TOTAL=17
2021-08-05 18:08:57.669530 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10928f350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109264ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10944ac90>]}
2021-08-05 18:08:57.669737 (MainThread): Flushing usage events
2021-08-05 18:15:25.551833 (MainThread): Running with dbt=0.16.1
2021-08-05 18:15:25.647256 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['fct_order_ticket_details'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-08-05 18:15:25.648388 (MainThread): Tracking: tracking
2021-08-05 18:15:25.658820 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120e66d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120e2d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120e29d0>]}
2021-08-05 18:15:25.682960 (MainThread): Partial parsing not enabled
2021-08-05 18:15:25.685967 (MainThread): Parsing macros/core.sql
2021-08-05 18:15:25.692641 (MainThread): Parsing macros/materializations/helpers.sql
2021-08-05 18:15:25.701744 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-08-05 18:15:25.703749 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-08-05 18:15:25.722649 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-08-05 18:15:25.757556 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-08-05 18:15:25.779906 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-08-05 18:15:25.782085 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-08-05 18:15:25.789109 (MainThread): Parsing macros/materializations/common/merge.sql
2021-08-05 18:15:25.802292 (MainThread): Parsing macros/materializations/table/table.sql
2021-08-05 18:15:25.809570 (MainThread): Parsing macros/materializations/view/view.sql
2021-08-05 18:15:25.816205 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-08-05 18:15:25.821519 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-08-05 18:15:25.822718 (MainThread): Parsing macros/etc/query.sql
2021-08-05 18:15:25.824362 (MainThread): Parsing macros/etc/is_incremental.sql
2021-08-05 18:15:25.826286 (MainThread): Parsing macros/etc/get_relation_comment.sql
2021-08-05 18:15:25.828638 (MainThread): Parsing macros/etc/datetime.sql
2021-08-05 18:15:25.838201 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-08-05 18:15:25.840491 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-08-05 18:15:25.842121 (MainThread): Parsing macros/adapters/common.sql
2021-08-05 18:15:25.885286 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-08-05 18:15:25.886741 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-08-05 18:15:25.887873 (MainThread): Parsing macros/schema_tests/unique.sql
2021-08-05 18:15:25.889225 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-08-05 18:15:25.891806 (MainThread): Parsing macros/catalog.sql
2021-08-05 18:15:25.894465 (MainThread): Parsing macros/relations.sql
2021-08-05 18:15:25.896085 (MainThread): Parsing macros/adapters.sql
2021-08-05 18:15:25.913602 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2021-08-05 18:15:25.932078 (MainThread): Partial parsing not enabled
2021-08-05 18:15:25.960033 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:15:25.960159 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:25.975939 (MainThread): Acquiring new postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:15:25.976046 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:25.985273 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:15:25.985417 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:25.990224 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2021-08-05 18:15:25.990360 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:25.995492 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:15:25.995635 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.000482 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:15:26.000578 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.005921 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2021-08-05 18:15:26.006063 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.011367 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:15:26.011529 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.016991 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2021-08-05 18:15:26.017109 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.022506 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:15:26.022631 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.030265 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:15:26.030383 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.038235 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:15:26.038329 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.044874 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:15:26.045000 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.052593 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:15:26.052703 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.059401 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:15:26.059536 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.067532 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:15:26.067647 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.074574 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:15:26.074716 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.269827 (MainThread): Found 17 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2021-08-05 18:15:26.275819 (MainThread): 
2021-08-05 18:15:26.276138 (MainThread): Acquiring new postgres connection "master".
2021-08-05 18:15:26.276231 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:15:26.281441 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2021-08-05 18:15:26.281586 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-08-05 18:15:26.374727 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2021-08-05 18:15:26.374874 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2021-08-05 18:15:27.292086 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.92 seconds
2021-08-05 18:15:27.363846 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:15:27.364039 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2021-08-05 18:15:27.365440 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:15:27.365530 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2021-08-05 18:15:27.450764 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.09 seconds
2021-08-05 18:15:27.451186 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:15:27.451450 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2021-08-05 18:15:27.527649 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.08 seconds
2021-08-05 18:15:27.566352 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2021-08-05 18:15:27.696414 (MainThread): Using postgres connection "master".
2021-08-05 18:15:27.696548 (MainThread): On master: BEGIN
2021-08-05 18:15:28.134441 (MainThread): SQL status: BEGIN in 0.44 seconds
2021-08-05 18:15:28.134893 (MainThread): Using postgres connection "master".
2021-08-05 18:15:28.135110 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2021-08-05 18:15:28.295847 (MainThread): SQL status: SELECT in 0.16 seconds
2021-08-05 18:15:28.543398 (MainThread): On master: ROLLBACK
2021-08-05 18:15:28.595825 (MainThread): Using postgres connection "master".
2021-08-05 18:15:28.596214 (MainThread): On master: BEGIN
2021-08-05 18:15:28.687482 (MainThread): SQL status: BEGIN in 0.09 seconds
2021-08-05 18:15:28.687693 (MainThread): On master: COMMIT
2021-08-05 18:15:28.687811 (MainThread): Using postgres connection "master".
2021-08-05 18:15:28.687914 (MainThread): On master: COMMIT
2021-08-05 18:15:28.734218 (MainThread): SQL status: COMMIT in 0.05 seconds
2021-08-05 18:15:28.735080 (MainThread): 11:15:28 | Concurrency: 1 threads (target='dev')
2021-08-05 18:15:28.735316 (MainThread): 11:15:28 | 
2021-08-05 18:15:28.737995 (Thread-1): Began running node model.customer_history.fct_order_ticket_details
2021-08-05 18:15:28.738225 (Thread-1): 11:15:28 | 1 of 1 START table model data_science.fct_order_ticket_details....... [RUN]
2021-08-05 18:15:28.738586 (Thread-1): Acquiring new postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:15:28.738718 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2021-08-05 18:15:28.738853 (Thread-1): Compiling model.customer_history.fct_order_ticket_details
2021-08-05 18:15:28.760865 (Thread-1): Writing injected SQL for node "model.customer_history.fct_order_ticket_details"
2021-08-05 18:15:28.761952 (Thread-1): finished collecting timing info
2021-08-05 18:15:28.784713 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:15:28.784862 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp" cascade
2021-08-05 18:15:28.885420 (Thread-1): SQL status: DROP TABLE in 0.10 seconds
2021-08-05 18:15:28.889746 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:15:28.889904 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_backup" cascade
2021-08-05 18:15:28.941881 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:15:28.963337 (Thread-1): Writing runtime SQL for node "model.customer_history.fct_order_ticket_details"
2021-08-05 18:15:28.964133 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:15:28.964274 (Thread-1): On model.customer_history.fct_order_ticket_details: BEGIN
2021-08-05 18:15:29.027600 (Thread-1): SQL status: BEGIN in 0.06 seconds
2021-08-05 18:15:29.028023 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:15:29.028292 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */


  create  table "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp"
  as (
    

with cst as (
    select * from "data_platform_prod"."data_science"."order_ticket_details_cst"
),
est as (
    select * from "data_platform_prod"."data_science"."order_ticket_details_est"
),
mst as (
    select * from "data_platform_prod"."data_science"."order_ticket_details_mst"
),
pst as (
    select * from "data_platform_prod"."data_science"."order_ticket_details_pst"
),
final as (
    SELECT * from cst
    UNION
    SELECT * from est
    UNION
    SELECT * from mst
    UNION
    SELECT * from pst
)

SELECT * FROM final
  );
2021-08-05 18:18:33.485545 (Thread-1): SQL status: SELECT in 184.46 seconds
2021-08-05 18:18:33.490711 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:18:33.490859 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
alter table "data_platform_prod"."data_science"."fct_order_ticket_details" rename to "fct_order_ticket_details__dbt_backup"
2021-08-05 18:18:33.551983 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:18:33.555109 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:18:33.555247 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
alter table "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp" rename to "fct_order_ticket_details"
2021-08-05 18:18:33.608739 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:18:33.610473 (Thread-1): On model.customer_history.fct_order_ticket_details: COMMIT
2021-08-05 18:18:33.610702 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:18:33.610869 (Thread-1): On model.customer_history.fct_order_ticket_details: COMMIT
2021-08-05 18:18:34.790948 (Thread-1): SQL status: COMMIT in 1.18 seconds
2021-08-05 18:18:34.794597 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:18:34.794779 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_backup" cascade
2021-08-05 18:18:35.005615 (Thread-1): SQL status: DROP TABLE in 0.21 seconds
2021-08-05 18:18:35.008618 (Thread-1): finished collecting timing info
2021-08-05 18:18:35.009397 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dee1d522-8fc4-4bf8-8fa4-c49ce46c2f21', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122dd250>]}
2021-08-05 18:18:35.009694 (Thread-1): 11:18:35 | 1 of 1 OK created table model data_science.fct_order_ticket_details.. [SELECT in 186.27s]
2021-08-05 18:18:35.009856 (Thread-1): Finished running node model.customer_history.fct_order_ticket_details
2021-08-05 18:18:35.037434 (MainThread): Using postgres connection "master".
2021-08-05 18:18:35.037657 (MainThread): On master: BEGIN
2021-08-05 18:18:35.099495 (MainThread): SQL status: BEGIN in 0.06 seconds
2021-08-05 18:18:35.099833 (MainThread): On master: COMMIT
2021-08-05 18:18:35.099985 (MainThread): Using postgres connection "master".
2021-08-05 18:18:35.100115 (MainThread): On master: COMMIT
2021-08-05 18:18:35.147243 (MainThread): SQL status: COMMIT in 0.05 seconds
2021-08-05 18:18:35.148118 (MainThread): 11:18:35 | 
2021-08-05 18:18:35.148359 (MainThread): 11:18:35 | Finished running 1 table model in 188.87s.
2021-08-05 18:18:35.148555 (MainThread): Connection 'master' was left open.
2021-08-05 18:18:35.148714 (MainThread): On master: Close
2021-08-05 18:18:35.149162 (MainThread): Connection 'model.customer_history.fct_order_ticket_details' was left open.
2021-08-05 18:18:35.149329 (MainThread): On model.customer_history.fct_order_ticket_details: Close
2021-08-05 18:18:35.156741 (MainThread): 
2021-08-05 18:18:35.156973 (MainThread): Completed successfully
2021-08-05 18:18:35.157146 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2021-08-05 18:18:35.157400 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112457610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122ce350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122ce750>]}
2021-08-05 18:18:35.157643 (MainThread): Flushing usage events
2021-08-05 18:28:04.578537 (MainThread): Running with dbt=0.16.1
2021-08-05 18:28:04.649538 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['dim_customers_exmail_acxiom'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-08-05 18:28:04.650301 (MainThread): Tracking: tracking
2021-08-05 18:28:04.656705 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108157d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f2d590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f2d090>]}
2021-08-05 18:28:04.676307 (MainThread): Partial parsing not enabled
2021-08-05 18:28:04.679592 (MainThread): Parsing macros/core.sql
2021-08-05 18:28:04.684814 (MainThread): Parsing macros/materializations/helpers.sql
2021-08-05 18:28:04.693480 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-08-05 18:28:04.695381 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-08-05 18:28:04.714178 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-08-05 18:28:04.749184 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-08-05 18:28:04.771367 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-08-05 18:28:04.773447 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-08-05 18:28:04.780254 (MainThread): Parsing macros/materializations/common/merge.sql
2021-08-05 18:28:04.793670 (MainThread): Parsing macros/materializations/table/table.sql
2021-08-05 18:28:04.800913 (MainThread): Parsing macros/materializations/view/view.sql
2021-08-05 18:28:04.807875 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-08-05 18:28:04.813255 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-08-05 18:28:04.814331 (MainThread): Parsing macros/etc/query.sql
2021-08-05 18:28:04.815510 (MainThread): Parsing macros/etc/is_incremental.sql
2021-08-05 18:28:04.817310 (MainThread): Parsing macros/etc/get_relation_comment.sql
2021-08-05 18:28:04.819543 (MainThread): Parsing macros/etc/datetime.sql
2021-08-05 18:28:04.829379 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-08-05 18:28:04.831533 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-08-05 18:28:04.832690 (MainThread): Parsing macros/adapters/common.sql
2021-08-05 18:28:04.876391 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-08-05 18:28:04.877739 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-08-05 18:28:04.878811 (MainThread): Parsing macros/schema_tests/unique.sql
2021-08-05 18:28:04.880046 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-08-05 18:28:04.882458 (MainThread): Parsing macros/catalog.sql
2021-08-05 18:28:04.884979 (MainThread): Parsing macros/relations.sql
2021-08-05 18:28:04.886430 (MainThread): Parsing macros/adapters.sql
2021-08-05 18:28:04.904009 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2021-08-05 18:28:04.925224 (MainThread): Partial parsing not enabled
2021-08-05 18:28:04.954579 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:28:04.954724 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:04.971126 (MainThread): Acquiring new postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:28:04.971244 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:04.979834 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:28:04.979943 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:04.984884 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2021-08-05 18:28:04.984994 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:04.989186 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:28:04.989285 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:04.993427 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:28:04.993522 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:04.997981 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2021-08-05 18:28:04.998078 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.002197 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:28:05.002294 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.006563 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2021-08-05 18:28:05.006666 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.011153 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:28:05.011251 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.017610 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:28:05.017715 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.025012 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:28:05.025115 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.031466 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:28:05.031569 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.038788 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:28:05.038892 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.045271 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:28:05.045380 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.053188 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:28:05.053300 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.059599 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:28:05.059700 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:05.235786 (MainThread): Found 17 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2021-08-05 18:28:05.241695 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2021-08-05 18:28:05.241886 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084366d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108343810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108343550>]}
2021-08-05 18:28:05.242054 (MainThread): Flushing usage events
2021-08-05 18:28:05.700895 (MainThread): Connection 'model.customer_history.order_flash_events_mst' was properly closed.
2021-08-05 18:28:24.664225 (MainThread): Running with dbt=0.16.1
2021-08-05 18:28:24.732886 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=['dim_customers_email_acxiom'], partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-08-05 18:28:24.733789 (MainThread): Tracking: tracking
2021-08-05 18:28:24.740150 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b4dad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103da5690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b40b50>]}
2021-08-05 18:28:24.760666 (MainThread): Partial parsing not enabled
2021-08-05 18:28:24.762685 (MainThread): Parsing macros/core.sql
2021-08-05 18:28:24.768176 (MainThread): Parsing macros/materializations/helpers.sql
2021-08-05 18:28:24.776654 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-08-05 18:28:24.778543 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-08-05 18:28:24.797207 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-08-05 18:28:24.831995 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-08-05 18:28:24.854121 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-08-05 18:28:24.856202 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-08-05 18:28:24.862866 (MainThread): Parsing macros/materializations/common/merge.sql
2021-08-05 18:28:24.876413 (MainThread): Parsing macros/materializations/table/table.sql
2021-08-05 18:28:24.883948 (MainThread): Parsing macros/materializations/view/view.sql
2021-08-05 18:28:24.891001 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-08-05 18:28:24.896626 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-08-05 18:28:24.897715 (MainThread): Parsing macros/etc/query.sql
2021-08-05 18:28:24.898976 (MainThread): Parsing macros/etc/is_incremental.sql
2021-08-05 18:28:24.901126 (MainThread): Parsing macros/etc/get_relation_comment.sql
2021-08-05 18:28:24.903771 (MainThread): Parsing macros/etc/datetime.sql
2021-08-05 18:28:24.914251 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-08-05 18:28:24.916365 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-08-05 18:28:24.917571 (MainThread): Parsing macros/adapters/common.sql
2021-08-05 18:28:24.960531 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-08-05 18:28:24.961858 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-08-05 18:28:24.962928 (MainThread): Parsing macros/schema_tests/unique.sql
2021-08-05 18:28:24.964260 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-08-05 18:28:24.966765 (MainThread): Parsing macros/catalog.sql
2021-08-05 18:28:24.969583 (MainThread): Parsing macros/relations.sql
2021-08-05 18:28:24.971818 (MainThread): Parsing macros/adapters.sql
2021-08-05 18:28:24.989955 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2021-08-05 18:28:25.009018 (MainThread): Partial parsing not enabled
2021-08-05 18:28:25.038482 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:28:25.038619 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.055197 (MainThread): Acquiring new postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:28:25.055330 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.064279 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:28:25.064409 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.069111 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2021-08-05 18:28:25.069218 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.073430 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:28:25.073540 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.077774 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:28:25.077877 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.082012 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2021-08-05 18:28:25.082108 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.086196 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:28:25.086294 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.090481 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2021-08-05 18:28:25.090576 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.095093 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:28:25.095225 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.101754 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:28:25.101874 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.109003 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:28:25.109112 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.115343 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:28:25.115448 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.122431 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:28:25.122532 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.128713 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:28:25.128815 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.136503 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:28:25.136622 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.143049 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:28:25.143161 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.317599 (MainThread): Found 17 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2021-08-05 18:28:25.325230 (MainThread): 
2021-08-05 18:28:25.325627 (MainThread): Acquiring new postgres connection "master".
2021-08-05 18:28:25.325725 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:28:25.330587 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2021-08-05 18:28:25.330816 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-08-05 18:28:25.429081 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2021-08-05 18:28:25.429216 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2021-08-05 18:28:26.104681 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.68 seconds
2021-08-05 18:28:26.157821 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:28:26.158014 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2021-08-05 18:28:26.159407 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:28:26.159506 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2021-08-05 18:28:26.204224 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.04 seconds
2021-08-05 18:28:26.204381 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:28:26.204466 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2021-08-05 18:28:26.279219 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.07 seconds
2021-08-05 18:28:26.306674 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2021-08-05 18:28:26.410113 (MainThread): Using postgres connection "master".
2021-08-05 18:28:26.410242 (MainThread): On master: BEGIN
2021-08-05 18:28:26.838924 (MainThread): SQL status: BEGIN in 0.43 seconds
2021-08-05 18:28:26.839321 (MainThread): Using postgres connection "master".
2021-08-05 18:28:26.839584 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2021-08-05 18:28:27.011300 (MainThread): SQL status: SELECT in 0.17 seconds
2021-08-05 18:28:27.251054 (MainThread): On master: ROLLBACK
2021-08-05 18:28:27.299176 (MainThread): Using postgres connection "master".
2021-08-05 18:28:27.299332 (MainThread): On master: BEGIN
2021-08-05 18:28:27.394768 (MainThread): SQL status: BEGIN in 0.10 seconds
2021-08-05 18:28:27.395036 (MainThread): On master: COMMIT
2021-08-05 18:28:27.395181 (MainThread): Using postgres connection "master".
2021-08-05 18:28:27.395285 (MainThread): On master: COMMIT
2021-08-05 18:28:27.443002 (MainThread): SQL status: COMMIT in 0.05 seconds
2021-08-05 18:28:27.443450 (MainThread): 11:28:27 | Concurrency: 1 threads (target='dev')
2021-08-05 18:28:27.443621 (MainThread): 11:28:27 | 
2021-08-05 18:28:27.445292 (Thread-1): Began running node model.customer_history.dim_customers_email_acxiom
2021-08-05 18:28:27.445501 (Thread-1): 11:28:27 | 1 of 1 START table model data_science.dim_customers_email_acxiom..... [RUN]
2021-08-05 18:28:27.445830 (Thread-1): Acquiring new postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:28:27.445947 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2021-08-05 18:28:27.446066 (Thread-1): Compiling model.customer_history.dim_customers_email_acxiom
2021-08-05 18:28:27.464330 (Thread-1): Writing injected SQL for node "model.customer_history.dim_customers_email_acxiom"
2021-08-05 18:28:27.465029 (Thread-1): finished collecting timing info
2021-08-05 18:28:27.489662 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:28:27.489838 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_tmp" cascade
2021-08-05 18:28:27.590704 (Thread-1): SQL status: DROP TABLE in 0.10 seconds
2021-08-05 18:28:27.592997 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:28:27.593100 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_backup" cascade
2021-08-05 18:28:27.639188 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:28:27.654039 (Thread-1): Writing runtime SQL for node "model.customer_history.dim_customers_email_acxiom"
2021-08-05 18:28:27.654549 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:28:27.654648 (Thread-1): On model.customer_history.dim_customers_email_acxiom: BEGIN
2021-08-05 18:28:27.702614 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:28:27.702790 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:28:27.702874 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */


  create  table "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_tmp"
  as (
    

with orders as (
    select * from "data_platform_prod"."data_science"."fct_order_ticket_details"
),

customer_orders as (
    select
        axs_email_hash,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        COUNT(DISTINCT event_unique_id) AS number_of_events,

        SUM(CASE WHEN order_ticket_identifier=1 THEN amount_gross ELSE 0 END) AS total_revenue,

        SUM(FLOOR(COALESCE((CASE WHEN order_ticket_identifier=1 THEN days_sold_after_onsale ELSE 0 END), 0))) / COUNT(DISTINCT CASE WHEN days_sold_after_onsale IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE((CASE WHEN order_ticket_identifier=1 THEN  days_sold_before_event ELSE 0 END), 0)))/ COUNT(DISTINCT CASE WHEN days_sold_before_event IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers,

        AVG(CASE WHEN order_ticket_identifier=1 THEN order_distance_in_km ELSE NULL END) AS average_order_distance_in_km,

        COUNT(DISTINCT venue_unique_id) AS number_of_venues,

        ROUND(COUNT(DISTINCT CASE WHEN channel='Back Office' THEN order_ticket_unique_id ELSE NULL END) *1.0 / number_of_tickets_sold, 2) AS channel_back_office_percent,
        ROUND(COUNT(DISTINCT CASE WHEN channel='Web' THEN
            order_ticket_unique_id ELSE NULL END) *1.0 / number_of_tickets_sold, 2) AS channel_web_percent,

        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Sports' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_sports_percent,
        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Music' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_music_percent,
        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Arts & Family' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_arts_family_percent,

        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Arena' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_arena_percent,
        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Large Music Venue' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_large_music_percent,
        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Club and Theater' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_club_theatre_percent,

        COUNT(DISTINCT CASE WHEN is_season_ticket = 1 THEN order_ticket_unique_id ELSE NULL END) AS number_of_season_tickets
        -- COUNT(DISTINCT CASE WHEN price_code_type ilike '%season%' THEN order_ticket_unique_id ELSE NULL END) AS number_of_season_tickets

    from orders
    WHERE is_canceled is FALSE -- shall this condition live elsewhere?
    group by 1 
)
select * from customer_orders
  );
2021-08-05 18:33:30.252026 (Thread-1): SQL status: SELECT in 302.55 seconds
2021-08-05 18:33:30.258298 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:33:30.258462 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
alter table "data_platform_prod"."data_science"."dim_customers_email_acxiom" rename to "dim_customers_email_acxiom__dbt_backup"
2021-08-05 18:33:30.308418 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:33:30.312854 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:33:30.313014 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
alter table "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_tmp" rename to "dim_customers_email_acxiom"
2021-08-05 18:33:30.369295 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:33:30.371249 (Thread-1): On model.customer_history.dim_customers_email_acxiom: COMMIT
2021-08-05 18:33:30.371450 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:33:30.371613 (Thread-1): On model.customer_history.dim_customers_email_acxiom: COMMIT
2021-08-05 18:33:30.629194 (Thread-1): SQL status: COMMIT in 0.26 seconds
2021-08-05 18:33:30.631061 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:33:30.631176 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_backup" cascade
2021-08-05 18:33:30.923349 (Thread-1): SQL status: DROP TABLE in 0.29 seconds
2021-08-05 18:33:30.926478 (Thread-1): finished collecting timing info
2021-08-05 18:33:30.927326 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e13b392-bc8e-4966-9f62-bcc7f543b106', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041eacd0>]}
2021-08-05 18:33:30.927670 (Thread-1): 11:33:30 | 1 of 1 OK created table model data_science.dim_customers_email_acxiom [SELECT in 303.48s]
2021-08-05 18:33:30.927855 (Thread-1): Finished running node model.customer_history.dim_customers_email_acxiom
2021-08-05 18:33:30.962784 (MainThread): Using postgres connection "master".
2021-08-05 18:33:30.962930 (MainThread): On master: BEGIN
2021-08-05 18:33:31.036144 (MainThread): SQL status: BEGIN in 0.07 seconds
2021-08-05 18:33:31.036447 (MainThread): On master: COMMIT
2021-08-05 18:33:31.036633 (MainThread): Using postgres connection "master".
2021-08-05 18:33:31.036795 (MainThread): On master: COMMIT
2021-08-05 18:33:31.097521 (MainThread): SQL status: COMMIT in 0.06 seconds
2021-08-05 18:33:31.098213 (MainThread): 11:33:31 | 
2021-08-05 18:33:31.098462 (MainThread): 11:33:31 | Finished running 1 table model in 305.77s.
2021-08-05 18:33:31.098667 (MainThread): Connection 'master' was left open.
2021-08-05 18:33:31.098828 (MainThread): On master: Close
2021-08-05 18:33:31.099259 (MainThread): Connection 'model.customer_history.dim_customers_email_acxiom' was left open.
2021-08-05 18:33:31.099445 (MainThread): On model.customer_history.dim_customers_email_acxiom: Close
2021-08-05 18:33:31.105311 (MainThread): 
2021-08-05 18:33:31.105501 (MainThread): Completed successfully
2021-08-05 18:33:31.105644 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2021-08-05 18:33:31.105837 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fd90d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fd9790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fd9c50>]}
2021-08-05 18:33:31.106052 (MainThread): Flushing usage events
2021-08-05 18:39:03.231788 (MainThread): Running with dbt=0.16.1
2021-08-05 18:39:03.310889 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/Users/jdeng/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-08-05 18:39:03.312145 (MainThread): Tracking: tracking
2021-08-05 18:39:03.321686 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111876f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11186ea10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ac2b50>]}
2021-08-05 18:39:03.346525 (MainThread): Partial parsing not enabled
2021-08-05 18:39:03.350331 (MainThread): Parsing macros/core.sql
2021-08-05 18:39:03.356599 (MainThread): Parsing macros/materializations/helpers.sql
2021-08-05 18:39:03.366761 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-08-05 18:39:03.368922 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-08-05 18:39:03.387693 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-08-05 18:39:03.423008 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-08-05 18:39:03.445301 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-08-05 18:39:03.447382 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-08-05 18:39:03.454074 (MainThread): Parsing macros/materializations/common/merge.sql
2021-08-05 18:39:03.467420 (MainThread): Parsing macros/materializations/table/table.sql
2021-08-05 18:39:03.474636 (MainThread): Parsing macros/materializations/view/view.sql
2021-08-05 18:39:03.481310 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-08-05 18:39:03.486682 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-08-05 18:39:03.487752 (MainThread): Parsing macros/etc/query.sql
2021-08-05 18:39:03.488915 (MainThread): Parsing macros/etc/is_incremental.sql
2021-08-05 18:39:03.490783 (MainThread): Parsing macros/etc/get_relation_comment.sql
2021-08-05 18:39:03.493037 (MainThread): Parsing macros/etc/datetime.sql
2021-08-05 18:39:03.502639 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-08-05 18:39:03.504753 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-08-05 18:39:03.505855 (MainThread): Parsing macros/adapters/common.sql
2021-08-05 18:39:03.548859 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-08-05 18:39:03.550376 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-08-05 18:39:03.551512 (MainThread): Parsing macros/schema_tests/unique.sql
2021-08-05 18:39:03.552692 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-08-05 18:39:03.555086 (MainThread): Parsing macros/catalog.sql
2021-08-05 18:39:03.557967 (MainThread): Parsing macros/relations.sql
2021-08-05 18:39:03.559677 (MainThread): Parsing macros/adapters.sql
2021-08-05 18:39:03.577619 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2021-08-05 18:39:03.598089 (MainThread): Partial parsing not enabled
2021-08-05 18:39:03.629222 (MainThread): Acquiring new postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 18:39:03.629363 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.646164 (MainThread): Acquiring new postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:39:03.646294 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.654693 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:03.654809 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.660052 (MainThread): Acquiring new postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:03.660166 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.664391 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:03.664490 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.668754 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:03.668857 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.673183 (MainThread): Acquiring new postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:03.673292 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.677486 (MainThread): Acquiring new postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:03.677588 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.681825 (MainThread): Acquiring new postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:03.681925 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.686679 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:03.686792 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.693685 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:03.693804 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.700939 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:03.701041 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.707303 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:03.707413 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.714354 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:03.714457 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.720743 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:03.720853 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.728443 (MainThread): Acquiring new postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:03.728557 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.735118 (MainThread): Acquiring new postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:03.735228 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.912996 (MainThread): Found 17 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 0 sources
2021-08-05 18:39:03.920241 (MainThread): 
2021-08-05 18:39:03.920597 (MainThread): Acquiring new postgres connection "master".
2021-08-05 18:39:03.920730 (MainThread): Opening a new connection, currently in state init
2021-08-05 18:39:03.972214 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_data_platform_prod".
2021-08-05 18:39:03.972419 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-08-05 18:39:04.060244 (ThreadPoolExecutor-0_0): Using postgres connection "list_data_platform_prod".
2021-08-05 18:39:04.060384 (ThreadPoolExecutor-0_0): On list_data_platform_prod: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod"} */

    select distinct nspname from pg_namespace
  
2021-08-05 18:39:04.716311 (ThreadPoolExecutor-0_0): SQL status: SELECT in 0.66 seconds
2021-08-05 18:39:04.785810 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:39:04.786034 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_data_platform_prod).
2021-08-05 18:39:04.787766 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:39:04.787891 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: BEGIN
2021-08-05 18:39:04.836170 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:04.836677 (ThreadPoolExecutor-1_0): Using postgres connection "list_data_platform_prod_data_science".
2021-08-05 18:39:04.836992 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "list_data_platform_prod_data_science"} */
select
      'data_platform_prod' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'data_science'
    union all
    select
      'data_platform_prod' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'data_science'
  
2021-08-05 18:39:04.954471 (ThreadPoolExecutor-1_0): SQL status: SELECT in 0.12 seconds
2021-08-05 18:39:04.988807 (ThreadPoolExecutor-1_0): On list_data_platform_prod_data_science: ROLLBACK
2021-08-05 18:39:05.094060 (MainThread): Using postgres connection "master".
2021-08-05 18:39:05.094204 (MainThread): On master: BEGIN
2021-08-05 18:39:05.577701 (MainThread): SQL status: BEGIN in 0.48 seconds
2021-08-05 18:39:05.577952 (MainThread): Using postgres connection "master".
2021-08-05 18:39:05.578079 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2021-08-05 18:39:05.764167 (MainThread): SQL status: SELECT in 0.19 seconds
2021-08-05 18:39:06.030936 (MainThread): On master: ROLLBACK
2021-08-05 18:39:06.082360 (MainThread): Using postgres connection "master".
2021-08-05 18:39:06.082530 (MainThread): On master: BEGIN
2021-08-05 18:39:06.180483 (MainThread): SQL status: BEGIN in 0.10 seconds
2021-08-05 18:39:06.180727 (MainThread): On master: COMMIT
2021-08-05 18:39:06.180861 (MainThread): Using postgres connection "master".
2021-08-05 18:39:06.180962 (MainThread): On master: COMMIT
2021-08-05 18:39:06.232766 (MainThread): SQL status: COMMIT in 0.05 seconds
2021-08-05 18:39:06.233647 (MainThread): 11:39:06 | Concurrency: 1 threads (target='dev')
2021-08-05 18:39:06.233895 (MainThread): 11:39:06 | 
2021-08-05 18:39:06.236025 (Thread-1): Began running node model.customer_history.stg_events
2021-08-05 18:39:06.236260 (Thread-1): 11:39:06 | 1 of 17 START view model data_science.stg_events..................... [RUN]
2021-08-05 18:39:06.236625 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.236767 (Thread-1): Re-using an available connection from the pool (formerly list_data_platform_prod_data_science).
2021-08-05 18:39:06.236910 (Thread-1): Compiling model.customer_history.stg_events
2021-08-05 18:39:06.253346 (Thread-1): Writing injected SQL for node "model.customer_history.stg_events"
2021-08-05 18:39:06.254080 (Thread-1): finished collecting timing info
2021-08-05 18:39:06.296820 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.297000 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_tmp" cascade
2021-08-05 18:39:06.401489 (Thread-1): SQL status: DROP VIEW in 0.10 seconds
2021-08-05 18:39:06.404370 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.404499 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2021-08-05 18:39:06.453378 (Thread-1): SQL status: DROP VIEW in 0.05 seconds
2021-08-05 18:39:06.457150 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_events"
2021-08-05 18:39:06.457803 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.457951 (Thread-1): On model.customer_history.stg_events: BEGIN
2021-08-05 18:39:06.507147 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:06.507582 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.507873 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */

  create view "data_platform_prod"."data_science"."stg_events__dbt_tmp" as (
    with events as (
    SELECT
        event_unique_id,
        onsale_date,
        event_datetime,
        venue_unique_id,
        major_category_name
    FROM
        ticketing.events
        INNER JOIN analytics.event_onsale USING (event_unique_id)
        LEFT JOIN analytics.mdl_major_category_event USING (event_unique_id)
    WHERE event_name NOT ilike 'test event%'
        AND event_name NOT ilike '%base event%'
        AND event_name NOT ilike '% test event%'
        AND event_name NOT ilike '%- RR Base%'
        AND (nvl(ticketing.events.is_exclude,false)) is false
),
venues as (
    SELECT
        venue_unique_id,
        left(venue_zip, 5) as venue_zip,
        venue_type
        from ticketing.venues LEFT JOIN data_science.venue_type
        USING (venue_unique_id)
),
final as (
    SELECT
        *
    FROM events INNER JOIN venues USING (venue_unique_id)
)
SELECT * FROM final
  );

2021-08-05 18:39:06.590907 (Thread-1): SQL status: CREATE VIEW in 0.08 seconds
2021-08-05 18:39:06.595635 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.595790 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events" rename to "stg_events__dbt_backup"
2021-08-05 18:39:06.651362 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:39:06.654151 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.654280 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
alter table "data_platform_prod"."data_science"."stg_events__dbt_tmp" rename to "stg_events"
2021-08-05 18:39:06.704059 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:06.705314 (Thread-1): On model.customer_history.stg_events: COMMIT
2021-08-05 18:39:06.705464 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.705581 (Thread-1): On model.customer_history.stg_events: COMMIT
2021-08-05 18:39:06.951394 (Thread-1): SQL status: COMMIT in 0.25 seconds
2021-08-05 18:39:06.953737 (Thread-1): Using postgres connection "model.customer_history.stg_events".
2021-08-05 18:39:06.953935 (Thread-1): On model.customer_history.stg_events: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_events"} */
drop view if exists "data_platform_prod"."data_science"."stg_events__dbt_backup" cascade
2021-08-05 18:39:07.165749 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 18:39:07.168870 (Thread-1): finished collecting timing info
2021-08-05 18:39:07.169620 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c8e4d0>]}
2021-08-05 18:39:07.169888 (Thread-1): 11:39:07 | 1 of 17 OK created view model data_science.stg_events................ [CREATE VIEW in 0.93s]
2021-08-05 18:39:07.170041 (Thread-1): Finished running node model.customer_history.stg_events
2021-08-05 18:39:07.170198 (Thread-1): Began running node model.customer_history.stg_flash
2021-08-05 18:39:07.170490 (Thread-1): 11:39:07 | 2 of 17 START view model data_science.stg_flash...................... [RUN]
2021-08-05 18:39:07.170798 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:07.170909 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_events).
2021-08-05 18:39:07.171019 (Thread-1): Compiling model.customer_history.stg_flash
2021-08-05 18:39:07.177071 (Thread-1): Writing injected SQL for node "model.customer_history.stg_flash"
2021-08-05 18:39:07.177508 (Thread-1): finished collecting timing info
2021-08-05 18:39:07.185495 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:07.185641 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_tmp" cascade
2021-08-05 18:39:07.401403 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 18:39:07.403803 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:07.403920 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2021-08-05 18:39:07.629491 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:07.631022 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_flash"
2021-08-05 18:39:07.631468 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:07.631559 (Thread-1): On model.customer_history.stg_flash: BEGIN
2021-08-05 18:39:07.677355 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:07.677518 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:07.677605 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */

  create view "data_platform_prod"."data_science"."stg_flash__dbt_tmp" as (
    SELECT
    ticket_state,
    ticket_id,
    transfer_action_id,
    fk_order_unique_id,
    fk_seat_unique_id
FROM
    flash.tickets LEFT JOIN flash.forwards USING (ticket_id)
  );

2021-08-05 18:39:07.734837 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2021-08-05 18:39:07.738429 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:07.738553 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash" rename to "stg_flash__dbt_backup"
2021-08-05 18:39:08.527657 (Thread-1): SQL status: ALTER TABLE in 0.79 seconds
2021-08-05 18:39:08.530306 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:08.530431 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
alter table "data_platform_prod"."data_science"."stg_flash__dbt_tmp" rename to "stg_flash"
2021-08-05 18:39:08.587322 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:39:08.588247 (Thread-1): On model.customer_history.stg_flash: COMMIT
2021-08-05 18:39:08.588348 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:08.588443 (Thread-1): On model.customer_history.stg_flash: COMMIT
2021-08-05 18:39:08.809261 (Thread-1): SQL status: COMMIT in 0.22 seconds
2021-08-05 18:39:08.812196 (Thread-1): Using postgres connection "model.customer_history.stg_flash".
2021-08-05 18:39:08.812322 (Thread-1): On model.customer_history.stg_flash: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_flash"} */
drop view if exists "data_platform_prod"."data_science"."stg_flash__dbt_backup" cascade
2021-08-05 18:39:09.451449 (Thread-1): SQL status: DROP VIEW in 0.64 seconds
2021-08-05 18:39:09.454455 (Thread-1): finished collecting timing info
2021-08-05 18:39:09.455173 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f0f890>]}
2021-08-05 18:39:09.455431 (Thread-1): 11:39:09 | 2 of 17 OK created view model data_science.stg_flash................. [CREATE VIEW in 2.28s]
2021-08-05 18:39:09.455610 (Thread-1): Finished running node model.customer_history.stg_flash
2021-08-05 18:39:09.455764 (Thread-1): Began running node model.customer_history.stg_customers
2021-08-05 18:39:09.455930 (Thread-1): 11:39:09 | 3 of 17 START view model data_science.stg_customers.................. [RUN]
2021-08-05 18:39:09.456416 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:09.456528 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_flash).
2021-08-05 18:39:09.456635 (Thread-1): Compiling model.customer_history.stg_customers
2021-08-05 18:39:09.462675 (Thread-1): Writing injected SQL for node "model.customer_history.stg_customers"
2021-08-05 18:39:09.463139 (Thread-1): finished collecting timing info
2021-08-05 18:39:09.471044 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:09.471172 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_tmp" cascade
2021-08-05 18:39:09.859321 (Thread-1): SQL status: DROP VIEW in 0.39 seconds
2021-08-05 18:39:09.862010 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:09.862121 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2021-08-05 18:39:10.072517 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 18:39:10.074086 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_customers"
2021-08-05 18:39:10.074521 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:10.074611 (Thread-1): On model.customer_history.stg_customers: BEGIN
2021-08-05 18:39:10.122599 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:10.122766 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:10.122853 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */

  create view "data_platform_prod"."data_science"."stg_customers__dbt_tmp" as (
    SELECT 
    axs_customer_id as customer_unique_id,
    axs_email_hash,
    -- left(zip, 5) as zip -- eleminate situation as 01234-1234
    zip_code as zip
FROM analytics.demographics_all -- instead of ticketing.customers

--  no need to join SQL at this moment
--     CASE WHEN b.email is not null THEN 1 ELSE 0 END AS is_broker
-- FROM ticketing.customers c LEFT JOIN analytics.yield_manager_partners b 
-- on lower(c.email)=lower(b.email)
  );

2021-08-05 18:39:10.183856 (Thread-1): SQL status: CREATE VIEW in 0.06 seconds
2021-08-05 18:39:10.187463 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:10.187573 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers" rename to "stg_customers__dbt_backup"
2021-08-05 18:39:10.251417 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:39:10.254012 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:10.254133 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
alter table "data_platform_prod"."data_science"."stg_customers__dbt_tmp" rename to "stg_customers"
2021-08-05 18:39:10.306260 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:10.307193 (Thread-1): On model.customer_history.stg_customers: COMMIT
2021-08-05 18:39:10.307295 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:10.307372 (Thread-1): On model.customer_history.stg_customers: COMMIT
2021-08-05 18:39:10.551448 (Thread-1): SQL status: COMMIT in 0.24 seconds
2021-08-05 18:39:10.553217 (Thread-1): Using postgres connection "model.customer_history.stg_customers".
2021-08-05 18:39:10.553320 (Thread-1): On model.customer_history.stg_customers: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_customers"} */
drop view if exists "data_platform_prod"."data_science"."stg_customers__dbt_backup" cascade
2021-08-05 18:39:10.801573 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2021-08-05 18:39:10.804650 (Thread-1): finished collecting timing info
2021-08-05 18:39:10.805631 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d42ed0>]}
2021-08-05 18:39:10.805962 (Thread-1): 11:39:10 | 3 of 17 OK created view model data_science.stg_customers............. [CREATE VIEW in 1.35s]
2021-08-05 18:39:10.806140 (Thread-1): Finished running node model.customer_history.stg_customers
2021-08-05 18:39:10.806355 (Thread-1): Began running node model.customer_history.stg_order_cst
2021-08-05 18:39:10.806556 (Thread-1): 11:39:10 | 4 of 17 START view model data_science.stg_order_cst.................. [RUN]
2021-08-05 18:39:10.807368 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:10.807573 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_customers).
2021-08-05 18:39:10.807705 (Thread-1): Compiling model.customer_history.stg_order_cst
2021-08-05 18:39:10.814546 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order_cst"
2021-08-05 18:39:10.815013 (Thread-1): finished collecting timing info
2021-08-05 18:39:10.824512 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:10.824691 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_cst__dbt_tmp" cascade
2021-08-05 18:39:11.034177 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 18:39:11.036473 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:11.036586 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_cst__dbt_backup" cascade
2021-08-05 18:39:11.251425 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 18:39:11.253044 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order_cst"
2021-08-05 18:39:11.253658 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:11.253765 (Thread-1): On model.customer_history.stg_order_cst: BEGIN
2021-08-05 18:39:11.308735 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:11.308893 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:11.308978 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */

  create view "data_platform_prod"."data_science"."stg_order_cst__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    -- zone_unique_id,
    pricing_mode_id,
    -- price_code_type,
    CASE WHEN price_code_type ilike '%season%' THEN 1 ELSE 0 END AS is_season_ticket,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN config.context_configuration config USING(context_id)
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
config.timezone = 'CST' and lower(zone_type_description) in ('admissions', 'premium seating')
  );

2021-08-05 18:39:11.401632 (Thread-1): SQL status: CREATE VIEW in 0.09 seconds
2021-08-05 18:39:11.406313 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:11.406456 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
alter table "data_platform_prod"."data_science"."stg_order_cst" rename to "stg_order_cst__dbt_backup"
2021-08-05 18:39:11.465272 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:39:11.467749 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:11.467848 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
alter table "data_platform_prod"."data_science"."stg_order_cst__dbt_tmp" rename to "stg_order_cst"
2021-08-05 18:39:11.541461 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 18:39:11.542468 (Thread-1): On model.customer_history.stg_order_cst: COMMIT
2021-08-05 18:39:11.542592 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:11.542681 (Thread-1): On model.customer_history.stg_order_cst: COMMIT
2021-08-05 18:39:11.781921 (Thread-1): SQL status: COMMIT in 0.24 seconds
2021-08-05 18:39:11.786884 (Thread-1): Using postgres connection "model.customer_history.stg_order_cst".
2021-08-05 18:39:11.787054 (Thread-1): On model.customer_history.stg_order_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_cst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_cst__dbt_backup" cascade
2021-08-05 18:39:12.004726 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 18:39:12.007207 (Thread-1): finished collecting timing info
2021-08-05 18:39:12.007879 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f1a110>]}
2021-08-05 18:39:12.008143 (Thread-1): 11:39:12 | 4 of 17 OK created view model data_science.stg_order_cst............. [CREATE VIEW in 1.20s]
2021-08-05 18:39:12.008266 (Thread-1): Finished running node model.customer_history.stg_order_cst
2021-08-05 18:39:12.008388 (Thread-1): Began running node model.customer_history.stg_order_est
2021-08-05 18:39:12.008609 (Thread-1): 11:39:12 | 5 of 17 START view model data_science.stg_order_est.................. [RUN]
2021-08-05 18:39:12.008998 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.009091 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order_cst).
2021-08-05 18:39:12.009195 (Thread-1): Compiling model.customer_history.stg_order_est
2021-08-05 18:39:12.014418 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order_est"
2021-08-05 18:39:12.014875 (Thread-1): finished collecting timing info
2021-08-05 18:39:12.021402 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.021532 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_est__dbt_tmp" cascade
2021-08-05 18:39:12.201448 (Thread-1): SQL status: DROP VIEW in 0.18 seconds
2021-08-05 18:39:12.203810 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.203924 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_est__dbt_backup" cascade
2021-08-05 18:39:12.410973 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 18:39:12.412487 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order_est"
2021-08-05 18:39:12.413005 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.413096 (Thread-1): On model.customer_history.stg_order_est: BEGIN
2021-08-05 18:39:12.472489 (Thread-1): SQL status: BEGIN in 0.06 seconds
2021-08-05 18:39:12.472667 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.472756 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */

  create view "data_platform_prod"."data_science"."stg_order_est__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    -- zone_unique_id,
    pricing_mode_id,
    -- price_code_type,
    CASE WHEN price_code_type ilike '%season%' THEN 1 ELSE 0 END AS is_season_ticket,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN config.context_configuration config USING(context_id)
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
config.timezone = 'EST' and lower(zone_type_description) in ('admissions', 'premium seating')
  );

2021-08-05 18:39:12.570790 (Thread-1): SQL status: CREATE VIEW in 0.10 seconds
2021-08-05 18:39:12.574632 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.574792 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
alter table "data_platform_prod"."data_science"."stg_order_est" rename to "stg_order_est__dbt_backup"
2021-08-05 18:39:12.630660 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:39:12.634100 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.634240 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
alter table "data_platform_prod"."data_science"."stg_order_est__dbt_tmp" rename to "stg_order_est"
2021-08-05 18:39:12.701473 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 18:39:12.702713 (Thread-1): On model.customer_history.stg_order_est: COMMIT
2021-08-05 18:39:12.702868 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.702951 (Thread-1): On model.customer_history.stg_order_est: COMMIT
2021-08-05 18:39:12.976845 (Thread-1): SQL status: COMMIT in 0.27 seconds
2021-08-05 18:39:12.978628 (Thread-1): Using postgres connection "model.customer_history.stg_order_est".
2021-08-05 18:39:12.978731 (Thread-1): On model.customer_history.stg_order_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_est"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_est__dbt_backup" cascade
2021-08-05 18:39:13.309039 (Thread-1): SQL status: DROP VIEW in 0.33 seconds
2021-08-05 18:39:13.311340 (Thread-1): finished collecting timing info
2021-08-05 18:39:13.311923 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111de5090>]}
2021-08-05 18:39:13.312130 (Thread-1): 11:39:13 | 5 of 17 OK created view model data_science.stg_order_est............. [CREATE VIEW in 1.30s]
2021-08-05 18:39:13.312249 (Thread-1): Finished running node model.customer_history.stg_order_est
2021-08-05 18:39:13.312370 (Thread-1): Began running node model.customer_history.stg_order_mst
2021-08-05 18:39:13.312648 (Thread-1): 11:39:13 | 6 of 17 START view model data_science.stg_order_mst.................. [RUN]
2021-08-05 18:39:13.313030 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:13.313129 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order_est).
2021-08-05 18:39:13.313218 (Thread-1): Compiling model.customer_history.stg_order_mst
2021-08-05 18:39:13.318068 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order_mst"
2021-08-05 18:39:13.318454 (Thread-1): finished collecting timing info
2021-08-05 18:39:13.324649 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:13.324773 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_mst__dbt_tmp" cascade
2021-08-05 18:39:13.553791 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:13.557379 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:13.557538 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_mst__dbt_backup" cascade
2021-08-05 18:39:13.801576 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2021-08-05 18:39:13.803088 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order_mst"
2021-08-05 18:39:13.803589 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:13.803696 (Thread-1): On model.customer_history.stg_order_mst: BEGIN
2021-08-05 18:39:13.859375 (Thread-1): SQL status: BEGIN in 0.06 seconds
2021-08-05 18:39:13.859570 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:13.859656 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */

  create view "data_platform_prod"."data_science"."stg_order_mst__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    -- zone_unique_id,
    pricing_mode_id,
    -- price_code_type,
    CASE WHEN price_code_type ilike '%season%' THEN 1 ELSE 0 END AS is_season_ticket,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN config.context_configuration config USING(context_id)
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
config.timezone = 'MST' and lower(zone_type_description) in ('admissions', 'premium seating')
  );

2021-08-05 18:39:13.969248 (Thread-1): SQL status: CREATE VIEW in 0.11 seconds
2021-08-05 18:39:13.973209 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:13.973335 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
alter table "data_platform_prod"."data_science"."stg_order_mst" rename to "stg_order_mst__dbt_backup"
2021-08-05 18:39:14.039099 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 18:39:14.041371 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:14.041473 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
alter table "data_platform_prod"."data_science"."stg_order_mst__dbt_tmp" rename to "stg_order_mst"
2021-08-05 18:39:14.115773 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 18:39:14.116682 (Thread-1): On model.customer_history.stg_order_mst: COMMIT
2021-08-05 18:39:14.116785 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:14.116866 (Thread-1): On model.customer_history.stg_order_mst: COMMIT
2021-08-05 18:39:14.334603 (Thread-1): SQL status: COMMIT in 0.22 seconds
2021-08-05 18:39:14.336465 (Thread-1): Using postgres connection "model.customer_history.stg_order_mst".
2021-08-05 18:39:14.336575 (Thread-1): On model.customer_history.stg_order_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_mst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_mst__dbt_backup" cascade
2021-08-05 18:39:14.572901 (Thread-1): SQL status: DROP VIEW in 0.24 seconds
2021-08-05 18:39:14.575222 (Thread-1): finished collecting timing info
2021-08-05 18:39:14.575850 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ac4110>]}
2021-08-05 18:39:14.576154 (Thread-1): 11:39:14 | 6 of 17 OK created view model data_science.stg_order_mst............. [CREATE VIEW in 1.26s]
2021-08-05 18:39:14.576269 (Thread-1): Finished running node model.customer_history.stg_order_mst
2021-08-05 18:39:14.576384 (Thread-1): Began running node model.customer_history.stg_order_pst
2021-08-05 18:39:14.576530 (Thread-1): 11:39:14 | 7 of 17 START view model data_science.stg_order_pst.................. [RUN]
2021-08-05 18:39:14.577330 (Thread-1): Acquiring new postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:14.577429 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order_mst).
2021-08-05 18:39:14.577515 (Thread-1): Compiling model.customer_history.stg_order_pst
2021-08-05 18:39:14.582346 (Thread-1): Writing injected SQL for node "model.customer_history.stg_order_pst"
2021-08-05 18:39:14.582832 (Thread-1): finished collecting timing info
2021-08-05 18:39:14.589785 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:14.589888 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_pst__dbt_tmp" cascade
2021-08-05 18:39:14.820799 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:14.823956 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:14.824091 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_pst__dbt_backup" cascade
2021-08-05 18:39:15.051709 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:15.053150 (Thread-1): Writing runtime SQL for node "model.customer_history.stg_order_pst"
2021-08-05 18:39:15.053775 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:15.053884 (Thread-1): On model.customer_history.stg_order_pst: BEGIN
2021-08-05 18:39:15.105823 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:15.105995 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:15.106079 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */

  create view "data_platform_prod"."data_science"."stg_order_pst__dbt_tmp" as (
    select
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    -- zone_unique_id,
    pricing_mode_id,
    -- price_code_type,
    CASE WHEN price_code_type ilike '%season%' THEN 1 ELSE 0 END AS is_season_ticket,
    seat_unique_id,
    ticketing.order_tickets.event_unique_id,
    is_canceled
from ticketing.order_tickets
INNER JOIN config.context_configuration config USING(context_id)
INNER JOIN ticketing.price_codes USING(price_code_unique_id)
INNER JOIN ticketing.zones USING (zone_unique_id)
WHERE 
config.timezone = 'PST' and lower(zone_type_description) in ('admissions', 'premium seating')
  );

2021-08-05 18:39:15.326061 (Thread-1): SQL status: CREATE VIEW in 0.22 seconds
2021-08-05 18:39:15.330073 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:15.330230 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
alter table "data_platform_prod"."data_science"."stg_order_pst" rename to "stg_order_pst__dbt_backup"
2021-08-05 18:39:15.385229 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:15.387499 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:15.387596 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
alter table "data_platform_prod"."data_science"."stg_order_pst__dbt_tmp" rename to "stg_order_pst"
2021-08-05 18:39:15.440742 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:15.441780 (Thread-1): On model.customer_history.stg_order_pst: COMMIT
2021-08-05 18:39:15.441899 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:15.441995 (Thread-1): On model.customer_history.stg_order_pst: COMMIT
2021-08-05 18:39:15.701573 (Thread-1): SQL status: COMMIT in 0.26 seconds
2021-08-05 18:39:15.703674 (Thread-1): Using postgres connection "model.customer_history.stg_order_pst".
2021-08-05 18:39:15.703806 (Thread-1): On model.customer_history.stg_order_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.stg_order_pst"} */
drop view if exists "data_platform_prod"."data_science"."stg_order_pst__dbt_backup" cascade
2021-08-05 18:39:15.925081 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 18:39:15.928011 (Thread-1): finished collecting timing info
2021-08-05 18:39:15.928741 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d60f90>]}
2021-08-05 18:39:15.929005 (Thread-1): 11:39:15 | 7 of 17 OK created view model data_science.stg_order_pst............. [CREATE VIEW in 1.35s]
2021-08-05 18:39:15.929160 (Thread-1): Finished running node model.customer_history.stg_order_pst
2021-08-05 18:39:15.929321 (Thread-1): Began running node model.customer_history.order_flash_events_cst
2021-08-05 18:39:15.929614 (Thread-1): 11:39:15 | 8 of 17 START view model data_science.order_flash_events_cst......... [RUN]
2021-08-05 18:39:15.930075 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:15.930205 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.stg_order_pst).
2021-08-05 18:39:15.930321 (Thread-1): Compiling model.customer_history.order_flash_events_cst
2021-08-05 18:39:15.940934 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events_cst"
2021-08-05 18:39:15.941425 (Thread-1): finished collecting timing info
2021-08-05 18:39:15.949833 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:15.950008 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_cst__dbt_tmp" cascade
2021-08-05 18:39:16.208762 (Thread-1): SQL status: DROP VIEW in 0.26 seconds
2021-08-05 18:39:16.210936 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:16.211033 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_cst__dbt_backup" cascade
2021-08-05 18:39:16.514124 (Thread-1): SQL status: DROP VIEW in 0.30 seconds
2021-08-05 18:39:16.515753 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events_cst"
2021-08-05 18:39:16.516446 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:16.516561 (Thread-1): On model.customer_history.order_flash_events_cst: BEGIN
2021-08-05 18:39:16.574476 (Thread-1): SQL status: BEGIN in 0.06 seconds
2021-08-05 18:39:16.574684 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:16.574767 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */

  create view "data_platform_prod"."data_science"."order_flash_events_cst__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order_cst"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),

final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    pricing_mode_id,
    is_season_ticket,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    venue_unique_id,
    venue_zip,
    venue_type,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    major_category_name,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
  );

2021-08-05 18:39:16.699614 (Thread-1): SQL status: CREATE VIEW in 0.12 seconds
2021-08-05 18:39:16.701875 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:16.701974 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */
alter table "data_platform_prod"."data_science"."order_flash_events_cst__dbt_tmp" rename to "order_flash_events_cst"
2021-08-05 18:39:16.764059 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:39:16.764977 (Thread-1): On model.customer_history.order_flash_events_cst: COMMIT
2021-08-05 18:39:16.765080 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:16.765162 (Thread-1): On model.customer_history.order_flash_events_cst: COMMIT
2021-08-05 18:39:16.962607 (Thread-1): SQL status: COMMIT in 0.20 seconds
2021-08-05 18:39:16.965467 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_cst".
2021-08-05 18:39:16.965572 (Thread-1): On model.customer_history.order_flash_events_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_cst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_cst__dbt_backup" cascade
2021-08-05 18:39:17.335035 (Thread-1): SQL status: DROP VIEW in 0.37 seconds
2021-08-05 18:39:17.337251 (Thread-1): finished collecting timing info
2021-08-05 18:39:17.337800 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111bc6850>]}
2021-08-05 18:39:17.337998 (Thread-1): 11:39:17 | 8 of 17 OK created view model data_science.order_flash_events_cst.... [CREATE VIEW in 1.41s]
2021-08-05 18:39:17.338114 (Thread-1): Finished running node model.customer_history.order_flash_events_cst
2021-08-05 18:39:17.338282 (Thread-1): Began running node model.customer_history.order_flash_events_est
2021-08-05 18:39:17.338624 (Thread-1): 11:39:17 | 9 of 17 START view model data_science.order_flash_events_est......... [RUN]
2021-08-05 18:39:17.339106 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:17.339333 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events_cst).
2021-08-05 18:39:17.339434 (Thread-1): Compiling model.customer_history.order_flash_events_est
2021-08-05 18:39:17.347543 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events_est"
2021-08-05 18:39:17.348109 (Thread-1): finished collecting timing info
2021-08-05 18:39:17.354157 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:17.354261 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_est__dbt_tmp" cascade
2021-08-05 18:39:17.574389 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 18:39:17.577044 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:17.577162 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_est__dbt_backup" cascade
2021-08-05 18:39:17.789448 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 18:39:17.791215 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events_est"
2021-08-05 18:39:17.791694 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:17.791803 (Thread-1): On model.customer_history.order_flash_events_est: BEGIN
2021-08-05 18:39:17.848393 (Thread-1): SQL status: BEGIN in 0.06 seconds
2021-08-05 18:39:17.848605 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:17.848689 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */

  create view "data_platform_prod"."data_science"."order_flash_events_est__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order_est"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),

final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    pricing_mode_id,
    is_season_ticket,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    venue_unique_id,
    venue_zip,
    venue_type,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    major_category_name,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
  );

2021-08-05 18:39:17.961130 (Thread-1): SQL status: CREATE VIEW in 0.11 seconds
2021-08-05 18:39:17.963536 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:17.963652 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */
alter table "data_platform_prod"."data_science"."order_flash_events_est__dbt_tmp" rename to "order_flash_events_est"
2021-08-05 18:39:18.012689 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:18.013630 (Thread-1): On model.customer_history.order_flash_events_est: COMMIT
2021-08-05 18:39:18.013731 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:18.013811 (Thread-1): On model.customer_history.order_flash_events_est: COMMIT
2021-08-05 18:39:19.120824 (Thread-1): SQL status: COMMIT in 1.11 seconds
2021-08-05 18:39:19.122744 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_est".
2021-08-05 18:39:19.122866 (Thread-1): On model.customer_history.order_flash_events_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_est"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_est__dbt_backup" cascade
2021-08-05 18:39:19.351500 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:19.354143 (Thread-1): finished collecting timing info
2021-08-05 18:39:19.354794 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d43850>]}
2021-08-05 18:39:19.355021 (Thread-1): 11:39:19 | 9 of 17 OK created view model data_science.order_flash_events_est.... [CREATE VIEW in 2.02s]
2021-08-05 18:39:19.355156 (Thread-1): Finished running node model.customer_history.order_flash_events_est
2021-08-05 18:39:19.355293 (Thread-1): Began running node model.customer_history.order_flash_events_mst
2021-08-05 18:39:19.355430 (Thread-1): 11:39:19 | 10 of 17 START view model data_science.order_flash_events_mst........ [RUN]
2021-08-05 18:39:19.355677 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:19.355775 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events_est).
2021-08-05 18:39:19.355873 (Thread-1): Compiling model.customer_history.order_flash_events_mst
2021-08-05 18:39:19.365159 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events_mst"
2021-08-05 18:39:19.365662 (Thread-1): finished collecting timing info
2021-08-05 18:39:19.372680 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:19.372788 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_mst__dbt_tmp" cascade
2021-08-05 18:39:19.604125 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:19.606383 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:19.606478 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_mst__dbt_backup" cascade
2021-08-05 18:39:19.838182 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:19.840911 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events_mst"
2021-08-05 18:39:19.841395 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:19.841505 (Thread-1): On model.customer_history.order_flash_events_mst: BEGIN
2021-08-05 18:39:19.898424 (Thread-1): SQL status: BEGIN in 0.06 seconds
2021-08-05 18:39:19.898596 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:19.898682 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */

  create view "data_platform_prod"."data_science"."order_flash_events_mst__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order_mst"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),

final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    pricing_mode_id,
    is_season_ticket,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    venue_unique_id,
    venue_zip,
    venue_type,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    major_category_name,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
  );

2021-08-05 18:39:20.018244 (Thread-1): SQL status: CREATE VIEW in 0.12 seconds
2021-08-05 18:39:20.020764 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:20.020866 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */
alter table "data_platform_prod"."data_science"."order_flash_events_mst__dbt_tmp" rename to "order_flash_events_mst"
2021-08-05 18:39:20.087698 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 18:39:20.088627 (Thread-1): On model.customer_history.order_flash_events_mst: COMMIT
2021-08-05 18:39:20.088726 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:20.088803 (Thread-1): On model.customer_history.order_flash_events_mst: COMMIT
2021-08-05 18:39:20.366150 (Thread-1): SQL status: COMMIT in 0.28 seconds
2021-08-05 18:39:20.367840 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_mst".
2021-08-05 18:39:20.367939 (Thread-1): On model.customer_history.order_flash_events_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_mst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_mst__dbt_backup" cascade
2021-08-05 18:39:20.591563 (Thread-1): SQL status: DROP VIEW in 0.22 seconds
2021-08-05 18:39:20.594241 (Thread-1): finished collecting timing info
2021-08-05 18:39:20.594886 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ef6a50>]}
2021-08-05 18:39:20.595108 (Thread-1): 11:39:20 | 10 of 17 OK created view model data_science.order_flash_events_mst... [CREATE VIEW in 1.24s]
2021-08-05 18:39:20.595234 (Thread-1): Finished running node model.customer_history.order_flash_events_mst
2021-08-05 18:39:20.595365 (Thread-1): Began running node model.customer_history.order_flash_events_pst
2021-08-05 18:39:20.595499 (Thread-1): 11:39:20 | 11 of 17 START view model data_science.order_flash_events_pst........ [RUN]
2021-08-05 18:39:20.596043 (Thread-1): Acquiring new postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:20.596185 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events_mst).
2021-08-05 18:39:20.596297 (Thread-1): Compiling model.customer_history.order_flash_events_pst
2021-08-05 18:39:20.606635 (Thread-1): Writing injected SQL for node "model.customer_history.order_flash_events_pst"
2021-08-05 18:39:20.607129 (Thread-1): finished collecting timing info
2021-08-05 18:39:20.614441 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:20.614631 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_pst__dbt_tmp" cascade
2021-08-05 18:39:20.848456 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:20.850752 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:20.850849 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_pst__dbt_backup" cascade
2021-08-05 18:39:21.098398 (Thread-1): SQL status: DROP VIEW in 0.25 seconds
2021-08-05 18:39:21.099853 (Thread-1): Writing runtime SQL for node "model.customer_history.order_flash_events_pst"
2021-08-05 18:39:21.100483 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:21.100571 (Thread-1): On model.customer_history.order_flash_events_pst: BEGIN
2021-08-05 18:39:21.163623 (Thread-1): SQL status: BEGIN in 0.06 seconds
2021-08-05 18:39:21.163800 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:21.163884 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */

  create view "data_platform_prod"."data_science"."order_flash_events_pst__dbt_tmp" as (
    with orders as (
    select * from "data_platform_prod"."data_science"."stg_order_pst"
),
events as (
    select * from "data_platform_prod"."data_science"."stg_events"
),
flash as (
    select * from "data_platform_prod"."data_science"."stg_flash"
),
order_flash as (
    SELECT *
    from orders LEFT JOIN flash ON flash.fk_order_unique_id=orders.order_unique_id
        and flash.fk_seat_unique_id=orders.seat_unique_id
),

final as (
    SELECT
    order_ticket_unique_id,
    order_unique_id,
    customer_unique_id,
    amount_gross,
    channel,
    sale_datetime,
    pricing_mode_id,
    is_season_ticket,
    transfer_action_id,
    events.event_unique_id,
    ticket_id,
    ticket_state,
    venue_unique_id,
    venue_zip,
    venue_type,
    datediff(days, onsale_date, sale_datetime) AS days_sold_after_onsale,
    datediff(days, sale_datetime, event_datetime) AS days_sold_before_event,
    major_category_name,
    is_canceled
    FROM order_flash INNER JOIN events USING (event_unique_id)
)

SELECT * FROM final
  );

2021-08-05 18:39:21.275038 (Thread-1): SQL status: CREATE VIEW in 0.11 seconds
2021-08-05 18:39:21.277957 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:21.278105 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */
alter table "data_platform_prod"."data_science"."order_flash_events_pst__dbt_tmp" rename to "order_flash_events_pst"
2021-08-05 18:39:21.339306 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:39:21.340206 (Thread-1): On model.customer_history.order_flash_events_pst: COMMIT
2021-08-05 18:39:21.340299 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:21.340377 (Thread-1): On model.customer_history.order_flash_events_pst: COMMIT
2021-08-05 18:39:21.609117 (Thread-1): SQL status: COMMIT in 0.27 seconds
2021-08-05 18:39:21.610993 (Thread-1): Using postgres connection "model.customer_history.order_flash_events_pst".
2021-08-05 18:39:21.611100 (Thread-1): On model.customer_history.order_flash_events_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_flash_events_pst"} */
drop view if exists "data_platform_prod"."data_science"."order_flash_events_pst__dbt_backup" cascade
2021-08-05 18:39:21.819582 (Thread-1): SQL status: DROP VIEW in 0.21 seconds
2021-08-05 18:39:21.821990 (Thread-1): finished collecting timing info
2021-08-05 18:39:21.822520 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d382d0>]}
2021-08-05 18:39:21.822712 (Thread-1): 11:39:21 | 11 of 17 OK created view model data_science.order_flash_events_pst... [CREATE VIEW in 1.23s]
2021-08-05 18:39:21.822824 (Thread-1): Finished running node model.customer_history.order_flash_events_pst
2021-08-05 18:39:21.822937 (Thread-1): Began running node model.customer_history.order_ticket_details_cst
2021-08-05 18:39:21.823115 (Thread-1): 11:39:21 | 12 of 17 START view model data_science.order_ticket_details_cst...... [RUN]
2021-08-05 18:39:21.823812 (Thread-1): Acquiring new postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:21.824062 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_flash_events_pst).
2021-08-05 18:39:21.824161 (Thread-1): Compiling model.customer_history.order_ticket_details_cst
2021-08-05 18:39:21.830907 (Thread-1): Writing injected SQL for node "model.customer_history.order_ticket_details_cst"
2021-08-05 18:39:21.831439 (Thread-1): finished collecting timing info
2021-08-05 18:39:21.838313 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:21.838411 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
drop view if exists "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_tmp" cascade
2021-08-05 18:39:22.064286 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:22.067027 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:22.067165 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_backup" cascade
2021-08-05 18:39:22.124072 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2021-08-05 18:39:22.125568 (Thread-1): Writing runtime SQL for node "model.customer_history.order_ticket_details_cst"
2021-08-05 18:39:22.126097 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:22.126187 (Thread-1): On model.customer_history.order_ticket_details_cst: BEGIN
2021-08-05 18:39:22.173197 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:22.173447 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:22.173591 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */

  create view "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_tmp" as (
    -- calculate distance between customer location vs event location

with orders as (
    SELECT * FROM "data_platform_prod"."data_science"."order_flash_events_cst"       
),
customers as (
    SELECT * FROM "data_platform_prod"."data_science"."stg_customers"
),
zipcodes as (
    select zipcode, longitude, latitude from public.us_zipcodes
    where primary_record='P'
),
-- customers_zip as (
--     Select
--         customers.*,
--         CAST(longitude AS DOUBLE PRECISION) as customer_long,
--         CAST(latitude AS DOUBLE PRECISION) as customer_lat
--     FROM customers LEFT JOIN zipcodes ON customers.zip=zipcodes.zipcode
-- ),
-- orders_zip as (
--     Select
--         orders.*,
--         CAST(longitude AS DOUBLE PRECISION) as venue_long,
--         CAST(latitude AS DOUBLE PRECISION) as venue_lat
--     FROM orders LEFT JOIN zipcodes ON orders.venue_zip=zipcodes.zipcode
-- ),
final as (
    SELECT
        customer_unique_id,
        axs_email_hash,
        order_ticket_unique_id,
        ROW_NUMBER() over (PARTITION BY order_ticket_unique_id ORDER BY 
        order_ticket_unique_id) AS order_ticket_identifier, -- to remove duplicate order_ticket_unique_id
        order_unique_id,
        amount_gross,
        channel,
        sale_datetime,
        pricing_mode_id,
        -- price_code_type,
        is_season_ticket,
        transfer_action_id,
        event_unique_id,
        ticket_id,
        ticket_state,
        days_sold_after_onsale,
        days_sold_before_event,
        venue_unique_id,
        venue_type,
        major_category_name,
        is_canceled,
        round(ST_DistanceSphere(ST_Point(CAST(c_zip.longitude AS DOUBLE PRECISION), CAST(c_zip.latitude AS DOUBLE PRECISION)), 
        ST_Point(CAST(v_zip.longitude AS DOUBLE PRECISION), CAST(v_zip.latitude AS DOUBLE PRECISION))) / 1000, 0) AS order_distance_in_km,
        order_distance_in_km / 1.6 AS order_distance_in_miles
    from customers
    INNER join orders using (customer_unique_id)
    LEFT JOIN zipcodes c_zip ON customers.zip=c_zip.zipcode
    LEFT JOIN zipcodes v_zip ON orders.venue_zip=v_zip.zipcode
)

SELECT * FROM final
  );

2021-08-05 18:39:22.277026 (Thread-1): SQL status: CREATE VIEW in 0.10 seconds
2021-08-05 18:39:22.282717 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:22.282851 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_cst" rename to "order_ticket_details_cst__dbt_backup"
2021-08-05 18:39:22.351527 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 18:39:22.354344 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:22.354459 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_tmp" rename to "order_ticket_details_cst"
2021-08-05 18:39:22.423053 (Thread-1): SQL status: ALTER TABLE in 0.07 seconds
2021-08-05 18:39:22.424265 (Thread-1): On model.customer_history.order_ticket_details_cst: COMMIT
2021-08-05 18:39:22.424367 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:22.424444 (Thread-1): On model.customer_history.order_ticket_details_cst: COMMIT
2021-08-05 18:39:23.298337 (Thread-1): SQL status: COMMIT in 0.87 seconds
2021-08-05 18:39:23.300156 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_cst".
2021-08-05 18:39:23.300255 (Thread-1): On model.customer_history.order_ticket_details_cst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_cst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_cst__dbt_backup" cascade
2021-08-05 18:39:25.320590 (Thread-1): SQL status: DROP TABLE in 2.02 seconds
2021-08-05 18:39:25.322884 (Thread-1): finished collecting timing info
2021-08-05 18:39:25.323419 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e64fd0>]}
2021-08-05 18:39:25.323611 (Thread-1): 11:39:25 | 12 of 17 OK created view model data_science.order_ticket_details_cst. [CREATE VIEW in 3.50s]
2021-08-05 18:39:25.323720 (Thread-1): Finished running node model.customer_history.order_ticket_details_cst
2021-08-05 18:39:25.323888 (Thread-1): Began running node model.customer_history.order_ticket_details_est
2021-08-05 18:39:25.324108 (Thread-1): 11:39:25 | 13 of 17 START view model data_science.order_ticket_details_est...... [RUN]
2021-08-05 18:39:25.324701 (Thread-1): Acquiring new postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:25.324792 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_ticket_details_cst).
2021-08-05 18:39:25.324875 (Thread-1): Compiling model.customer_history.order_ticket_details_est
2021-08-05 18:39:25.331830 (Thread-1): Writing injected SQL for node "model.customer_history.order_ticket_details_est"
2021-08-05 18:39:25.332339 (Thread-1): finished collecting timing info
2021-08-05 18:39:25.338348 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:25.338450 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
drop view if exists "data_platform_prod"."data_science"."order_ticket_details_est__dbt_tmp" cascade
2021-08-05 18:39:26.353906 (Thread-1): SQL status: DROP VIEW in 1.02 seconds
2021-08-05 18:39:26.356301 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:26.356406 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_est__dbt_backup" cascade
2021-08-05 18:39:26.429386 (Thread-1): SQL status: DROP TABLE in 0.07 seconds
2021-08-05 18:39:26.431188 (Thread-1): Writing runtime SQL for node "model.customer_history.order_ticket_details_est"
2021-08-05 18:39:26.431725 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:26.431858 (Thread-1): On model.customer_history.order_ticket_details_est: BEGIN
2021-08-05 18:39:26.498439 (Thread-1): SQL status: BEGIN in 0.07 seconds
2021-08-05 18:39:26.498666 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:26.498750 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */

  create view "data_platform_prod"."data_science"."order_ticket_details_est__dbt_tmp" as (
    -- calculate distance between customer location vs event location

with orders as (
    SELECT * FROM "data_platform_prod"."data_science"."order_flash_events_est"       
),
customers as (
    SELECT * FROM "data_platform_prod"."data_science"."stg_customers"
),
zipcodes as (
    select zipcode, longitude, latitude from public.us_zipcodes
    where primary_record='P'
),
-- customers_zip as (
--     Select
--         customers.*,
--         CAST(longitude AS DOUBLE PRECISION) as customer_long,
--         CAST(latitude AS DOUBLE PRECISION) as customer_lat
--     FROM customers LEFT JOIN zipcodes ON customers.zip=zipcodes.zipcode
-- ),
-- orders_zip as (
--     Select
--         orders.*,
--         CAST(longitude AS DOUBLE PRECISION) as venue_long,
--         CAST(latitude AS DOUBLE PRECISION) as venue_lat
--     FROM orders LEFT JOIN zipcodes ON orders.venue_zip=zipcodes.zipcode
-- ),
final as (
    SELECT
        customer_unique_id,
        axs_email_hash,
        order_ticket_unique_id,
        ROW_NUMBER() over (PARTITION BY order_ticket_unique_id ORDER BY 
        order_ticket_unique_id) AS order_ticket_identifier, -- to remove duplicate order_ticket_unique_id
        order_unique_id,
        amount_gross,
        channel,
        sale_datetime,
        pricing_mode_id,
        -- price_code_type,
        is_season_ticket,
        transfer_action_id,
        event_unique_id,
        ticket_id,
        ticket_state,
        days_sold_after_onsale,
        days_sold_before_event,
        venue_unique_id,
        venue_type,
        major_category_name,
        is_canceled,
        round(ST_DistanceSphere(ST_Point(CAST(c_zip.longitude AS DOUBLE PRECISION), CAST(c_zip.latitude AS DOUBLE PRECISION)), 
        ST_Point(CAST(v_zip.longitude AS DOUBLE PRECISION), CAST(v_zip.latitude AS DOUBLE PRECISION))) / 1000, 0) AS order_distance_in_km,
        order_distance_in_km / 1.6 AS order_distance_in_miles
    from customers
    INNER join orders using (customer_unique_id)
    LEFT JOIN zipcodes c_zip ON customers.zip=c_zip.zipcode
    LEFT JOIN zipcodes v_zip ON orders.venue_zip=v_zip.zipcode
)

SELECT * FROM final
  );

2021-08-05 18:39:26.614239 (Thread-1): SQL status: CREATE VIEW in 0.12 seconds
2021-08-05 18:39:26.617937 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:26.618041 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_est" rename to "order_ticket_details_est__dbt_backup"
2021-08-05 18:39:26.698375 (Thread-1): SQL status: ALTER TABLE in 0.08 seconds
2021-08-05 18:39:26.702092 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:26.702202 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_est__dbt_tmp" rename to "order_ticket_details_est"
2021-08-05 18:39:26.749088 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:26.750142 (Thread-1): On model.customer_history.order_ticket_details_est: COMMIT
2021-08-05 18:39:26.750261 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:26.750357 (Thread-1): On model.customer_history.order_ticket_details_est: COMMIT
2021-08-05 18:39:28.287857 (Thread-1): SQL status: COMMIT in 1.54 seconds
2021-08-05 18:39:28.289996 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_est".
2021-08-05 18:39:28.290111 (Thread-1): On model.customer_history.order_ticket_details_est: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_est"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_est__dbt_backup" cascade
2021-08-05 18:39:28.565418 (Thread-1): SQL status: DROP TABLE in 0.28 seconds
2021-08-05 18:39:28.567621 (Thread-1): finished collecting timing info
2021-08-05 18:39:28.568154 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ed0950>]}
2021-08-05 18:39:28.568344 (Thread-1): 11:39:28 | 13 of 17 OK created view model data_science.order_ticket_details_est. [CREATE VIEW in 3.24s]
2021-08-05 18:39:28.568452 (Thread-1): Finished running node model.customer_history.order_ticket_details_est
2021-08-05 18:39:28.568646 (Thread-1): Began running node model.customer_history.order_ticket_details_mst
2021-08-05 18:39:28.569081 (Thread-1): 11:39:28 | 14 of 17 START view model data_science.order_ticket_details_mst...... [RUN]
2021-08-05 18:39:28.569627 (Thread-1): Acquiring new postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:28.569743 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_ticket_details_est).
2021-08-05 18:39:28.569820 (Thread-1): Compiling model.customer_history.order_ticket_details_mst
2021-08-05 18:39:28.576450 (Thread-1): Writing injected SQL for node "model.customer_history.order_ticket_details_mst"
2021-08-05 18:39:28.577781 (Thread-1): finished collecting timing info
2021-08-05 18:39:28.584173 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:28.584278 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
drop view if exists "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_tmp" cascade
2021-08-05 18:39:28.815190 (Thread-1): SQL status: DROP VIEW in 0.23 seconds
2021-08-05 18:39:28.817835 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:28.817952 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_backup" cascade
2021-08-05 18:39:28.875199 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2021-08-05 18:39:28.877112 (Thread-1): Writing runtime SQL for node "model.customer_history.order_ticket_details_mst"
2021-08-05 18:39:28.877977 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:28.878107 (Thread-1): On model.customer_history.order_ticket_details_mst: BEGIN
2021-08-05 18:39:28.929326 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:28.929577 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:28.929701 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */

  create view "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_tmp" as (
    -- calculate distance between customer location vs event location

with orders as (
    SELECT * FROM "data_platform_prod"."data_science"."order_flash_events_mst"       
),
customers as (
    SELECT * FROM "data_platform_prod"."data_science"."stg_customers"
),
zipcodes as (
    select zipcode, longitude, latitude from public.us_zipcodes
    where primary_record='P'
),
-- customers_zip as (
--     Select
--         customers.*,
--         CAST(longitude AS DOUBLE PRECISION) as customer_long,
--         CAST(latitude AS DOUBLE PRECISION) as customer_lat
--     FROM customers LEFT JOIN zipcodes ON customers.zip=zipcodes.zipcode
-- ),
-- orders_zip as (
--     Select
--         orders.*,
--         CAST(longitude AS DOUBLE PRECISION) as venue_long,
--         CAST(latitude AS DOUBLE PRECISION) as venue_lat
--     FROM orders LEFT JOIN zipcodes ON orders.venue_zip=zipcodes.zipcode
-- ),
final as (
    SELECT
        customer_unique_id,
        axs_email_hash,
        order_ticket_unique_id,
        ROW_NUMBER() over (PARTITION BY order_ticket_unique_id ORDER BY 
        order_ticket_unique_id) AS order_ticket_identifier, -- to remove duplicate order_ticket_unique_id
        order_unique_id,
        amount_gross,
        channel,
        sale_datetime,
        pricing_mode_id,
        -- price_code_type,
        is_season_ticket,
        transfer_action_id,
        event_unique_id,
        ticket_id,
        ticket_state,
        days_sold_after_onsale,
        days_sold_before_event,
        venue_unique_id,
        venue_type,
        major_category_name,
        is_canceled,
        round(ST_DistanceSphere(ST_Point(CAST(c_zip.longitude AS DOUBLE PRECISION), CAST(c_zip.latitude AS DOUBLE PRECISION)), 
        ST_Point(CAST(v_zip.longitude AS DOUBLE PRECISION), CAST(v_zip.latitude AS DOUBLE PRECISION))) / 1000, 0) AS order_distance_in_km,
        order_distance_in_km / 1.6 AS order_distance_in_miles
    from customers
    INNER join orders using (customer_unique_id)
    LEFT JOIN zipcodes c_zip ON customers.zip=c_zip.zipcode
    LEFT JOIN zipcodes v_zip ON orders.venue_zip=v_zip.zipcode
)

SELECT * FROM final
  );

2021-08-05 18:39:29.000344 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2021-08-05 18:39:29.004728 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:29.004846 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_mst" rename to "order_ticket_details_mst__dbt_backup"
2021-08-05 18:39:29.055945 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:29.058822 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:29.058946 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_tmp" rename to "order_ticket_details_mst"
2021-08-05 18:39:29.116793 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2021-08-05 18:39:29.117678 (Thread-1): On model.customer_history.order_ticket_details_mst: COMMIT
2021-08-05 18:39:29.117778 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:29.117855 (Thread-1): On model.customer_history.order_ticket_details_mst: COMMIT
2021-08-05 18:39:29.328548 (Thread-1): SQL status: COMMIT in 0.21 seconds
2021-08-05 18:39:29.330448 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_mst".
2021-08-05 18:39:29.330544 (Thread-1): On model.customer_history.order_ticket_details_mst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_mst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_mst__dbt_backup" cascade
2021-08-05 18:39:29.555674 (Thread-1): SQL status: DROP TABLE in 0.23 seconds
2021-08-05 18:39:29.558190 (Thread-1): finished collecting timing info
2021-08-05 18:39:29.558810 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d4ac10>]}
2021-08-05 18:39:29.559030 (Thread-1): 11:39:29 | 14 of 17 OK created view model data_science.order_ticket_details_mst. [CREATE VIEW in 0.99s]
2021-08-05 18:39:29.559159 (Thread-1): Finished running node model.customer_history.order_ticket_details_mst
2021-08-05 18:39:29.559290 (Thread-1): Began running node model.customer_history.order_ticket_details_pst
2021-08-05 18:39:29.559427 (Thread-1): 11:39:29 | 15 of 17 START view model data_science.order_ticket_details_pst...... [RUN]
2021-08-05 18:39:29.559758 (Thread-1): Acquiring new postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:29.559860 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_ticket_details_mst).
2021-08-05 18:39:29.559954 (Thread-1): Compiling model.customer_history.order_ticket_details_pst
2021-08-05 18:39:29.567759 (Thread-1): Writing injected SQL for node "model.customer_history.order_ticket_details_pst"
2021-08-05 18:39:29.568550 (Thread-1): finished collecting timing info
2021-08-05 18:39:29.575646 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:29.575745 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
drop view if exists "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_tmp" cascade
2021-08-05 18:39:29.848507 (Thread-1): SQL status: DROP VIEW in 0.27 seconds
2021-08-05 18:39:29.850963 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:29.851064 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_backup" cascade
2021-08-05 18:39:29.902090 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:39:29.903535 (Thread-1): Writing runtime SQL for node "model.customer_history.order_ticket_details_pst"
2021-08-05 18:39:29.904095 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:29.904200 (Thread-1): On model.customer_history.order_ticket_details_pst: BEGIN
2021-08-05 18:39:29.950054 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:29.950327 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:29.950492 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */

  create view "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_tmp" as (
    -- calculate distance between customer location vs event location

with orders as (
    SELECT * FROM "data_platform_prod"."data_science"."order_flash_events_pst"       
),
customers as (
    SELECT * FROM "data_platform_prod"."data_science"."stg_customers"
),
zipcodes as (
    select zipcode, longitude, latitude from public.us_zipcodes
    where primary_record='P'
),
-- customers_zip as (
--     Select
--         customers.*,
--         CAST(longitude AS DOUBLE PRECISION) as customer_long,
--         CAST(latitude AS DOUBLE PRECISION) as customer_lat
--     FROM customers LEFT JOIN zipcodes ON customers.zip=zipcodes.zipcode
-- ),
-- orders_zip as (
--     Select
--         orders.*,
--         CAST(longitude AS DOUBLE PRECISION) as venue_long,
--         CAST(latitude AS DOUBLE PRECISION) as venue_lat
--     FROM orders LEFT JOIN zipcodes ON orders.venue_zip=zipcodes.zipcode
-- ),
final as (
    SELECT
        customer_unique_id,
        axs_email_hash,
        order_ticket_unique_id,
        ROW_NUMBER() over (PARTITION BY order_ticket_unique_id ORDER BY 
        order_ticket_unique_id) AS order_ticket_identifier, -- to remove duplicate order_ticket_unique_id
        order_unique_id,
        amount_gross,
        channel,
        sale_datetime,
        pricing_mode_id,
        -- price_code_type,
        is_season_ticket,
        transfer_action_id,
        event_unique_id,
        ticket_id,
        ticket_state,
        days_sold_after_onsale,
        days_sold_before_event,
        venue_unique_id,
        venue_type,
        major_category_name,
        is_canceled,
        round(ST_DistanceSphere(ST_Point(CAST(c_zip.longitude AS DOUBLE PRECISION), CAST(c_zip.latitude AS DOUBLE PRECISION)), 
        ST_Point(CAST(v_zip.longitude AS DOUBLE PRECISION), CAST(v_zip.latitude AS DOUBLE PRECISION))) / 1000, 0) AS order_distance_in_km,
        order_distance_in_km / 1.6 AS order_distance_in_miles
    from customers
    INNER join orders using (customer_unique_id)
    LEFT JOIN zipcodes c_zip ON customers.zip=c_zip.zipcode
    LEFT JOIN zipcodes v_zip ON orders.venue_zip=v_zip.zipcode
)

SELECT * FROM final
  );

2021-08-05 18:39:30.021720 (Thread-1): SQL status: CREATE VIEW in 0.07 seconds
2021-08-05 18:39:30.025850 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:30.025993 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_pst" rename to "order_ticket_details_pst__dbt_backup"
2021-08-05 18:39:30.079860 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:30.082546 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:30.082671 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
alter table "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_tmp" rename to "order_ticket_details_pst"
2021-08-05 18:39:30.130768 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 18:39:30.131713 (Thread-1): On model.customer_history.order_ticket_details_pst: COMMIT
2021-08-05 18:39:30.131813 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:30.131893 (Thread-1): On model.customer_history.order_ticket_details_pst: COMMIT
2021-08-05 18:39:30.339291 (Thread-1): SQL status: COMMIT in 0.21 seconds
2021-08-05 18:39:30.341285 (Thread-1): Using postgres connection "model.customer_history.order_ticket_details_pst".
2021-08-05 18:39:30.341396 (Thread-1): On model.customer_history.order_ticket_details_pst: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.order_ticket_details_pst"} */
drop table if exists "data_platform_prod"."data_science"."order_ticket_details_pst__dbt_backup" cascade
2021-08-05 18:39:30.564425 (Thread-1): SQL status: DROP TABLE in 0.22 seconds
2021-08-05 18:39:30.566815 (Thread-1): finished collecting timing info
2021-08-05 18:39:30.567413 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ca97d0>]}
2021-08-05 18:39:30.567619 (Thread-1): 11:39:30 | 15 of 17 OK created view model data_science.order_ticket_details_pst. [CREATE VIEW in 1.01s]
2021-08-05 18:39:30.567736 (Thread-1): Finished running node model.customer_history.order_ticket_details_pst
2021-08-05 18:39:30.568044 (Thread-1): Began running node model.customer_history.fct_order_ticket_details
2021-08-05 18:39:30.568197 (Thread-1): 11:39:30 | 16 of 17 START table model data_science.fct_order_ticket_details..... [RUN]
2021-08-05 18:39:30.568447 (Thread-1): Acquiring new postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:39:30.568550 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.order_ticket_details_pst).
2021-08-05 18:39:30.568635 (Thread-1): Compiling model.customer_history.fct_order_ticket_details
2021-08-05 18:39:30.578822 (Thread-1): Writing injected SQL for node "model.customer_history.fct_order_ticket_details"
2021-08-05 18:39:30.579205 (Thread-1): finished collecting timing info
2021-08-05 18:39:30.600766 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:39:30.600920 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp" cascade
2021-08-05 18:39:30.652734 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:39:30.655007 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:39:30.655110 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_backup" cascade
2021-08-05 18:39:30.706636 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 18:39:30.708190 (Thread-1): Writing runtime SQL for node "model.customer_history.fct_order_ticket_details"
2021-08-05 18:39:30.708806 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:39:30.708904 (Thread-1): On model.customer_history.fct_order_ticket_details: BEGIN
2021-08-05 18:39:30.756388 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 18:39:30.756698 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 18:39:30.756854 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */


  create  table "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp"
  as (
    

with cst as (
    select * from "data_platform_prod"."data_science"."order_ticket_details_cst"
),
est as (
    select * from "data_platform_prod"."data_science"."order_ticket_details_est"
),
mst as (
    select * from "data_platform_prod"."data_science"."order_ticket_details_mst"
),
pst as (
    select * from "data_platform_prod"."data_science"."order_ticket_details_pst"
),
final as (
    SELECT * from cst
    UNION
    SELECT * from est
    UNION
    SELECT * from mst
    UNION
    SELECT * from pst
)

SELECT * FROM final
  );
2021-08-05 19:19:42.169859 (Thread-1): SQL status: SELECT in 2411.41 seconds
2021-08-05 19:19:42.177059 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 19:19:42.177225 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
alter table "data_platform_prod"."data_science"."fct_order_ticket_details" rename to "fct_order_ticket_details__dbt_backup"
2021-08-05 19:19:42.414951 (Thread-1): SQL status: ALTER TABLE in 0.24 seconds
2021-08-05 19:19:42.420890 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 19:19:42.421063 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
alter table "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_tmp" rename to "fct_order_ticket_details"
2021-08-05 19:19:42.471512 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 19:19:42.472951 (Thread-1): On model.customer_history.fct_order_ticket_details: COMMIT
2021-08-05 19:19:42.473130 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 19:19:42.473260 (Thread-1): On model.customer_history.fct_order_ticket_details: COMMIT
2021-08-05 19:19:50.735450 (Thread-1): SQL status: COMMIT in 8.26 seconds
2021-08-05 19:19:50.739019 (Thread-1): Using postgres connection "model.customer_history.fct_order_ticket_details".
2021-08-05 19:19:50.739177 (Thread-1): On model.customer_history.fct_order_ticket_details: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.fct_order_ticket_details"} */
drop table if exists "data_platform_prod"."data_science"."fct_order_ticket_details__dbt_backup" cascade
2021-08-05 19:19:52.838626 (Thread-1): SQL status: DROP TABLE in 2.10 seconds
2021-08-05 19:19:52.842927 (Thread-1): finished collecting timing info
2021-08-05 19:19:52.843812 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b93290>]}
2021-08-05 19:19:52.844145 (Thread-1): 12:19:52 | 16 of 17 OK created table model data_science.fct_order_ticket_details [SELECT in 2422.28s]
2021-08-05 19:19:52.844332 (Thread-1): Finished running node model.customer_history.fct_order_ticket_details
2021-08-05 19:19:52.844802 (Thread-1): Began running node model.customer_history.dim_customers_email_acxiom
2021-08-05 19:19:52.845011 (Thread-1): 12:19:52 | 17 of 17 START table model data_science.dim_customers_email_acxiom... [RUN]
2021-08-05 19:19:52.845371 (Thread-1): Acquiring new postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:19:52.845489 (Thread-1): Re-using an available connection from the pool (formerly model.customer_history.fct_order_ticket_details).
2021-08-05 19:19:52.845606 (Thread-1): Compiling model.customer_history.dim_customers_email_acxiom
2021-08-05 19:19:52.854381 (Thread-1): Writing injected SQL for node "model.customer_history.dim_customers_email_acxiom"
2021-08-05 19:19:52.856292 (Thread-1): finished collecting timing info
2021-08-05 19:19:52.864882 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:19:52.865087 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_tmp" cascade
2021-08-05 19:19:52.922046 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2021-08-05 19:19:52.926168 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:19:52.926330 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_backup" cascade
2021-08-05 19:19:52.977367 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2021-08-05 19:19:52.980671 (Thread-1): Writing runtime SQL for node "model.customer_history.dim_customers_email_acxiom"
2021-08-05 19:19:52.981291 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:19:52.981440 (Thread-1): On model.customer_history.dim_customers_email_acxiom: BEGIN
2021-08-05 19:19:53.027651 (Thread-1): SQL status: BEGIN in 0.05 seconds
2021-08-05 19:19:53.028115 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:19:53.028311 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */


  create  table "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_tmp"
  as (
    

with orders as (
    select * from "data_platform_prod"."data_science"."fct_order_ticket_details"
),

customer_orders as (
    select
        axs_email_hash,
        min(sale_datetime) as first_order_date,
        max(sale_datetime) as most_recent_order_date,
        COUNT(DISTINCT CASE WHEN (NOT COALESCE(pricing_mode_id = 1 , FALSE)) THEN 
        order_ticket_unique_id ELSE NULL END) AS tickets_sold_no_comps,
        COUNT(DISTINCT order_ticket_unique_id) AS number_of_tickets_sold,
        COUNT(DISTINCT order_unique_id) AS number_of_orders,
        COUNT(DISTINCT event_unique_id) AS number_of_events,

        SUM(CASE WHEN order_ticket_identifier=1 THEN amount_gross ELSE 0 END) AS total_revenue,

        SUM(FLOOR(COALESCE((CASE WHEN order_ticket_identifier=1 THEN days_sold_after_onsale ELSE 0 END), 0))) / COUNT(DISTINCT CASE WHEN days_sold_after_onsale IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_after_onsale,
        SUM(FLOOR(COALESCE((CASE WHEN order_ticket_identifier=1 THEN  days_sold_before_event ELSE 0 END), 0)))/ COUNT(DISTINCT CASE WHEN days_sold_before_event IS NOT NULL THEN 
        order_ticket_unique_id  ELSE NULL END) AS average_days_sold_before_event,

        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        ticket_id ELSE NULL END) AS count_transferred_tickets,
        COUNT(DISTINCT CASE WHEN (ticket_state = 'TRANSFERRED') THEN 
        transfer_action_id || ':' || ticket_id  ELSE NULL END) AS count_transfers,

        AVG(CASE WHEN order_ticket_identifier=1 THEN order_distance_in_km ELSE NULL END) AS average_order_distance_in_km,

        COUNT(DISTINCT venue_unique_id) AS number_of_venues,

        ROUND(COUNT(DISTINCT CASE WHEN channel='Back Office' THEN order_ticket_unique_id ELSE NULL END) *1.0 / number_of_tickets_sold, 2) AS channel_back_office_percent,
        ROUND(COUNT(DISTINCT CASE WHEN channel='Web' THEN
            order_ticket_unique_id ELSE NULL END) *1.0 / number_of_tickets_sold, 2) AS channel_web_percent,

        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Sports' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_sports_percent,
        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Music' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_music_percent,
        ROUND(COUNT(DISTINCT CASE WHEN major_category_name='Arts & Family' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS cat_arts_family_percent,

        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Arena' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_arena_percent,
        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Large Music Venue' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_large_music_percent,
        ROUND(COUNT(DISTINCT CASE WHEN venue_type='Club and Theater' THEN
            event_unique_id ELSE NULL END) *1.0 / number_of_events, 2) AS venue_club_theatre_percent,

        COUNT(DISTINCT CASE WHEN is_season_ticket = 1 THEN order_ticket_unique_id ELSE NULL END) AS number_of_season_tickets
        -- COUNT(DISTINCT CASE WHEN price_code_type ilike '%season%' THEN order_ticket_unique_id ELSE NULL END) AS number_of_season_tickets

    from orders
    WHERE is_canceled is FALSE -- shall this condition live elsewhere?
    group by 1 
)
select * from customer_orders
  );
2021-08-05 19:23:53.388795 (Thread-1): SQL status: SELECT in 240.36 seconds
2021-08-05 19:23:53.395098 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:23:53.395262 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
alter table "data_platform_prod"."data_science"."dim_customers_email_acxiom" rename to "dim_customers_email_acxiom__dbt_backup"
2021-08-05 19:23:53.449217 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 19:23:53.453574 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:23:53.453734 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
alter table "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_tmp" rename to "dim_customers_email_acxiom"
2021-08-05 19:23:53.506609 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2021-08-05 19:23:53.508565 (Thread-1): On model.customer_history.dim_customers_email_acxiom: COMMIT
2021-08-05 19:23:53.508777 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:23:53.508943 (Thread-1): On model.customer_history.dim_customers_email_acxiom: COMMIT
2021-08-05 19:23:53.974143 (Thread-1): SQL status: COMMIT in 0.46 seconds
2021-08-05 19:23:53.977638 (Thread-1): Using postgres connection "model.customer_history.dim_customers_email_acxiom".
2021-08-05 19:23:53.977790 (Thread-1): On model.customer_history.dim_customers_email_acxiom: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "axs", "target_name": "dev", "node_id": "model.customer_history.dim_customers_email_acxiom"} */
drop table if exists "data_platform_prod"."data_science"."dim_customers_email_acxiom__dbt_backup" cascade
2021-08-05 19:23:54.192125 (Thread-1): SQL status: DROP TABLE in 0.21 seconds
2021-08-05 19:23:54.196543 (Thread-1): finished collecting timing info
2021-08-05 19:23:54.197415 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e312cc4-fd4e-4cf9-aa72-eb147671162d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d3bed0>]}
2021-08-05 19:23:54.197724 (Thread-1): 12:23:54 | 17 of 17 OK created table model data_science.dim_customers_email_acxiom [SELECT in 241.35s]
2021-08-05 19:23:54.197904 (Thread-1): Finished running node model.customer_history.dim_customers_email_acxiom
2021-08-05 19:23:54.277513 (MainThread): Using postgres connection "master".
2021-08-05 19:23:54.277825 (MainThread): On master: BEGIN
2021-08-05 19:23:54.336555 (MainThread): SQL status: BEGIN in 0.06 seconds
2021-08-05 19:23:54.337023 (MainThread): On master: COMMIT
2021-08-05 19:23:54.337309 (MainThread): Using postgres connection "master".
2021-08-05 19:23:54.337466 (MainThread): On master: COMMIT
2021-08-05 19:23:54.384312 (MainThread): SQL status: COMMIT in 0.05 seconds
2021-08-05 19:23:54.385286 (MainThread): 12:23:54 | 
2021-08-05 19:23:54.385529 (MainThread): 12:23:54 | Finished running 15 view models, 2 table models in 2690.46s.
2021-08-05 19:23:54.385725 (MainThread): Connection 'master' was left open.
2021-08-05 19:23:54.385883 (MainThread): On master: Close
2021-08-05 19:23:54.386333 (MainThread): Connection 'model.customer_history.dim_customers_email_acxiom' was left open.
2021-08-05 19:23:54.386502 (MainThread): On model.customer_history.dim_customers_email_acxiom: Close
2021-08-05 19:23:54.439640 (MainThread): 
2021-08-05 19:23:54.439885 (MainThread): Completed successfully
2021-08-05 19:23:54.440065 (MainThread): 
Done. PASS=17 WARN=0 ERROR=0 SKIP=0 TOTAL=17
2021-08-05 19:23:54.440296 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ed2190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e0c510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e67c50>]}
2021-08-05 19:23:54.440518 (MainThread): Flushing usage events
